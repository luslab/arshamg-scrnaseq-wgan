{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN - CHECK SPELLING\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "feature_path = data_path + '/tpm_combined.csv'\n",
    "gene_name_path = data_path + '/tpm_combined_rows.csv'\n",
    "cell_name_path = data_path + '/tpm_combined_cols.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load datasets into frames and check all the shapes match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19248, 1)\n",
      "(771, 1)\n",
      "(19248, 771)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names = pd.read_csv(gene_name_path, header=None)\n",
    "df_cell_names = pd.read_csv(cell_name_path, header=None)\n",
    "df_training_data = pd.read_csv(feature_path, header=None)\n",
    "\n",
    "print(df_gene_names.shape)\n",
    "print(df_cell_names.shape)\n",
    "print(df_training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of genes in the input dataset determines the generator output as well as the dicriminator inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19248, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_genes = df_gene_names.shape[0]\n",
    "df_gene_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19248 entries, 0 to 19247\n",
      "Columns: 771 entries, 0 to 770\n",
      "dtypes: float64(771)\n",
      "memory usage: 113.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.217231</td>\n",
       "      <td>3.003602</td>\n",
       "      <td>4.209453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.296824</td>\n",
       "      <td>5.300856</td>\n",
       "      <td>5.587965</td>\n",
       "      <td>3.826803</td>\n",
       "      <td>3.414136</td>\n",
       "      <td>4.888013</td>\n",
       "      <td>...</td>\n",
       "      <td>4.441616</td>\n",
       "      <td>3.732269</td>\n",
       "      <td>3.347666</td>\n",
       "      <td>3.945795</td>\n",
       "      <td>3.503349</td>\n",
       "      <td>1.992768</td>\n",
       "      <td>0.739848</td>\n",
       "      <td>4.901108</td>\n",
       "      <td>4.623516</td>\n",
       "      <td>4.512859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.547203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.368768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.269033</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.821838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.835924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.109361</td>\n",
       "      <td>5.938286</td>\n",
       "      <td>5.093391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.693766</td>\n",
       "      <td>4.627607</td>\n",
       "      <td>6.537141</td>\n",
       "      <td>3.842979</td>\n",
       "      <td>2.786596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.545968</td>\n",
       "      <td>0.641546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.670161</td>\n",
       "      <td>3.389567</td>\n",
       "      <td>5.522307</td>\n",
       "      <td>3.014355</td>\n",
       "      <td>6.703627</td>\n",
       "      <td>2.608809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19243</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19244</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.440952</td>\n",
       "      <td>2.097611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.385431</td>\n",
       "      <td>3.339137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.599318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.193378</td>\n",
       "      <td>6.386294</td>\n",
       "      <td>2.114367</td>\n",
       "      <td>6.742680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.936402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.265287</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19245</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19246</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>7.179013</td>\n",
       "      <td>10.450252</td>\n",
       "      <td>8.366235</td>\n",
       "      <td>8.709325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.797662</td>\n",
       "      <td>9.444497</td>\n",
       "      <td>9.306312</td>\n",
       "      <td>6.193575</td>\n",
       "      <td>7.741602</td>\n",
       "      <td>...</td>\n",
       "      <td>11.506675</td>\n",
       "      <td>11.987982</td>\n",
       "      <td>10.014341</td>\n",
       "      <td>10.988521</td>\n",
       "      <td>9.570615</td>\n",
       "      <td>11.136901</td>\n",
       "      <td>11.464178</td>\n",
       "      <td>9.998844</td>\n",
       "      <td>10.938102</td>\n",
       "      <td>10.111149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19248 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2         3         4         5         6    \\\n",
       "0      4.217231   3.003602  4.209453  0.000000  5.296824  5.300856  5.587965   \n",
       "1      0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2      0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3      0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4      3.109361   5.938286  5.093391  0.000000  0.000000  2.693766  4.627607   \n",
       "...         ...        ...       ...       ...       ...       ...       ...   \n",
       "19243  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19244  0.000000   5.440952  2.097611  0.000000  0.000000  3.385431  3.339137   \n",
       "19245  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19246  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19247  7.179013  10.450252  8.366235  8.709325  0.000000  7.797662  9.444497   \n",
       "\n",
       "            7         8         9    ...        761        762        763  \\\n",
       "0      3.826803  3.414136  4.888013  ...   4.441616   3.732269   3.347666   \n",
       "1      0.000000  0.000000  0.000000  ...   3.547203   0.000000   0.000000   \n",
       "2      6.821838  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "3      0.000000  0.000000  3.835924  ...   0.000000   0.000000   0.000000   \n",
       "4      6.537141  3.842979  2.786596  ...   0.000000   2.545968   0.641546   \n",
       "...         ...       ...       ...  ...        ...        ...        ...   \n",
       "19243  0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "19244  0.000000  5.599318  0.000000  ...   6.193378   6.386294   2.114367   \n",
       "19245  0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "19246  0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "19247  9.306312  6.193575  7.741602  ...  11.506675  11.987982  10.014341   \n",
       "\n",
       "             764       765        766        767       768        769  \\\n",
       "0       3.945795  3.503349   1.992768   0.739848  4.901108   4.623516   \n",
       "1       0.000000  0.000000   3.368768   0.000000  0.000000   1.269033   \n",
       "2       0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "3       0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "4       0.000000  3.670161   3.389567   5.522307  3.014355   6.703627   \n",
       "...          ...       ...        ...        ...       ...        ...   \n",
       "19243   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "19244   6.742680  0.000000   5.936402   0.000000  0.000000   6.265287   \n",
       "19245   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "19246   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "19247  10.988521  9.570615  11.136901  11.464178  9.998844  10.938102   \n",
       "\n",
       "             770  \n",
       "0       4.512859  \n",
       "1       0.000000  \n",
       "2       0.000000  \n",
       "3       0.000000  \n",
       "4       2.608809  \n",
       "...          ...  \n",
       "19243   0.000000  \n",
       "19244   0.000000  \n",
       "19245   0.000000  \n",
       "19246   0.000000  \n",
       "19247  10.111149  \n",
       "\n",
       "[19248 rows x 771 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.161062384674267\n"
     ]
    }
   ],
   "source": [
    "training_data_max = df_training_data.max()\n",
    "training_data_max = training_data_max.max()\n",
    "print(training_data_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "(19248,)\n"
     ]
    }
   ],
   "source": [
    "np_training_data = df_training_data.T.values\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(np_training_data))\n",
    "\n",
    "# Check which dimension we are fitting to - if we are fitting to gene expression then should be equal to number of genes\n",
    "print(scaler.data_max_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19248, 771)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_training_data_norm = np.transpose(scaler.transform(np_training_data))\n",
    "np_training_data_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get max values for noise generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "training_data_max = np_training_data_norm.max()\n",
    "training_data_max = training_data_max.max()\n",
    "print(training_data_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model variables - COMMENT ON EACH ONE TO DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LATENT_VARIABLE_SIZE = 100\n",
    "GEN_L1_DENSE_SIZE = 600\n",
    "GEN_L2_DENSE_SIZE = 600\n",
    "GEN_L3_DENSE_SIZE = num_genes\n",
    "\n",
    "DIS_INPUT_SIZE = num_genes\n",
    "DIS_L1_DENSE_SIZE = 200\n",
    "DIS_L2_DENSE_SIZE = 200\n",
    "\n",
    "NOISE_STDEV = training_data_max / 10\n",
    "POISSON_LAM = 1\n",
    "\n",
    "# Training params\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "GEN_BATCH_SIZE = 10\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(NOISE_STDEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "Create tensors from training data - Convert to Int32 for better work on GPU with batch and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 19248), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(df_training_data.T.values.astype('float32')).shuffle(BUFFER_SIZE).batch(TRAIN_BATCH_SIZE)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GANN model\n",
    "\n",
    "Define function for contructing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(GEN_L1_DENSE_SIZE, use_bias=False, input_shape=(LATENT_VARIABLE_SIZE,)))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L1_DENSE_SIZE, 1)  # Note: None is the batch size\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(GEN_L2_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L2_DENSE_SIZE, 1)\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Dense(GEN_L3_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L3_DENSE_SIZE, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for constructing discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(DIS_L1_DENSE_SIZE, use_bias=False, input_shape=(DIS_INPUT_SIZE,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(DIS_L2_DENSE_SIZE, use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise():\n",
    "    # Create some random noise for the generator\n",
    "    n_noise = tf.random.normal([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], mean=0.0, stddev=NOISE_STDEV)\n",
    "    p_noise = tf.random.poisson([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], lam=POISSON_LAM)\n",
    "    noise = tf.abs(n_noise + p_noise)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "    #total_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #total_loss = -tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a batch of real cell profiles from the training set\n",
    "# @tf.function\n",
    "def train_step(cell_profiles):\n",
    "    noise = gen_noise()\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_profiles = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(cell_profiles, training=True)\n",
    "        fake_output = discriminator(generated_profiles, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        met_gen_loss(gen_loss)\n",
    "        met_disc_loss(disc_loss)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GANN model\n",
    "\n",
    "Create generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from test data to check network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 19248)\n",
      "-0.14763017\n",
      "0.4243678\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "noise = gen_noise()\n",
    "generated_profile = generator(noise, training=False)\n",
    "print(generated_profile.shape)\n",
    "print(generated_profile.numpy().min())\n",
    "print(generated_profile.numpy().max())\n",
    "\n",
    "decision = discriminator(generated_profile)\n",
    "print(decision.shape)\n",
    "#print(decision.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GANN\n",
    "\n",
    "Define tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_gen_loss = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
    "met_disc_loss = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create log directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Epoch 1, Gen_loss: 26.995113372802734, Disc_loss: 13.918001174926758\n",
      "Epoch 2, Gen_loss: 139.33612060546875, Disc_loss: 25.888126373291016\n",
      "Epoch 3, Gen_loss: 8845.865234375, Disc_loss: 977.6671752929688\n",
      "Epoch 4, Gen_loss: 5823.51025390625, Disc_loss: 391.9749450683594\n",
      "Epoch 5, Gen_loss: 186.37045288085938, Disc_loss: 11.700782775878906\n",
      "Epoch 6, Gen_loss: 325.5460510253906, Disc_loss: 23.696300506591797\n",
      "Epoch 7, Gen_loss: 48.48832321166992, Disc_loss: 4.544403076171875\n",
      "Epoch 8, Gen_loss: 142.6480712890625, Disc_loss: 4.089254856109619\n",
      "Epoch 9, Gen_loss: 61.97325134277344, Disc_loss: 3.2251739501953125\n",
      "Time for epoch 10 is 10.437034845352173 sec.\n",
      "Epoch 10, Gen_loss: 43.58432388305664, Disc_loss: 2.6628577709198\n",
      "Epoch 11, Gen_loss: 22.50831413269043, Disc_loss: 3.883500337600708\n",
      "Epoch 12, Gen_loss: 33.06168746948242, Disc_loss: 2.6860525608062744\n",
      "Epoch 13, Gen_loss: 110.39103698730469, Disc_loss: 7.737926959991455\n",
      "Epoch 14, Gen_loss: 55.52409362792969, Disc_loss: 5.7330121994018555\n",
      "Epoch 15, Gen_loss: 43.65978240966797, Disc_loss: 11.207966804504395\n",
      "Epoch 16, Gen_loss: 90.4746322631836, Disc_loss: 10.580686569213867\n",
      "Epoch 17, Gen_loss: 98.50695037841797, Disc_loss: 9.481226921081543\n",
      "Epoch 18, Gen_loss: 70.53363800048828, Disc_loss: 9.199311256408691\n",
      "Epoch 19, Gen_loss: 139.5149688720703, Disc_loss: 31.745229721069336\n",
      "Time for epoch 20 is 8.713078498840332 sec.\n",
      "Epoch 20, Gen_loss: 160.81727600097656, Disc_loss: 32.49962615966797\n",
      "Epoch 21, Gen_loss: 95.81185150146484, Disc_loss: 26.966304779052734\n",
      "Epoch 22, Gen_loss: 173.77362060546875, Disc_loss: 31.972747802734375\n",
      "Epoch 23, Gen_loss: 174.38967895507812, Disc_loss: 20.5239200592041\n",
      "Epoch 24, Gen_loss: 95.69249725341797, Disc_loss: 15.878120422363281\n",
      "Epoch 25, Gen_loss: 96.25416564941406, Disc_loss: 15.093149185180664\n",
      "Epoch 26, Gen_loss: 151.01402282714844, Disc_loss: 19.120187759399414\n",
      "Epoch 27, Gen_loss: 149.4448699951172, Disc_loss: 30.329330444335938\n",
      "Epoch 28, Gen_loss: 161.6808319091797, Disc_loss: 25.229156494140625\n",
      "Epoch 29, Gen_loss: 266.64312744140625, Disc_loss: 36.80097579956055\n",
      "Time for epoch 30 is 9.194823265075684 sec.\n",
      "Epoch 30, Gen_loss: 130.92393493652344, Disc_loss: 12.047540664672852\n",
      "Epoch 31, Gen_loss: 71.12972259521484, Disc_loss: 6.812121868133545\n",
      "Epoch 32, Gen_loss: 58.61876678466797, Disc_loss: 6.498625755310059\n",
      "Epoch 33, Gen_loss: 61.1545524597168, Disc_loss: 11.610671997070312\n",
      "Epoch 34, Gen_loss: 86.32000732421875, Disc_loss: 12.617695808410645\n",
      "Epoch 35, Gen_loss: 83.34208679199219, Disc_loss: 13.151601791381836\n",
      "Epoch 36, Gen_loss: 142.98052978515625, Disc_loss: 19.557252883911133\n",
      "Epoch 37, Gen_loss: 184.1487579345703, Disc_loss: 20.06693458557129\n",
      "Epoch 38, Gen_loss: 137.7880401611328, Disc_loss: 19.39142608642578\n",
      "Epoch 39, Gen_loss: 90.30853271484375, Disc_loss: 11.187868118286133\n",
      "Time for epoch 40 is 9.104838371276855 sec.\n",
      "Epoch 40, Gen_loss: 110.99749755859375, Disc_loss: 9.690774917602539\n",
      "Epoch 41, Gen_loss: 209.63917541503906, Disc_loss: 28.59101676940918\n",
      "Epoch 42, Gen_loss: 103.84062194824219, Disc_loss: 15.659626007080078\n",
      "Epoch 43, Gen_loss: 126.96039581298828, Disc_loss: 14.061484336853027\n",
      "Epoch 44, Gen_loss: 62.76890563964844, Disc_loss: 5.470016956329346\n",
      "Epoch 45, Gen_loss: 47.73543167114258, Disc_loss: 6.35439395904541\n",
      "Epoch 46, Gen_loss: 75.80351257324219, Disc_loss: 11.928037643432617\n",
      "Epoch 47, Gen_loss: 102.66229248046875, Disc_loss: 13.664618492126465\n",
      "Epoch 48, Gen_loss: 111.39997863769531, Disc_loss: 12.701915740966797\n",
      "Epoch 49, Gen_loss: 99.82524108886719, Disc_loss: 12.258818626403809\n",
      "Time for epoch 50 is 8.522576093673706 sec.\n",
      "Epoch 50, Gen_loss: 80.19514465332031, Disc_loss: 12.82224178314209\n",
      "Epoch 51, Gen_loss: 106.60823822021484, Disc_loss: 16.2454891204834\n",
      "Epoch 52, Gen_loss: 79.8847885131836, Disc_loss: 7.932490348815918\n",
      "Epoch 53, Gen_loss: 120.67723083496094, Disc_loss: 8.78912353515625\n",
      "Epoch 54, Gen_loss: 65.72146606445312, Disc_loss: 7.794928073883057\n",
      "Epoch 55, Gen_loss: 43.909568786621094, Disc_loss: 3.697624444961548\n",
      "Epoch 56, Gen_loss: 57.67008590698242, Disc_loss: 8.721100807189941\n",
      "Epoch 57, Gen_loss: 76.22398376464844, Disc_loss: 10.01998519897461\n",
      "Epoch 58, Gen_loss: 79.80135345458984, Disc_loss: 7.782475471496582\n",
      "Epoch 59, Gen_loss: 67.82818603515625, Disc_loss: 9.120227813720703\n",
      "Time for epoch 60 is 8.755854606628418 sec.\n",
      "Epoch 60, Gen_loss: 60.63837814331055, Disc_loss: 8.481316566467285\n",
      "Epoch 61, Gen_loss: 122.76160430908203, Disc_loss: 14.295363426208496\n",
      "Epoch 62, Gen_loss: 138.91807556152344, Disc_loss: 8.119209289550781\n",
      "Epoch 63, Gen_loss: 30.081260681152344, Disc_loss: 2.0261247158050537\n",
      "Epoch 64, Gen_loss: 32.27817153930664, Disc_loss: 4.016323566436768\n",
      "Epoch 65, Gen_loss: 54.45246887207031, Disc_loss: 4.939978122711182\n",
      "Epoch 66, Gen_loss: 31.58479881286621, Disc_loss: 3.4918134212493896\n",
      "Epoch 67, Gen_loss: 26.138994216918945, Disc_loss: 2.988814353942871\n",
      "Epoch 68, Gen_loss: 40.63102340698242, Disc_loss: 6.12446403503418\n",
      "Epoch 69, Gen_loss: 58.927520751953125, Disc_loss: 6.752187728881836\n",
      "Time for epoch 70 is 9.21770191192627 sec.\n",
      "Epoch 70, Gen_loss: 44.08055877685547, Disc_loss: 7.74394416809082\n",
      "Epoch 71, Gen_loss: 75.76535034179688, Disc_loss: 13.041474342346191\n",
      "Epoch 72, Gen_loss: 61.657344818115234, Disc_loss: 5.873600006103516\n",
      "Epoch 73, Gen_loss: 36.968353271484375, Disc_loss: 4.042290687561035\n",
      "Epoch 74, Gen_loss: 46.49870681762695, Disc_loss: 6.205604076385498\n",
      "Epoch 75, Gen_loss: 38.52405548095703, Disc_loss: 6.147815227508545\n",
      "Epoch 76, Gen_loss: 37.06261444091797, Disc_loss: 4.237107276916504\n",
      "Epoch 77, Gen_loss: 16.91844367980957, Disc_loss: 1.4386931657791138\n",
      "Epoch 78, Gen_loss: 3.823598623275757, Disc_loss: 0.27569377422332764\n",
      "Epoch 79, Gen_loss: 2.5455315113067627, Disc_loss: 0.4466646611690521\n",
      "Time for epoch 80 is 9.229915380477905 sec.\n",
      "Epoch 80, Gen_loss: 2.0692243576049805, Disc_loss: 0.7518016695976257\n",
      "Epoch 81, Gen_loss: 2.823763608932495, Disc_loss: 0.5005801320075989\n",
      "Epoch 82, Gen_loss: 2.52955961227417, Disc_loss: 0.5131505727767944\n",
      "Epoch 83, Gen_loss: 2.6668038368225098, Disc_loss: 0.597895622253418\n",
      "Epoch 84, Gen_loss: 2.8395957946777344, Disc_loss: 0.6103824973106384\n",
      "Epoch 85, Gen_loss: 4.3360772132873535, Disc_loss: 0.6426021456718445\n",
      "Epoch 86, Gen_loss: 9.582534790039062, Disc_loss: 1.4712238311767578\n",
      "Epoch 87, Gen_loss: 32.73854064941406, Disc_loss: 3.3314061164855957\n",
      "Epoch 88, Gen_loss: 24.20899772644043, Disc_loss: 2.3268885612487793\n",
      "Epoch 89, Gen_loss: 21.164796829223633, Disc_loss: 1.8327319622039795\n",
      "Time for epoch 90 is 9.210111618041992 sec.\n",
      "Epoch 90, Gen_loss: 16.519685745239258, Disc_loss: 1.8258095979690552\n",
      "Epoch 91, Gen_loss: 18.265300750732422, Disc_loss: 1.8566787242889404\n",
      "Epoch 92, Gen_loss: 8.644118309020996, Disc_loss: 0.6110966801643372\n",
      "Epoch 93, Gen_loss: 3.8481483459472656, Disc_loss: 0.28848662972450256\n",
      "Epoch 94, Gen_loss: 2.721418857574463, Disc_loss: 0.40241461992263794\n",
      "Epoch 95, Gen_loss: 2.278252124786377, Disc_loss: 0.6212260723114014\n",
      "Epoch 96, Gen_loss: 4.392510890960693, Disc_loss: 0.6102519631385803\n",
      "Epoch 97, Gen_loss: 17.564489364624023, Disc_loss: 2.428220510482788\n",
      "Epoch 98, Gen_loss: 42.04849624633789, Disc_loss: 5.101726531982422\n",
      "Epoch 99, Gen_loss: 17.85232925415039, Disc_loss: 1.9599241018295288\n",
      "Time for epoch 100 is 9.200422525405884 sec.\n",
      "Epoch 100, Gen_loss: 12.630658149719238, Disc_loss: 1.164123296737671\n",
      "Epoch 101, Gen_loss: 8.519452095031738, Disc_loss: 0.930279552936554\n",
      "Epoch 102, Gen_loss: 11.669269561767578, Disc_loss: 1.0723910331726074\n",
      "Epoch 103, Gen_loss: 3.762099504470825, Disc_loss: 0.2753303349018097\n",
      "Epoch 104, Gen_loss: 2.3609817028045654, Disc_loss: 0.47753021121025085\n",
      "Epoch 105, Gen_loss: 2.6865530014038086, Disc_loss: 0.5781744718551636\n",
      "Epoch 106, Gen_loss: 2.834420919418335, Disc_loss: 0.5870848298072815\n",
      "Epoch 107, Gen_loss: 5.740884304046631, Disc_loss: 0.9357590079307556\n",
      "Epoch 108, Gen_loss: 42.064754486083984, Disc_loss: 6.565941333770752\n",
      "Epoch 109, Gen_loss: 43.87511444091797, Disc_loss: 7.091318130493164\n",
      "Time for epoch 110 is 9.271908044815063 sec.\n",
      "Epoch 110, Gen_loss: 81.35022735595703, Disc_loss: 17.467464447021484\n",
      "Epoch 111, Gen_loss: 23961.705078125, Disc_loss: 3658.48876953125\n",
      "Epoch 112, Gen_loss: 392567.46875, Disc_loss: 18750.50390625\n",
      "Epoch 113, Gen_loss: 62812.9296875, Disc_loss: 8319.048828125\n",
      "Epoch 114, Gen_loss: 18140.080078125, Disc_loss: 1807.47314453125\n",
      "Epoch 115, Gen_loss: 6688.70849609375, Disc_loss: 372.037841796875\n",
      "Epoch 116, Gen_loss: 5110.330078125, Disc_loss: 239.62948608398438\n",
      "Epoch 117, Gen_loss: 4155.0078125, Disc_loss: 371.6259765625\n",
      "Epoch 118, Gen_loss: 8843.7763671875, Disc_loss: 289.8850402832031\n",
      "Epoch 119, Gen_loss: 2255.060546875, Disc_loss: 57.24589920043945\n",
      "Time for epoch 120 is 8.622585535049438 sec.\n",
      "Epoch 120, Gen_loss: 1796.7890625, Disc_loss: 70.16358947753906\n",
      "Epoch 121, Gen_loss: 2596.38916015625, Disc_loss: 127.09269714355469\n",
      "Epoch 122, Gen_loss: 1239.6065673828125, Disc_loss: 48.85520935058594\n",
      "Epoch 123, Gen_loss: 591.9758911132812, Disc_loss: 27.915761947631836\n",
      "Epoch 124, Gen_loss: 445.1597595214844, Disc_loss: 20.222734451293945\n",
      "Epoch 125, Gen_loss: 949.78125, Disc_loss: 35.133636474609375\n"
     ]
    }
   ],
   "source": [
    "print('Running...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    #Train the epoch\n",
    "    for data_batch in train_dataset:\n",
    "        train_step(data_batch)\n",
    "    \n",
    "    #Log metrics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_loss', met_gen_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('disc_loss', met_disc_loss.result(), step=epoch)\n",
    "    \n",
    "    #Do some basic time logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "    else:\n",
    "        time.time()\n",
    "    \n",
    "    #Log stats\n",
    "    template = 'Epoch {}, Gen_loss: {}, Disc_loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                           met_gen_loss.result(), \n",
    "                           met_disc_loss.result()))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    met_gen_loss.reset_states()\n",
    "    met_disc_loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {train_log_dir} --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
