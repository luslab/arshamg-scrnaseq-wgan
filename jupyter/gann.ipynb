{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN - CHECK SPELLING\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "\n",
    "train_feature_path = data_path + '/tpm_combined.csv'\n",
    "train_gene_name_path = data_path + '/tpm_combined_rows.csv'\n",
    "train_cell_name_path = data_path + '/tpm_combined_cols.csv'\n",
    "\n",
    "test_feature_path = data_path + '/tpm_combined_test.csv'\n",
    "test_gene_name_path = data_path + '/tpm_combined_rows_test.csv'\n",
    "test_cell_name_path = data_path + '/tpm_combined_cols_test.csv'\n",
    "\n",
    "train_nonorm_path = data_path + '/tpm_combined_train_nonorm.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load datasets into frames and check all the shapes match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene_names = pd.read_csv(train_gene_name_path, header=None)\n",
    "df_cell_names = pd.read_csv(train_cell_name_path, header=None)\n",
    "df_training_data = pd.read_csv(train_feature_path, header=None)\n",
    "\n",
    "df_gene_names.columns = ['gene_name']\n",
    "\n",
    "print(df_gene_names.shape)\n",
    "print(df_cell_names.shape)\n",
    "print(df_training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data_nonorm = pd.read_csv(train_nonorm_path)\n",
    "df_training_data_nonorm = df_training_data_nonorm.drop('gene_name', axis=1)\n",
    "\n",
    "nonorm_max = df_training_data_nonorm.max().max()\n",
    "nonorm_min = df_training_data_nonorm.min().min()\n",
    "del df_training_data_nonorm\n",
    "\n",
    "print(nonorm_max)\n",
    "print(nonorm_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene_names_test = pd.read_csv(test_gene_name_path, header=None)\n",
    "df_cell_names_test = pd.read_csv(test_cell_name_path, header=None)\n",
    "df_test_data = pd.read_csv(test_feature_path, header=None)\n",
    "\n",
    "print(df_gene_names_test.shape)\n",
    "print(df_cell_names_test.shape)\n",
    "print(df_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of genes in the input dataset determines the generator output as well as the dicriminator inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = df_gene_names.shape[0]\n",
    "df_gene_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model variables - COMMENT ON EACH ONE TO DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LATENT_VARIABLE_SIZE = 100\n",
    "GEN_L1_DENSE_SIZE = 600\n",
    "GEN_L2_DENSE_SIZE = 600\n",
    "GEN_L3_DENSE_SIZE = num_genes\n",
    "\n",
    "DIS_INPUT_SIZE = num_genes\n",
    "DIS_L1_DENSE_SIZE = 200\n",
    "DIS_L2_DENSE_SIZE = 200\n",
    "\n",
    "NOISE_STDEV = 0.1\n",
    "POISSON_LAM = 1\n",
    "\n",
    "# Training params\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_BUFFER_SIZE = 10000\n",
    "TEST_BATCH_SIZE = 500\n",
    "TEST_BUFFER_SIZE = 500\n",
    "GEN_BATCH_SIZE = 10\n",
    "EPOCHS = 1000\n",
    "\n",
    "EX_GEN_BATCH_SIZE = 500\n",
    "\n",
    "#LEARNING_RATE = 0.001\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test datasets\n",
    "\n",
    "Create tensors from training data - Convert to Int32 for better work on GPU with batch and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(df_training_data.T.values.astype('float32')).shuffle(TRAIN_BUFFER_SIZE).batch(TRAIN_BATCH_SIZE)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(df_test_data.T.values.astype('float32')).shuffle(TEST_BUFFER_SIZE).batch(TEST_BATCH_SIZE)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GANN model\n",
    "\n",
    "Define function for contructing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(GEN_L1_DENSE_SIZE, use_bias=False, input_shape=(LATENT_VARIABLE_SIZE,)))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L1_DENSE_SIZE, 1)  # Note: None is the batch size\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(GEN_L2_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L2_DENSE_SIZE, 1)\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Dense(GEN_L3_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L3_DENSE_SIZE, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for constructing discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(DIS_L1_DENSE_SIZE, use_bias=False, input_shape=(DIS_INPUT_SIZE,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(DIS_L2_DENSE_SIZE, use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(batch_size):\n",
    "    # Create some random noise for the generator\n",
    "    n_noise = tf.random.normal([batch_size, LATENT_VARIABLE_SIZE], mean=0.0, stddev=NOISE_STDEV)\n",
    "    p_noise = tf.random.poisson([batch_size, LATENT_VARIABLE_SIZE], lam=POISSON_LAM)\n",
    "    noise = tf.abs(n_noise + p_noise)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "    #total_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #total_loss = -tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_from_gen(profile, label):\n",
    "    # Create formatted dataframe from generator result\n",
    "    df_gen_prof = pd.DataFrame(generated_profile.numpy()).T\n",
    "    df_gen_prof = df_gene_names.join(df_gen_prof, lsuffix='', rsuffix='', how='inner')\n",
    "    df_gen_prof.index = df_gen_prof.gene_name\n",
    "    df_gen_prof = df_gen_prof.drop('gene_name', axis=1)\n",
    "    df_gen_prof = df_gen_prof.add_prefix(label)\n",
    "\n",
    "    # Get limits\n",
    "    gen_min = df_gen_prof.min().min()\n",
    "    gen_max = df_gen_prof.max().max()\n",
    "\n",
    "    # Scale everything up to 0\n",
    "    df_gen_prof = df_gen_prof + (gen_min*-1)\n",
    "    gen_max = df_gen_prof.max().max()\n",
    "    gen_min = df_gen_prof.min().min()\n",
    "\n",
    "    # Rescale to between real world min maxes\n",
    "    df_gen_prof = df_gen_prof / gen_max\n",
    "    df_gen_prof = df_gen_prof * nonorm_max\n",
    "    \n",
    "    return df_gen_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a batch of real cell profiles from the training set\n",
    "# @tf.function\n",
    "def train_step(cell_profiles):\n",
    "    noise = gen_noise(GEN_BATCH_SIZE)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_profiles = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(cell_profiles, training=True)\n",
    "        fake_output = discriminator(generated_profiles, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        met_gen_loss(gen_loss)\n",
    "        met_disc_loss(disc_loss)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GANN model\n",
    "\n",
    "Create generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from test data to check network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single test set\n",
    "noise = gen_noise(EX_GEN_BATCH_SIZE)\n",
    "generated_profile = generator(noise, training=False)\n",
    "df_gen_prof_1 = data_frame_from_gen(generated_profile, 'gencell_ep0_')\n",
    "\n",
    "# Vis\n",
    "df_gen_prof_1\n",
    "\n",
    "# Save to file\n",
    "#df_gen_prof_1.to_csv(data_path + '/gen_prof_pre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GANN\n",
    "\n",
    "Define tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_gen_loss = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
    "met_disc_loss = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)\n",
    "met_test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create log directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "gen_log_dir = 'logs/gradient_tape/' + current_time + '/gen_train'\n",
    "disc_log_dir = 'logs/gradient_tape/' + current_time + '/disc_train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/disc_test'\n",
    "all_log_dir = 'logs/gradient_tape/' + current_time + '/all'\n",
    "\n",
    "all_summary_writer = tf.summary.create_file_writer(all_log_dir)\n",
    "gen_summary_writer = tf.summary.create_file_writer(gen_log_dir)\n",
    "disc_summary_writer = tf.summary.create_file_writer(disc_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Save checkpoints and gen example data\n",
    "    if epoch % 10 == 0:   \n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # Generate a profile set\n",
    "        noise = gen_noise(EX_GEN_BATCH_SIZE)\n",
    "        generated_profile = generator(noise, training=False)\n",
    "        df_gen_prof = data_frame_from_gen(generated_profile, 'gencell_ep' + str(epoch) + '_')\n",
    "        df_gen_prof.to_csv(data_path + '/gen_prof_' + str(epoch) + '.csv')\n",
    "    \n",
    "    # Logging\n",
    "    start = time.time()\n",
    "    \n",
    "    #Train the epoch\n",
    "    for data_batch in train_dataset:\n",
    "        train_step(data_batch)\n",
    "        \n",
    "    #Run test data through discriminator\n",
    "    for data_batch in test_dataset:\n",
    "        test_decision = discriminator(data_batch, training=False)\n",
    "\n",
    "    test_loss = cross_entropy(tf.ones_like(test_decision), test_decision)\n",
    "    met_test_loss(test_loss)\n",
    "    \n",
    "    #Log metrics\n",
    "    with all_summary_writer.as_default():\n",
    "        tf.summary.scalar('2_gen_loss', met_gen_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_disc_loss', met_disc_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_test_loss', met_test_loss.result(), step=epoch)\n",
    "    \n",
    "    with gen_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_gen_loss.result(), step=epoch)\n",
    "           \n",
    "    with disc_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_disc_loss.result(), step=epoch)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_test_loss.result(), step=epoch)\n",
    "\n",
    "    # Logging\n",
    "    #print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "    time.time()\n",
    "      \n",
    "    #Log stats\n",
    "    template = 'Epoch {}, Gen_loss: {}, Disc_loss: {}, Test_loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                           met_gen_loss.result(), \n",
    "                           met_disc_loss.result(),\n",
    "                           met_test_loss.result()))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    met_gen_loss.reset_states()\n",
    "    met_disc_loss.reset_states()\n",
    "    met_test_loss.reset_states()\n",
    "    \n",
    "# Generate a profile set\n",
    "noise = gen_noise(EX_GEN_BATCH_SIZE)\n",
    "generated_profile = generator(noise, training=False)\n",
    "df_gen_prof = data_frame_from_gen(generated_profile, 'gencell_ep' + str(EPOCHS) + '_')\n",
    "df_gen_prof.to_csv(data_path + '/gen_prof_' + str(EPOCHS) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {train_log_dir} --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
