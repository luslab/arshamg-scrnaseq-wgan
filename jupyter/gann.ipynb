{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN - CHECK SPELLING\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "\n",
    "train_feature_path = data_path + '/tpm_combined.csv'\n",
    "train_gene_name_path = data_path + '/tpm_combined_rows.csv'\n",
    "train_cell_name_path = data_path + '/tpm_combined_cols.csv'\n",
    "\n",
    "test_feature_path = data_path + '/tpm_combined_test.csv'\n",
    "test_gene_name_path = data_path + '/tpm_combined_rows_test.csv'\n",
    "test_cell_name_path = data_path + '/tpm_combined_cols_test.csv'\n",
    "\n",
    "train_nonorm_path = data_path + '/tpm_combined_train_nonorm.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load datasets into frames and check all the shapes match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n",
      "(1798, 1)\n",
      "(6807, 1798)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names = pd.read_csv(train_gene_name_path, header=None)\n",
    "df_cell_names = pd.read_csv(train_cell_name_path, header=None)\n",
    "df_training_data = pd.read_csv(train_feature_path, header=None)\n",
    "\n",
    "df_gene_names.columns = ['gene_name']\n",
    "\n",
    "print(df_gene_names.shape)\n",
    "print(df_cell_names.shape)\n",
    "print(df_training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.828987016884007\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "df_training_data_nonorm = pd.read_csv(train_nonorm_path)\n",
    "df_training_data_nonorm = df_training_data_nonorm.drop('gene_name', axis=1)\n",
    "\n",
    "nonorm_max = df_training_data_nonorm.max().max()\n",
    "nonorm_min = df_training_data_nonorm.min().min()\n",
    "del df_training_data_nonorm\n",
    "\n",
    "print(nonorm_max)\n",
    "print(nonorm_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n",
      "(500, 1)\n",
      "(6807, 500)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names_test = pd.read_csv(test_gene_name_path, header=None)\n",
    "df_cell_names_test = pd.read_csv(test_cell_name_path, header=None)\n",
    "df_test_data = pd.read_csv(test_feature_path, header=None)\n",
    "\n",
    "print(df_gene_names_test.shape)\n",
    "print(df_cell_names_test.shape)\n",
    "print(df_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of genes in the input dataset determines the generator output as well as the dicriminator inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_genes = df_gene_names.shape[0]\n",
    "df_gene_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1788</th>\n",
       "      <th>1789</th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593519</td>\n",
       "      <td>0.724994</td>\n",
       "      <td>0.259323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397072</td>\n",
       "      <td>0.679657</td>\n",
       "      <td>0.541898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567361</td>\n",
       "      <td>0.442711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.529515</td>\n",
       "      <td>0.730789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291240</td>\n",
       "      <td>0.356175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252119</td>\n",
       "      <td>0.065884</td>\n",
       "      <td>0.734733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378425</td>\n",
       "      <td>0.347020</td>\n",
       "      <td>0.812647</td>\n",
       "      <td>0.487845</td>\n",
       "      <td>0.297422</td>\n",
       "      <td>0.516562</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>0.488694</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805992</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.351573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.661508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605841</td>\n",
       "      <td>0.802402</td>\n",
       "      <td>0.681187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.632745</td>\n",
       "      <td>0.442018</td>\n",
       "      <td>0.112515</td>\n",
       "      <td>0.506412</td>\n",
       "      <td>0.657094</td>\n",
       "      <td>0.628180</td>\n",
       "      <td>0.554082</td>\n",
       "      <td>0.738973</td>\n",
       "      <td>0.764555</td>\n",
       "      <td>0.726907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608894</td>\n",
       "      <td>0.586006</td>\n",
       "      <td>0.696437</td>\n",
       "      <td>0.427761</td>\n",
       "      <td>0.681078</td>\n",
       "      <td>0.672670</td>\n",
       "      <td>0.536335</td>\n",
       "      <td>0.451429</td>\n",
       "      <td>0.675192</td>\n",
       "      <td>0.527422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.681206</td>\n",
       "      <td>0.236664</td>\n",
       "      <td>0.428852</td>\n",
       "      <td>0.420482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267296</td>\n",
       "      <td>0.625472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423746</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.611686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>0.395655</td>\n",
       "      <td>0.749851</td>\n",
       "      <td>0.385655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483893</td>\n",
       "      <td>0.697602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.747306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407697</td>\n",
       "      <td>0.830038</td>\n",
       "      <td>0.741090</td>\n",
       "      <td>0.606057</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384173</td>\n",
       "      <td>0.265556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444041</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.557804</td>\n",
       "      <td>0.859781</td>\n",
       "      <td>0.868549</td>\n",
       "      <td>0.625063</td>\n",
       "      <td>0.571023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>0.430313</td>\n",
       "      <td>0.090124</td>\n",
       "      <td>0.605318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520718</td>\n",
       "      <td>0.162545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680744</td>\n",
       "      <td>0.128438</td>\n",
       "      <td>0.432505</td>\n",
       "      <td>0.656399</td>\n",
       "      <td>0.424025</td>\n",
       "      <td>0.275454</td>\n",
       "      <td>0.148875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.621492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.649158</td>\n",
       "      <td>0.808326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491346</td>\n",
       "      <td>0.146496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>0.588273</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.739593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294135</td>\n",
       "      <td>0.808406</td>\n",
       "      <td>0.561273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.639111</td>\n",
       "      <td>0.556518</td>\n",
       "      <td>0.384235</td>\n",
       "      <td>0.255971</td>\n",
       "      <td>0.819070</td>\n",
       "      <td>0.628633</td>\n",
       "      <td>0.633401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6807 rows × 1798 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.593519  0.724994  0.259323  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.529515  0.730789  0.000000  0.291240  0.356175  0.000000  0.000000   \n",
       "2     0.805992  0.015907  0.351573  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.632745  0.442018  0.112515  0.506412  0.657094  0.628180  0.554082   \n",
       "4     0.681206  0.236664  0.428852  0.420482  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6802  0.395655  0.749851  0.385655  0.000000  0.483893  0.697602  0.000000   \n",
       "6803  0.000000  0.384173  0.265556  0.000000  0.000000  0.000000  0.000000   \n",
       "6804  0.430313  0.090124  0.605318  0.000000  0.000000  0.000000  0.000000   \n",
       "6805  0.000000  0.000000  0.000000  0.621492  0.000000  0.649158  0.808326   \n",
       "6806  0.588273  0.442550  0.739593  0.000000  0.000000  0.459884  0.000000   \n",
       "\n",
       "          7         8         9     ...      1788      1789      1790  \\\n",
       "0     0.000000  0.000000  0.000000  ...  0.000000  0.397072  0.679657   \n",
       "1     0.252119  0.065884  0.734733  ...  0.378425  0.347020  0.812647   \n",
       "2     0.000000  0.000000  0.551268  ...  0.000000  0.661508  0.000000   \n",
       "3     0.738973  0.764555  0.726907  ...  0.608894  0.586006  0.696437   \n",
       "4     0.000000  0.267296  0.625472  ...  0.423746  0.111500  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6802  0.000000  0.000000  0.747306  ...  0.000000  0.455800  0.000000   \n",
       "6803  0.000000  0.444041  0.748770  ...  0.000000  0.000000  0.000000   \n",
       "6804  0.000000  0.520718  0.162545  ...  0.000000  0.680744  0.128438   \n",
       "6805  0.000000  0.589434  0.000000  ...  0.625170  0.000000  0.000000   \n",
       "6806  0.294135  0.808406  0.561273  ...  0.000000  0.639111  0.556518   \n",
       "\n",
       "          1791      1792      1793      1794      1795      1796      1797  \n",
       "0     0.541898  0.000000  0.000000  0.567361  0.442711  0.000000  0.000000  \n",
       "1     0.487845  0.297422  0.516562  0.304545  0.488694  0.266055  0.000000  \n",
       "2     0.605841  0.802402  0.681187  0.000000  0.000000  0.000000  0.000000  \n",
       "3     0.427761  0.681078  0.672670  0.536335  0.451429  0.675192  0.527422  \n",
       "4     0.622586  0.000000  0.611686  0.000000  0.502279  0.000000  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6802  0.420597  0.000000  0.407697  0.830038  0.741090  0.606057  0.000000  \n",
       "6803  0.000000  0.000000  0.557804  0.859781  0.868549  0.625063  0.571023  \n",
       "6804  0.432505  0.656399  0.424025  0.275454  0.148875  0.000000  0.000000  \n",
       "6805  0.000000  0.000000  0.491346  0.146496  0.000000  0.000000  0.000000  \n",
       "6806  0.384235  0.255971  0.819070  0.628633  0.633401  0.000000  0.518773  \n",
       "\n",
       "[6807 rows x 1798 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model variables - COMMENT ON EACH ONE TO DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LATENT_VARIABLE_SIZE = 100\n",
    "GEN_L1_DENSE_SIZE = 600\n",
    "GEN_L2_DENSE_SIZE = 600\n",
    "GEN_L3_DENSE_SIZE = num_genes\n",
    "\n",
    "DIS_INPUT_SIZE = num_genes\n",
    "DIS_L1_DENSE_SIZE = 200\n",
    "DIS_L2_DENSE_SIZE = 200\n",
    "\n",
    "NOISE_STDEV = 0.1\n",
    "POISSON_LAM = 1\n",
    "\n",
    "# Training params\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_BUFFER_SIZE = 10000\n",
    "TEST_BATCH_SIZE = 500\n",
    "TEST_BUFFER_SIZE = 500\n",
    "GEN_BATCH_SIZE = 10\n",
    "EPOCHS = 1000\n",
    "\n",
    "EX_GEN_BATCH_SIZE = 500\n",
    "\n",
    "#LEARNING_RATE = 0.001\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test datasets\n",
    "\n",
    "Create tensors from training data - Convert to Int32 for better work on GPU with batch and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 6807), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(df_training_data.T.values.astype('float32')).shuffle(TRAIN_BUFFER_SIZE).batch(TRAIN_BATCH_SIZE)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 6807), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(df_test_data.T.values.astype('float32')).shuffle(TEST_BUFFER_SIZE).batch(TEST_BATCH_SIZE)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GANN model\n",
    "\n",
    "Define function for contructing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(GEN_L1_DENSE_SIZE, use_bias=False, input_shape=(LATENT_VARIABLE_SIZE,)))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L1_DENSE_SIZE, 1)  # Note: None is the batch size\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(GEN_L2_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L2_DENSE_SIZE, 1)\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Dense(GEN_L3_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L3_DENSE_SIZE, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for constructing discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(DIS_L1_DENSE_SIZE, use_bias=False, input_shape=(DIS_INPUT_SIZE,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(DIS_L2_DENSE_SIZE, use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise(batch_size):\n",
    "    # Create some random noise for the generator\n",
    "    n_noise = tf.random.normal([batch_size, LATENT_VARIABLE_SIZE], mean=0.0, stddev=NOISE_STDEV)\n",
    "    p_noise = tf.random.poisson([batch_size, LATENT_VARIABLE_SIZE], lam=POISSON_LAM)\n",
    "    noise = tf.abs(n_noise + p_noise)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "    #total_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #total_loss = -tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_frame_from_gen(profile, label):\n",
    "    # Create formatted dataframe from generator result\n",
    "    df_gen_prof = pd.DataFrame(generated_profile.numpy()).T\n",
    "    df_gen_prof = df_gene_names.join(df_gen_prof, lsuffix='', rsuffix='', how='inner')\n",
    "    df_gen_prof.index = df_gen_prof.gene_name\n",
    "    df_gen_prof = df_gen_prof.drop('gene_name', axis=1)\n",
    "    df_gen_prof = df_gen_prof.add_prefix(label)\n",
    "\n",
    "    # Get limits\n",
    "    gen_min = df_gen_prof.min().min()\n",
    "    gen_max = df_gen_prof.max().max()\n",
    "\n",
    "    # Scale everything up to 0\n",
    "    df_gen_prof = df_gen_prof + (gen_min*-1)\n",
    "    gen_max = df_gen_prof.max().max()\n",
    "    gen_min = df_gen_prof.min().min()\n",
    "\n",
    "    # Rescale to between real world min maxes\n",
    "    df_gen_prof = df_gen_prof / gen_max\n",
    "    df_gen_prof = df_gen_prof * nonorm_max\n",
    "    \n",
    "    return df_gen_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a batch of real cell profiles from the training set\n",
    "# @tf.function\n",
    "def train_step(cell_profiles):\n",
    "    noise = gen_noise(GEN_BATCH_SIZE)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_profiles = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(cell_profiles, training=True)\n",
    "        fake_output = discriminator(generated_profiles, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        met_gen_loss(gen_loss)\n",
    "        met_disc_loss(disc_loss)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GANN model\n",
    "\n",
    "Create generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from test data to check network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gencell_ep0_0</th>\n",
       "      <th>gencell_ep0_1</th>\n",
       "      <th>gencell_ep0_2</th>\n",
       "      <th>gencell_ep0_3</th>\n",
       "      <th>gencell_ep0_4</th>\n",
       "      <th>gencell_ep0_5</th>\n",
       "      <th>gencell_ep0_6</th>\n",
       "      <th>gencell_ep0_7</th>\n",
       "      <th>gencell_ep0_8</th>\n",
       "      <th>gencell_ep0_9</th>\n",
       "      <th>...</th>\n",
       "      <th>gencell_ep0_490</th>\n",
       "      <th>gencell_ep0_491</th>\n",
       "      <th>gencell_ep0_492</th>\n",
       "      <th>gencell_ep0_493</th>\n",
       "      <th>gencell_ep0_494</th>\n",
       "      <th>gencell_ep0_495</th>\n",
       "      <th>gencell_ep0_496</th>\n",
       "      <th>gencell_ep0_497</th>\n",
       "      <th>gencell_ep0_498</th>\n",
       "      <th>gencell_ep0_499</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rfc3</th>\n",
       "      <td>7.200466</td>\n",
       "      <td>5.575809</td>\n",
       "      <td>12.312920</td>\n",
       "      <td>6.961092</td>\n",
       "      <td>5.385093</td>\n",
       "      <td>4.401385</td>\n",
       "      <td>3.806179</td>\n",
       "      <td>3.407456</td>\n",
       "      <td>4.484842</td>\n",
       "      <td>6.373935</td>\n",
       "      <td>...</td>\n",
       "      <td>4.687148</td>\n",
       "      <td>5.135033</td>\n",
       "      <td>4.271653</td>\n",
       "      <td>4.040664</td>\n",
       "      <td>4.002931</td>\n",
       "      <td>6.974885</td>\n",
       "      <td>7.373641</td>\n",
       "      <td>6.313165</td>\n",
       "      <td>4.716501</td>\n",
       "      <td>6.181706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cd47</th>\n",
       "      <td>8.052140</td>\n",
       "      <td>7.339159</td>\n",
       "      <td>8.263481</td>\n",
       "      <td>6.745177</td>\n",
       "      <td>5.466942</td>\n",
       "      <td>6.188019</td>\n",
       "      <td>7.103613</td>\n",
       "      <td>9.825594</td>\n",
       "      <td>8.034950</td>\n",
       "      <td>6.630358</td>\n",
       "      <td>...</td>\n",
       "      <td>7.174108</td>\n",
       "      <td>8.182070</td>\n",
       "      <td>9.142796</td>\n",
       "      <td>8.125615</td>\n",
       "      <td>7.295401</td>\n",
       "      <td>7.466935</td>\n",
       "      <td>7.011714</td>\n",
       "      <td>7.556933</td>\n",
       "      <td>7.607570</td>\n",
       "      <td>5.677191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elmo2</th>\n",
       "      <td>3.237847</td>\n",
       "      <td>3.383047</td>\n",
       "      <td>3.637261</td>\n",
       "      <td>3.051248</td>\n",
       "      <td>5.128806</td>\n",
       "      <td>2.605473</td>\n",
       "      <td>3.961656</td>\n",
       "      <td>4.540101</td>\n",
       "      <td>2.977164</td>\n",
       "      <td>5.189309</td>\n",
       "      <td>...</td>\n",
       "      <td>3.335412</td>\n",
       "      <td>3.036548</td>\n",
       "      <td>4.590815</td>\n",
       "      <td>4.364906</td>\n",
       "      <td>3.108225</td>\n",
       "      <td>3.692504</td>\n",
       "      <td>2.766385</td>\n",
       "      <td>3.138972</td>\n",
       "      <td>3.358127</td>\n",
       "      <td>4.026422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crip2</th>\n",
       "      <td>2.770261</td>\n",
       "      <td>2.115549</td>\n",
       "      <td>3.748849</td>\n",
       "      <td>3.183826</td>\n",
       "      <td>3.041842</td>\n",
       "      <td>2.651016</td>\n",
       "      <td>3.118462</td>\n",
       "      <td>2.995350</td>\n",
       "      <td>2.939400</td>\n",
       "      <td>3.137265</td>\n",
       "      <td>...</td>\n",
       "      <td>3.100469</td>\n",
       "      <td>2.801795</td>\n",
       "      <td>2.937439</td>\n",
       "      <td>3.132435</td>\n",
       "      <td>2.637694</td>\n",
       "      <td>2.486814</td>\n",
       "      <td>2.349024</td>\n",
       "      <td>3.635562</td>\n",
       "      <td>3.221894</td>\n",
       "      <td>3.532440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pprc1</th>\n",
       "      <td>5.994354</td>\n",
       "      <td>5.729683</td>\n",
       "      <td>5.641799</td>\n",
       "      <td>4.600624</td>\n",
       "      <td>2.712652</td>\n",
       "      <td>4.404141</td>\n",
       "      <td>5.381550</td>\n",
       "      <td>5.337330</td>\n",
       "      <td>4.834723</td>\n",
       "      <td>3.129318</td>\n",
       "      <td>...</td>\n",
       "      <td>2.956130</td>\n",
       "      <td>8.722386</td>\n",
       "      <td>3.236373</td>\n",
       "      <td>5.171160</td>\n",
       "      <td>4.491530</td>\n",
       "      <td>4.921404</td>\n",
       "      <td>4.775207</td>\n",
       "      <td>4.837671</td>\n",
       "      <td>3.328127</td>\n",
       "      <td>3.128874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hprt</th>\n",
       "      <td>4.051755</td>\n",
       "      <td>4.328464</td>\n",
       "      <td>2.921821</td>\n",
       "      <td>2.985272</td>\n",
       "      <td>3.123294</td>\n",
       "      <td>3.141180</td>\n",
       "      <td>3.251162</td>\n",
       "      <td>5.207484</td>\n",
       "      <td>3.314775</td>\n",
       "      <td>3.151452</td>\n",
       "      <td>...</td>\n",
       "      <td>3.210772</td>\n",
       "      <td>2.763375</td>\n",
       "      <td>4.190492</td>\n",
       "      <td>2.946907</td>\n",
       "      <td>2.913033</td>\n",
       "      <td>2.883234</td>\n",
       "      <td>2.293713</td>\n",
       "      <td>2.466670</td>\n",
       "      <td>2.934083</td>\n",
       "      <td>3.647069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atraid</th>\n",
       "      <td>5.513589</td>\n",
       "      <td>7.004598</td>\n",
       "      <td>3.636117</td>\n",
       "      <td>5.362860</td>\n",
       "      <td>7.248859</td>\n",
       "      <td>4.891181</td>\n",
       "      <td>3.454343</td>\n",
       "      <td>3.712314</td>\n",
       "      <td>5.088841</td>\n",
       "      <td>3.947650</td>\n",
       "      <td>...</td>\n",
       "      <td>3.895094</td>\n",
       "      <td>6.636585</td>\n",
       "      <td>5.725394</td>\n",
       "      <td>5.100912</td>\n",
       "      <td>3.328512</td>\n",
       "      <td>3.011475</td>\n",
       "      <td>5.242088</td>\n",
       "      <td>5.681060</td>\n",
       "      <td>5.035285</td>\n",
       "      <td>5.009675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chek2</th>\n",
       "      <td>4.779669</td>\n",
       "      <td>3.237050</td>\n",
       "      <td>4.097553</td>\n",
       "      <td>3.271571</td>\n",
       "      <td>3.290487</td>\n",
       "      <td>2.927680</td>\n",
       "      <td>2.616612</td>\n",
       "      <td>2.800702</td>\n",
       "      <td>4.551406</td>\n",
       "      <td>3.382791</td>\n",
       "      <td>...</td>\n",
       "      <td>2.780929</td>\n",
       "      <td>3.065887</td>\n",
       "      <td>2.927438</td>\n",
       "      <td>3.475044</td>\n",
       "      <td>2.579170</td>\n",
       "      <td>2.624908</td>\n",
       "      <td>3.021105</td>\n",
       "      <td>2.682960</td>\n",
       "      <td>2.880622</td>\n",
       "      <td>3.159295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pink1</th>\n",
       "      <td>3.337079</td>\n",
       "      <td>5.231534</td>\n",
       "      <td>3.082321</td>\n",
       "      <td>4.477653</td>\n",
       "      <td>5.481628</td>\n",
       "      <td>2.865719</td>\n",
       "      <td>3.179046</td>\n",
       "      <td>3.526146</td>\n",
       "      <td>6.183165</td>\n",
       "      <td>3.112204</td>\n",
       "      <td>...</td>\n",
       "      <td>4.816295</td>\n",
       "      <td>3.476117</td>\n",
       "      <td>6.467189</td>\n",
       "      <td>2.936959</td>\n",
       "      <td>3.918249</td>\n",
       "      <td>3.364490</td>\n",
       "      <td>5.114668</td>\n",
       "      <td>3.158574</td>\n",
       "      <td>3.159046</td>\n",
       "      <td>4.901445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aff4</th>\n",
       "      <td>3.065546</td>\n",
       "      <td>3.142897</td>\n",
       "      <td>3.084928</td>\n",
       "      <td>3.056261</td>\n",
       "      <td>2.785225</td>\n",
       "      <td>4.889519</td>\n",
       "      <td>3.563194</td>\n",
       "      <td>3.250390</td>\n",
       "      <td>2.775113</td>\n",
       "      <td>2.779677</td>\n",
       "      <td>...</td>\n",
       "      <td>2.745677</td>\n",
       "      <td>3.794436</td>\n",
       "      <td>4.483363</td>\n",
       "      <td>2.505854</td>\n",
       "      <td>2.609856</td>\n",
       "      <td>2.294276</td>\n",
       "      <td>2.921199</td>\n",
       "      <td>4.647150</td>\n",
       "      <td>3.264573</td>\n",
       "      <td>2.279474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6807 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           gencell_ep0_0  gencell_ep0_1  gencell_ep0_2  gencell_ep0_3  \\\n",
       "gene_name                                                               \n",
       "Rfc3            7.200466       5.575809      12.312920       6.961092   \n",
       "Cd47            8.052140       7.339159       8.263481       6.745177   \n",
       "Elmo2           3.237847       3.383047       3.637261       3.051248   \n",
       "Crip2           2.770261       2.115549       3.748849       3.183826   \n",
       "Pprc1           5.994354       5.729683       5.641799       4.600624   \n",
       "...                  ...            ...            ...            ...   \n",
       "Hprt            4.051755       4.328464       2.921821       2.985272   \n",
       "Atraid          5.513589       7.004598       3.636117       5.362860   \n",
       "Chek2           4.779669       3.237050       4.097553       3.271571   \n",
       "Pink1           3.337079       5.231534       3.082321       4.477653   \n",
       "Aff4            3.065546       3.142897       3.084928       3.056261   \n",
       "\n",
       "           gencell_ep0_4  gencell_ep0_5  gencell_ep0_6  gencell_ep0_7  \\\n",
       "gene_name                                                               \n",
       "Rfc3            5.385093       4.401385       3.806179       3.407456   \n",
       "Cd47            5.466942       6.188019       7.103613       9.825594   \n",
       "Elmo2           5.128806       2.605473       3.961656       4.540101   \n",
       "Crip2           3.041842       2.651016       3.118462       2.995350   \n",
       "Pprc1           2.712652       4.404141       5.381550       5.337330   \n",
       "...                  ...            ...            ...            ...   \n",
       "Hprt            3.123294       3.141180       3.251162       5.207484   \n",
       "Atraid          7.248859       4.891181       3.454343       3.712314   \n",
       "Chek2           3.290487       2.927680       2.616612       2.800702   \n",
       "Pink1           5.481628       2.865719       3.179046       3.526146   \n",
       "Aff4            2.785225       4.889519       3.563194       3.250390   \n",
       "\n",
       "           gencell_ep0_8  gencell_ep0_9  ...  gencell_ep0_490  \\\n",
       "gene_name                                ...                    \n",
       "Rfc3            4.484842       6.373935  ...         4.687148   \n",
       "Cd47            8.034950       6.630358  ...         7.174108   \n",
       "Elmo2           2.977164       5.189309  ...         3.335412   \n",
       "Crip2           2.939400       3.137265  ...         3.100469   \n",
       "Pprc1           4.834723       3.129318  ...         2.956130   \n",
       "...                  ...            ...  ...              ...   \n",
       "Hprt            3.314775       3.151452  ...         3.210772   \n",
       "Atraid          5.088841       3.947650  ...         3.895094   \n",
       "Chek2           4.551406       3.382791  ...         2.780929   \n",
       "Pink1           6.183165       3.112204  ...         4.816295   \n",
       "Aff4            2.775113       2.779677  ...         2.745677   \n",
       "\n",
       "           gencell_ep0_491  gencell_ep0_492  gencell_ep0_493  gencell_ep0_494  \\\n",
       "gene_name                                                                       \n",
       "Rfc3              5.135033         4.271653         4.040664         4.002931   \n",
       "Cd47              8.182070         9.142796         8.125615         7.295401   \n",
       "Elmo2             3.036548         4.590815         4.364906         3.108225   \n",
       "Crip2             2.801795         2.937439         3.132435         2.637694   \n",
       "Pprc1             8.722386         3.236373         5.171160         4.491530   \n",
       "...                    ...              ...              ...              ...   \n",
       "Hprt              2.763375         4.190492         2.946907         2.913033   \n",
       "Atraid            6.636585         5.725394         5.100912         3.328512   \n",
       "Chek2             3.065887         2.927438         3.475044         2.579170   \n",
       "Pink1             3.476117         6.467189         2.936959         3.918249   \n",
       "Aff4              3.794436         4.483363         2.505854         2.609856   \n",
       "\n",
       "           gencell_ep0_495  gencell_ep0_496  gencell_ep0_497  gencell_ep0_498  \\\n",
       "gene_name                                                                       \n",
       "Rfc3              6.974885         7.373641         6.313165         4.716501   \n",
       "Cd47              7.466935         7.011714         7.556933         7.607570   \n",
       "Elmo2             3.692504         2.766385         3.138972         3.358127   \n",
       "Crip2             2.486814         2.349024         3.635562         3.221894   \n",
       "Pprc1             4.921404         4.775207         4.837671         3.328127   \n",
       "...                    ...              ...              ...              ...   \n",
       "Hprt              2.883234         2.293713         2.466670         2.934083   \n",
       "Atraid            3.011475         5.242088         5.681060         5.035285   \n",
       "Chek2             2.624908         3.021105         2.682960         2.880622   \n",
       "Pink1             3.364490         5.114668         3.158574         3.159046   \n",
       "Aff4              2.294276         2.921199         4.647150         3.264573   \n",
       "\n",
       "           gencell_ep0_499  \n",
       "gene_name                   \n",
       "Rfc3              6.181706  \n",
       "Cd47              5.677191  \n",
       "Elmo2             4.026422  \n",
       "Crip2             3.532440  \n",
       "Pprc1             3.128874  \n",
       "...                    ...  \n",
       "Hprt              3.647069  \n",
       "Atraid            5.009675  \n",
       "Chek2             3.159295  \n",
       "Pink1             4.901445  \n",
       "Aff4              2.279474  \n",
       "\n",
       "[6807 rows x 500 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a single test set\n",
    "noise = gen_noise(EX_GEN_BATCH_SIZE)\n",
    "generated_profile = generator(noise, training=False)\n",
    "df_gen_prof_1 = data_frame_from_gen(generated_profile, 'gencell_ep0_')\n",
    "\n",
    "# Vis\n",
    "df_gen_prof_1\n",
    "\n",
    "# Save to file\n",
    "#df_gen_prof_1.to_csv(data_path + '/gen_prof_pre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GANN\n",
    "\n",
    "Define tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_gen_loss = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
    "met_disc_loss = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)\n",
    "met_test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create log directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "gen_log_dir = 'logs/gradient_tape/' + current_time + '/gen_train'\n",
    "disc_log_dir = 'logs/gradient_tape/' + current_time + '/disc_train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/disc_test'\n",
    "all_log_dir = 'logs/gradient_tape/' + current_time + '/all'\n",
    "\n",
    "all_summary_writer = tf.summary.create_file_writer(all_log_dir)\n",
    "gen_summary_writer = tf.summary.create_file_writer(gen_log_dir)\n",
    "disc_summary_writer = tf.summary.create_file_writer(disc_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Epoch 1, Gen_loss: 0.23448997735977173, Disc_loss: 1.8714780807495117, Test_loss: 0.17600056529045105\n",
      "Epoch 2, Gen_loss: 0.24080656468868256, Disc_loss: 1.8279056549072266, Test_loss: 0.48494765162467957\n",
      "Epoch 3, Gen_loss: 0.32508137822151184, Disc_loss: 2.1076934337615967, Test_loss: 0.941792905330658\n",
      "Epoch 4, Gen_loss: 0.4572354257106781, Disc_loss: 2.0163793563842773, Test_loss: 0.9000186324119568\n",
      "Epoch 5, Gen_loss: 0.5963371992111206, Disc_loss: 1.6074954271316528, Test_loss: 0.7591283321380615\n",
      "Epoch 6, Gen_loss: 0.5932952761650085, Disc_loss: 1.4658998250961304, Test_loss: 0.5436370372772217\n",
      "Epoch 7, Gen_loss: 0.7057238221168518, Disc_loss: 1.2379900217056274, Test_loss: 0.576341986656189\n",
      "Epoch 8, Gen_loss: 0.8157938718795776, Disc_loss: 1.1121573448181152, Test_loss: 0.511326789855957\n",
      "Epoch 9, Gen_loss: 0.8733988404273987, Disc_loss: 1.1379355192184448, Test_loss: 0.6946274638175964\n",
      "Epoch 10, Gen_loss: 0.8835011124610901, Disc_loss: 1.3008828163146973, Test_loss: 0.7859676480293274\n",
      "Epoch 11, Gen_loss: 1.027350664138794, Disc_loss: 1.2578120231628418, Test_loss: 0.8663756251335144\n",
      "Epoch 12, Gen_loss: 1.0650988817214966, Disc_loss: 1.2714877128601074, Test_loss: 0.9005522131919861\n",
      "Epoch 13, Gen_loss: 1.0373053550720215, Disc_loss: 1.3111770153045654, Test_loss: 0.7910624146461487\n",
      "Epoch 14, Gen_loss: 0.9968617558479309, Disc_loss: 1.2805893421173096, Test_loss: 0.8581416010856628\n",
      "Epoch 15, Gen_loss: 0.9669559001922607, Disc_loss: 1.340019702911377, Test_loss: 0.870306134223938\n",
      "Epoch 16, Gen_loss: 0.9584289789199829, Disc_loss: 1.2938477993011475, Test_loss: 0.7597655057907104\n",
      "Epoch 17, Gen_loss: 1.02364981174469, Disc_loss: 1.10442054271698, Test_loss: 0.6240925788879395\n",
      "Epoch 18, Gen_loss: 0.969365119934082, Disc_loss: 1.1337231397628784, Test_loss: 0.6571809649467468\n",
      "Epoch 19, Gen_loss: 0.8989136815071106, Disc_loss: 1.2087534666061401, Test_loss: 0.7461167573928833\n",
      "Epoch 20, Gen_loss: 0.9031229615211487, Disc_loss: 1.307450294494629, Test_loss: 0.7781453728675842\n",
      "Epoch 21, Gen_loss: 0.9640294313430786, Disc_loss: 1.233667254447937, Test_loss: 0.7412211894989014\n",
      "Epoch 22, Gen_loss: 0.9423105120658875, Disc_loss: 1.2772005796432495, Test_loss: 0.8086599111557007\n",
      "Epoch 23, Gen_loss: 0.8983395099639893, Disc_loss: 1.346980333328247, Test_loss: 0.7437662482261658\n",
      "Epoch 24, Gen_loss: 0.9160286784172058, Disc_loss: 1.2718174457550049, Test_loss: 0.7529724836349487\n",
      "Epoch 25, Gen_loss: 0.8561987280845642, Disc_loss: 1.28182053565979, Test_loss: 0.7246216535568237\n",
      "Epoch 26, Gen_loss: 0.8357201218605042, Disc_loss: 1.2573645114898682, Test_loss: 0.6524930000305176\n",
      "Epoch 27, Gen_loss: 0.8626976013183594, Disc_loss: 1.189576268196106, Test_loss: 0.6768640279769897\n",
      "Epoch 28, Gen_loss: 0.7865518927574158, Disc_loss: 1.3026528358459473, Test_loss: 0.7044165134429932\n",
      "Epoch 29, Gen_loss: 0.7652925848960876, Disc_loss: 1.3368028402328491, Test_loss: 0.7269764542579651\n",
      "Epoch 30, Gen_loss: 0.8154988884925842, Disc_loss: 1.3076918125152588, Test_loss: 0.7420191168785095\n",
      "Epoch 31, Gen_loss: 0.7821183800697327, Disc_loss: 1.3556245565414429, Test_loss: 0.7569586634635925\n",
      "Epoch 32, Gen_loss: 0.7587427496910095, Disc_loss: 1.395654320716858, Test_loss: 0.7761896252632141\n",
      "Epoch 33, Gen_loss: 0.7931389808654785, Disc_loss: 1.3159582614898682, Test_loss: 0.6888446807861328\n",
      "Epoch 34, Gen_loss: 0.7839140295982361, Disc_loss: 1.3074181079864502, Test_loss: 0.7253310084342957\n",
      "Epoch 35, Gen_loss: 0.7330003380775452, Disc_loss: 1.3937405347824097, Test_loss: 0.7832805514335632\n",
      "Epoch 36, Gen_loss: 0.7312178015708923, Disc_loss: 1.4285500049591064, Test_loss: 0.7246426343917847\n",
      "Epoch 37, Gen_loss: 0.7954957485198975, Disc_loss: 1.3292192220687866, Test_loss: 0.7749115824699402\n",
      "Epoch 38, Gen_loss: 0.7598348259925842, Disc_loss: 1.3847118616104126, Test_loss: 0.7303645610809326\n",
      "Epoch 39, Gen_loss: 0.7614539861679077, Disc_loss: 1.4012737274169922, Test_loss: 0.7779181599617004\n",
      "Epoch 40, Gen_loss: 0.7718080878257751, Disc_loss: 1.3717931509017944, Test_loss: 0.7205182909965515\n",
      "Epoch 41, Gen_loss: 0.7718825340270996, Disc_loss: 1.3110531568527222, Test_loss: 0.7097219824790955\n",
      "Epoch 42, Gen_loss: 0.7319942712783813, Disc_loss: 1.3851943016052246, Test_loss: 0.7530306577682495\n",
      "Epoch 43, Gen_loss: 0.7205548882484436, Disc_loss: 1.3895299434661865, Test_loss: 0.7545259594917297\n",
      "Epoch 44, Gen_loss: 0.7289826273918152, Disc_loss: 1.4071738719940186, Test_loss: 0.7797635793685913\n",
      "Epoch 45, Gen_loss: 0.7378360033035278, Disc_loss: 1.3904407024383545, Test_loss: 0.7671458125114441\n",
      "Epoch 46, Gen_loss: 0.7022353410720825, Disc_loss: 1.4517008066177368, Test_loss: 0.8143349289894104\n",
      "Epoch 47, Gen_loss: 0.7433614730834961, Disc_loss: 1.4099832773208618, Test_loss: 0.6774306893348694\n",
      "Epoch 48, Gen_loss: 0.7502943873405457, Disc_loss: 1.3813748359680176, Test_loss: 0.7918329238891602\n",
      "Epoch 49, Gen_loss: 0.7161132097244263, Disc_loss: 1.4654308557510376, Test_loss: 0.7680153250694275\n",
      "Epoch 50, Gen_loss: 0.746561586856842, Disc_loss: 1.3600800037384033, Test_loss: 0.7143445014953613\n",
      "Epoch 51, Gen_loss: 0.7037690281867981, Disc_loss: 1.4114946126937866, Test_loss: 0.7210128307342529\n",
      "Epoch 52, Gen_loss: 0.698357880115509, Disc_loss: 1.4094104766845703, Test_loss: 0.7555621266365051\n",
      "Epoch 53, Gen_loss: 0.7212880253791809, Disc_loss: 1.3774075508117676, Test_loss: 0.7397621870040894\n",
      "Epoch 54, Gen_loss: 0.6902801990509033, Disc_loss: 1.4529454708099365, Test_loss: 0.799972653388977\n",
      "Epoch 55, Gen_loss: 0.7177459001541138, Disc_loss: 1.4460545778274536, Test_loss: 0.7381476163864136\n",
      "Epoch 56, Gen_loss: 0.7384025454521179, Disc_loss: 1.3661552667617798, Test_loss: 0.744911789894104\n",
      "Epoch 57, Gen_loss: 0.690525472164154, Disc_loss: 1.4093852043151855, Test_loss: 0.7169265747070312\n",
      "Epoch 58, Gen_loss: 0.6915372610092163, Disc_loss: 1.4197784662246704, Test_loss: 0.7218803763389587\n",
      "Epoch 59, Gen_loss: 0.694465696811676, Disc_loss: 1.4378838539123535, Test_loss: 0.771073579788208\n",
      "Epoch 60, Gen_loss: 0.7181344032287598, Disc_loss: 1.3763601779937744, Test_loss: 0.6777544021606445\n",
      "Epoch 61, Gen_loss: 0.7007334232330322, Disc_loss: 1.4023432731628418, Test_loss: 0.7342142462730408\n",
      "Epoch 62, Gen_loss: 0.6781838536262512, Disc_loss: 1.435756802558899, Test_loss: 0.7387646436691284\n",
      "Epoch 63, Gen_loss: 0.6878255009651184, Disc_loss: 1.43430495262146, Test_loss: 0.71697998046875\n",
      "Epoch 64, Gen_loss: 0.7081810235977173, Disc_loss: 1.4048495292663574, Test_loss: 0.7377439737319946\n",
      "Epoch 65, Gen_loss: 0.6964316368103027, Disc_loss: 1.414649486541748, Test_loss: 0.7705141305923462\n",
      "Epoch 66, Gen_loss: 0.6805665493011475, Disc_loss: 1.436389446258545, Test_loss: 0.7775060534477234\n",
      "Epoch 67, Gen_loss: 0.7149242162704468, Disc_loss: 1.40225088596344, Test_loss: 0.7017470598220825\n",
      "Epoch 68, Gen_loss: 0.6780128479003906, Disc_loss: 1.413705587387085, Test_loss: 0.699613094329834\n",
      "Epoch 69, Gen_loss: 0.6806890368461609, Disc_loss: 1.4321560859680176, Test_loss: 0.753110945224762\n",
      "Epoch 70, Gen_loss: 0.6722425818443298, Disc_loss: 1.4283404350280762, Test_loss: 0.7683802247047424\n",
      "Epoch 71, Gen_loss: 0.7028337717056274, Disc_loss: 1.4061872959136963, Test_loss: 0.7446102499961853\n",
      "Epoch 72, Gen_loss: 0.681038498878479, Disc_loss: 1.444225788116455, Test_loss: 0.785129189491272\n",
      "Epoch 73, Gen_loss: 0.7049625515937805, Disc_loss: 1.4041061401367188, Test_loss: 0.7641200423240662\n",
      "Epoch 74, Gen_loss: 0.6786538362503052, Disc_loss: 1.4285495281219482, Test_loss: 0.7111644148826599\n",
      "Epoch 75, Gen_loss: 0.6840823888778687, Disc_loss: 1.4231719970703125, Test_loss: 0.7302630543708801\n",
      "Epoch 76, Gen_loss: 0.6733551621437073, Disc_loss: 1.4386652708053589, Test_loss: 0.7006514668464661\n",
      "Epoch 77, Gen_loss: 0.6928516626358032, Disc_loss: 1.397070288658142, Test_loss: 0.7105364799499512\n",
      "Epoch 78, Gen_loss: 0.6617358922958374, Disc_loss: 1.4524781703948975, Test_loss: 0.8063012957572937\n",
      "Epoch 79, Gen_loss: 0.6871649622917175, Disc_loss: 1.401806354522705, Test_loss: 0.6643485426902771\n",
      "Epoch 80, Gen_loss: 0.6748719215393066, Disc_loss: 1.4094709157943726, Test_loss: 0.7030124664306641\n",
      "Epoch 81, Gen_loss: 0.6698948740959167, Disc_loss: 1.4314574003219604, Test_loss: 0.6908496618270874\n",
      "Epoch 82, Gen_loss: 0.6917730569839478, Disc_loss: 1.407547116279602, Test_loss: 0.7407698631286621\n",
      "Epoch 83, Gen_loss: 0.6682462692260742, Disc_loss: 1.422675371170044, Test_loss: 0.7123086452484131\n",
      "Epoch 84, Gen_loss: 0.6853489875793457, Disc_loss: 1.410295009613037, Test_loss: 0.7037501335144043\n",
      "Epoch 85, Gen_loss: 0.6531789302825928, Disc_loss: 1.424978494644165, Test_loss: 0.7395117878913879\n",
      "Epoch 86, Gen_loss: 0.6712081432342529, Disc_loss: 1.436036229133606, Test_loss: 0.7274486422538757\n",
      "Epoch 87, Gen_loss: 0.6891142725944519, Disc_loss: 1.408693790435791, Test_loss: 0.6580524444580078\n",
      "Epoch 88, Gen_loss: 0.6915575265884399, Disc_loss: 1.3992596864700317, Test_loss: 0.6767451167106628\n",
      "Epoch 89, Gen_loss: 0.660420835018158, Disc_loss: 1.4091264009475708, Test_loss: 0.6910344958305359\n",
      "Epoch 90, Gen_loss: 0.6695759296417236, Disc_loss: 1.4141392707824707, Test_loss: 0.6644499897956848\n",
      "Epoch 91, Gen_loss: 0.6675354242324829, Disc_loss: 1.431485652923584, Test_loss: 0.7656334042549133\n",
      "Epoch 92, Gen_loss: 0.6836727857589722, Disc_loss: 1.417232632637024, Test_loss: 0.7387136816978455\n",
      "Epoch 93, Gen_loss: 0.6852408647537231, Disc_loss: 1.4085899591445923, Test_loss: 0.7381414771080017\n",
      "Epoch 94, Gen_loss: 0.667772650718689, Disc_loss: 1.4162566661834717, Test_loss: 0.7017076015472412\n",
      "Epoch 95, Gen_loss: 0.6836761236190796, Disc_loss: 1.4055638313293457, Test_loss: 0.6843435764312744\n",
      "Epoch 96, Gen_loss: 0.661115825176239, Disc_loss: 1.420230746269226, Test_loss: 0.6822395920753479\n",
      "Epoch 97, Gen_loss: 0.6811758875846863, Disc_loss: 1.4124951362609863, Test_loss: 0.685736894607544\n",
      "Epoch 98, Gen_loss: 0.6713298559188843, Disc_loss: 1.4151923656463623, Test_loss: 0.6903082728385925\n",
      "Epoch 99, Gen_loss: 0.6887471675872803, Disc_loss: 1.4015089273452759, Test_loss: 0.6548166275024414\n",
      "Epoch 100, Gen_loss: 0.6592203378677368, Disc_loss: 1.4231938123703003, Test_loss: 0.6710917353630066\n",
      "Epoch 101, Gen_loss: 0.6802007555961609, Disc_loss: 1.3926628828048706, Test_loss: 0.6761366724967957\n",
      "Epoch 102, Gen_loss: 0.6754521131515503, Disc_loss: 1.3955631256103516, Test_loss: 0.6862465143203735\n",
      "Epoch 103, Gen_loss: 0.6675487756729126, Disc_loss: 1.4403948783874512, Test_loss: 0.666041910648346\n",
      "Epoch 104, Gen_loss: 0.6673859357833862, Disc_loss: 1.420634150505066, Test_loss: 0.7303204536437988\n",
      "Epoch 105, Gen_loss: 0.6779005527496338, Disc_loss: 1.4179706573486328, Test_loss: 0.6557021141052246\n",
      "Epoch 106, Gen_loss: 0.6701275110244751, Disc_loss: 1.4175076484680176, Test_loss: 0.6944045424461365\n",
      "Epoch 107, Gen_loss: 0.688184916973114, Disc_loss: 1.4054734706878662, Test_loss: 0.7392090559005737\n",
      "Epoch 108, Gen_loss: 0.6652417778968811, Disc_loss: 1.4101840257644653, Test_loss: 0.707160234451294\n",
      "Epoch 109, Gen_loss: 0.6785851120948792, Disc_loss: 1.4163782596588135, Test_loss: 0.6882094144821167\n",
      "Epoch 110, Gen_loss: 0.6792968511581421, Disc_loss: 1.4228854179382324, Test_loss: 0.7996570467948914\n",
      "Epoch 111, Gen_loss: 0.6926469802856445, Disc_loss: 1.4146091938018799, Test_loss: 0.7242075800895691\n",
      "Epoch 112, Gen_loss: 0.671074390411377, Disc_loss: 1.3960431814193726, Test_loss: 0.7159464359283447\n",
      "Epoch 113, Gen_loss: 0.6687399744987488, Disc_loss: 1.425979733467102, Test_loss: 0.7425258159637451\n",
      "Epoch 114, Gen_loss: 0.6748157143592834, Disc_loss: 1.3997340202331543, Test_loss: 0.7712494134902954\n",
      "Epoch 115, Gen_loss: 0.6716753244400024, Disc_loss: 1.407137155532837, Test_loss: 0.729518711566925\n",
      "Epoch 116, Gen_loss: 0.6745675206184387, Disc_loss: 1.4336572885513306, Test_loss: 0.707891583442688\n",
      "Epoch 117, Gen_loss: 0.6980544328689575, Disc_loss: 1.38356614112854, Test_loss: 0.7259092330932617\n",
      "Epoch 118, Gen_loss: 0.685498058795929, Disc_loss: 1.4084746837615967, Test_loss: 0.7434009313583374\n",
      "Epoch 119, Gen_loss: 0.6797295212745667, Disc_loss: 1.404294729232788, Test_loss: 0.6410380601882935\n",
      "Epoch 120, Gen_loss: 0.6604257822036743, Disc_loss: 1.3943846225738525, Test_loss: 0.7297900915145874\n",
      "Epoch 121, Gen_loss: 0.6770334839820862, Disc_loss: 1.4108428955078125, Test_loss: 0.7213340401649475\n",
      "Epoch 122, Gen_loss: 0.6926711797714233, Disc_loss: 1.3666980266571045, Test_loss: 0.6812279224395752\n",
      "Epoch 123, Gen_loss: 0.6562709212303162, Disc_loss: 1.407141923904419, Test_loss: 0.7072543501853943\n",
      "Epoch 124, Gen_loss: 0.6795240044593811, Disc_loss: 1.3696740865707397, Test_loss: 0.6515529751777649\n",
      "Epoch 125, Gen_loss: 0.6851211190223694, Disc_loss: 1.368085503578186, Test_loss: 0.6732842326164246\n",
      "Epoch 126, Gen_loss: 0.6903111934661865, Disc_loss: 1.3512142896652222, Test_loss: 0.6261804699897766\n",
      "Epoch 127, Gen_loss: 0.6674269437789917, Disc_loss: 1.331932544708252, Test_loss: 0.6048109531402588\n",
      "Epoch 128, Gen_loss: 0.6857756972312927, Disc_loss: 1.2979228496551514, Test_loss: 0.5627733469009399\n",
      "Epoch 129, Gen_loss: 0.6625012159347534, Disc_loss: 1.2711905241012573, Test_loss: 0.5437722206115723\n",
      "Epoch 130, Gen_loss: 0.679719865322113, Disc_loss: 1.2403146028518677, Test_loss: 0.5487679243087769\n",
      "Epoch 131, Gen_loss: 0.7012781500816345, Disc_loss: 1.144480586051941, Test_loss: 0.4206753373146057\n",
      "Epoch 132, Gen_loss: 0.6028991341590881, Disc_loss: 1.3547786474227905, Test_loss: 0.5855152010917664\n",
      "Epoch 133, Gen_loss: 0.7217633724212646, Disc_loss: 1.2663918733596802, Test_loss: 0.6100142598152161\n",
      "Epoch 134, Gen_loss: 0.7260816097259521, Disc_loss: 1.2015010118484497, Test_loss: 0.530532717704773\n",
      "Epoch 135, Gen_loss: 0.6718915104866028, Disc_loss: 1.1172254085540771, Test_loss: 0.3051109313964844\n",
      "Epoch 136, Gen_loss: 0.6510744690895081, Disc_loss: 1.3494033813476562, Test_loss: 0.7067902684211731\n",
      "Epoch 137, Gen_loss: 0.7496492862701416, Disc_loss: 1.42563796043396, Test_loss: 0.6150792241096497\n",
      "Epoch 138, Gen_loss: 0.7480334639549255, Disc_loss: 1.2652263641357422, Test_loss: 0.6228389143943787\n",
      "Epoch 139, Gen_loss: 0.7019302845001221, Disc_loss: 1.204255223274231, Test_loss: 0.5096863508224487\n",
      "Epoch 140, Gen_loss: 0.7081025242805481, Disc_loss: 1.2121636867523193, Test_loss: 0.5957956910133362\n",
      "Epoch 141, Gen_loss: 0.7772185206413269, Disc_loss: 1.1942994594573975, Test_loss: 0.5693762898445129\n",
      "Epoch 142, Gen_loss: 0.7486472129821777, Disc_loss: 1.1925235986709595, Test_loss: 0.47666797041893005\n",
      "Epoch 143, Gen_loss: 0.7138195633888245, Disc_loss: 1.1468490362167358, Test_loss: 0.5484277009963989\n",
      "Epoch 144, Gen_loss: 0.6145964860916138, Disc_loss: 1.2969928979873657, Test_loss: 0.5367780923843384\n",
      "Epoch 145, Gen_loss: 0.7544824481010437, Disc_loss: 1.160723090171814, Test_loss: 0.4616738259792328\n",
      "Epoch 146, Gen_loss: 0.7235149145126343, Disc_loss: 1.191545009613037, Test_loss: 0.40559399127960205\n",
      "Epoch 147, Gen_loss: 0.6849890351295471, Disc_loss: 0.8561860918998718, Test_loss: 0.048856448382139206\n",
      "Epoch 148, Gen_loss: 0.5759336948394775, Disc_loss: 1.162956953048706, Test_loss: 0.6224278807640076\n",
      "Epoch 149, Gen_loss: 0.7297332882881165, Disc_loss: 1.3638911247253418, Test_loss: 0.6113704442977905\n",
      "Epoch 150, Gen_loss: 0.7544680833816528, Disc_loss: 1.210741400718689, Test_loss: 0.74261075258255\n",
      "Epoch 151, Gen_loss: 0.8828303217887878, Disc_loss: 1.2359576225280762, Test_loss: 0.48121407628059387\n",
      "Epoch 152, Gen_loss: 0.7765629291534424, Disc_loss: 1.0503649711608887, Test_loss: 0.41892895102500916\n",
      "Epoch 153, Gen_loss: 0.6224179863929749, Disc_loss: 1.2467658519744873, Test_loss: 0.5893604755401611\n",
      "Epoch 154, Gen_loss: 0.7799762487411499, Disc_loss: 1.1226743459701538, Test_loss: 0.43709421157836914\n",
      "Epoch 155, Gen_loss: 0.7419672608375549, Disc_loss: 1.1047004461288452, Test_loss: 0.5960119962692261\n",
      "Epoch 156, Gen_loss: 0.8308278322219849, Disc_loss: 1.232344627380371, Test_loss: 0.5434962511062622\n",
      "Epoch 157, Gen_loss: 0.7299832105636597, Disc_loss: 1.070616602897644, Test_loss: 0.37630465626716614\n",
      "Epoch 158, Gen_loss: 0.6229787468910217, Disc_loss: 1.0811456441879272, Test_loss: 0.33925172686576843\n",
      "Epoch 159, Gen_loss: 0.7915112972259521, Disc_loss: 1.1281993389129639, Test_loss: 0.7035619616508484\n",
      "Epoch 160, Gen_loss: 0.8795296549797058, Disc_loss: 1.0799739360809326, Test_loss: 0.44979384541511536\n",
      "Epoch 161, Gen_loss: 0.6514529585838318, Disc_loss: 1.1693817377090454, Test_loss: 0.3675733208656311\n",
      "Epoch 162, Gen_loss: 0.6599748730659485, Disc_loss: 1.205452799797058, Test_loss: 0.49140459299087524\n",
      "Epoch 163, Gen_loss: 0.7851737141609192, Disc_loss: 1.04763662815094, Test_loss: 0.3454417884349823\n",
      "Epoch 164, Gen_loss: 0.7351256608963013, Disc_loss: 1.1092694997787476, Test_loss: 0.8003415465354919\n",
      "Epoch 165, Gen_loss: 0.880872368812561, Disc_loss: 1.262162446975708, Test_loss: 0.46825578808784485\n",
      "Epoch 166, Gen_loss: 0.665103018283844, Disc_loss: 1.09142005443573, Test_loss: 0.3195554316043854\n",
      "Epoch 167, Gen_loss: 0.6499668955802917, Disc_loss: 1.227142572402954, Test_loss: 0.532951831817627\n",
      "Epoch 168, Gen_loss: 0.8169847130775452, Disc_loss: 1.0604902505874634, Test_loss: 0.43720611929893494\n",
      "Epoch 169, Gen_loss: 0.8620754480361938, Disc_loss: 1.2493282556533813, Test_loss: 0.8452541828155518\n",
      "Epoch 170, Gen_loss: 0.8370435237884521, Disc_loss: 1.1177700757980347, Test_loss: 0.4570365846157074\n",
      "Epoch 171, Gen_loss: 0.7518566846847534, Disc_loss: 1.0536744594573975, Test_loss: 0.39177560806274414\n",
      "Epoch 172, Gen_loss: 0.6448974013328552, Disc_loss: 1.0812174081802368, Test_loss: 0.43378397822380066\n",
      "Epoch 173, Gen_loss: 0.7309485673904419, Disc_loss: 1.080535888671875, Test_loss: 0.5091606974601746\n",
      "Epoch 174, Gen_loss: 0.7593069076538086, Disc_loss: 1.0628077983856201, Test_loss: 0.38150861859321594\n",
      "Epoch 175, Gen_loss: 0.7774583697319031, Disc_loss: 0.9981956481933594, Test_loss: 0.3783344030380249\n",
      "Epoch 176, Gen_loss: 0.7415106892585754, Disc_loss: 0.9642836451530457, Test_loss: 0.29708021879196167\n",
      "Epoch 177, Gen_loss: 0.7223979234695435, Disc_loss: 1.1103341579437256, Test_loss: 0.6828197836875916\n",
      "Epoch 178, Gen_loss: 0.7405839562416077, Disc_loss: 1.169670820236206, Test_loss: 0.44091928005218506\n",
      "Epoch 179, Gen_loss: 0.7606185674667358, Disc_loss: 1.0336744785308838, Test_loss: 0.36693400144577026\n",
      "Epoch 180, Gen_loss: 0.6844371557235718, Disc_loss: 1.040151834487915, Test_loss: 0.4587768018245697\n",
      "Epoch 181, Gen_loss: 0.7732730507850647, Disc_loss: 1.0883253812789917, Test_loss: 0.5134179592132568\n",
      "Epoch 182, Gen_loss: 0.7930241227149963, Disc_loss: 1.025076985359192, Test_loss: 0.37628573179244995\n",
      "Epoch 183, Gen_loss: 0.7386718988418579, Disc_loss: 1.0088906288146973, Test_loss: 0.330035924911499\n",
      "Epoch 184, Gen_loss: 0.7063650488853455, Disc_loss: 0.9672815799713135, Test_loss: 0.29567670822143555\n",
      "Epoch 185, Gen_loss: 0.760909378528595, Disc_loss: 1.1985682249069214, Test_loss: 0.5122172236442566\n",
      "Epoch 186, Gen_loss: 0.7768241763114929, Disc_loss: 1.0648585557937622, Test_loss: 0.46143466234207153\n",
      "Epoch 187, Gen_loss: 0.7831369042396545, Disc_loss: 1.0792896747589111, Test_loss: 0.5402917265892029\n",
      "Epoch 188, Gen_loss: 0.7090790867805481, Disc_loss: 1.0882936716079712, Test_loss: 0.34445133805274963\n",
      "Epoch 189, Gen_loss: 0.7240222692489624, Disc_loss: 1.0854589939117432, Test_loss: 0.44859424233436584\n",
      "Epoch 190, Gen_loss: 0.7872178554534912, Disc_loss: 1.0610167980194092, Test_loss: 0.4409946799278259\n",
      "Epoch 191, Gen_loss: 0.7764254212379456, Disc_loss: 1.020731806755066, Test_loss: 0.4625818133354187\n",
      "Epoch 192, Gen_loss: 0.8872965574264526, Disc_loss: 1.1806806325912476, Test_loss: 0.5614126324653625\n",
      "Epoch 193, Gen_loss: 0.7930105328559875, Disc_loss: 1.075116515159607, Test_loss: 0.43171510100364685\n",
      "Epoch 194, Gen_loss: 0.7652132511138916, Disc_loss: 1.0749505758285522, Test_loss: 0.4546423852443695\n",
      "Epoch 195, Gen_loss: 0.66969895362854, Disc_loss: 1.1270294189453125, Test_loss: 0.4719099998474121\n",
      "Epoch 196, Gen_loss: 0.7908700108528137, Disc_loss: 1.100029468536377, Test_loss: 0.386989027261734\n",
      "Epoch 197, Gen_loss: 0.8332619667053223, Disc_loss: 0.9696573615074158, Test_loss: 0.3299938142299652\n",
      "Epoch 198, Gen_loss: 0.7710232138633728, Disc_loss: 0.9842714667320251, Test_loss: 0.3737640976905823\n",
      "Epoch 199, Gen_loss: 0.7408768534660339, Disc_loss: 0.9819811582565308, Test_loss: 0.32852640748023987\n",
      "Epoch 200, Gen_loss: 0.7244686484336853, Disc_loss: 0.9739118218421936, Test_loss: 0.307547926902771\n",
      "Epoch 201, Gen_loss: 0.7564398050308228, Disc_loss: 0.9424784183502197, Test_loss: 0.3374990224838257\n",
      "Epoch 202, Gen_loss: 0.7399007081985474, Disc_loss: 1.0594098567962646, Test_loss: 0.4677918255329132\n",
      "Epoch 203, Gen_loss: 0.8002999424934387, Disc_loss: 1.0384608507156372, Test_loss: 0.4183137118816376\n",
      "Epoch 204, Gen_loss: 0.8379485011100769, Disc_loss: 0.958012044429779, Test_loss: 0.4355160593986511\n",
      "Epoch 205, Gen_loss: 0.7234677672386169, Disc_loss: 1.0873167514801025, Test_loss: 0.545439600944519\n",
      "Epoch 206, Gen_loss: 0.8493512272834778, Disc_loss: 1.195434331893921, Test_loss: 0.5715981125831604\n",
      "Epoch 207, Gen_loss: 0.8237777948379517, Disc_loss: 1.0371345281600952, Test_loss: 0.5015986561775208\n",
      "Epoch 208, Gen_loss: 0.8304668664932251, Disc_loss: 1.0720754861831665, Test_loss: 0.5320572853088379\n",
      "Epoch 209, Gen_loss: 0.8277238011360168, Disc_loss: 1.06197988986969, Test_loss: 0.48043307662010193\n",
      "Epoch 210, Gen_loss: 0.8114306330680847, Disc_loss: 1.065764307975769, Test_loss: 0.5375509262084961\n",
      "Epoch 211, Gen_loss: 0.7542060613632202, Disc_loss: 1.1176278591156006, Test_loss: 0.4760276675224304\n",
      "Epoch 212, Gen_loss: 0.8101457953453064, Disc_loss: 1.0649479627609253, Test_loss: 0.5101705193519592\n",
      "Epoch 213, Gen_loss: 0.8132850527763367, Disc_loss: 1.0537928342819214, Test_loss: 0.5257959961891174\n",
      "Epoch 214, Gen_loss: 0.7960618734359741, Disc_loss: 1.0755501985549927, Test_loss: 0.5188698172569275\n",
      "Epoch 215, Gen_loss: 0.7654827833175659, Disc_loss: 1.0939139127731323, Test_loss: 0.517529308795929\n",
      "Epoch 216, Gen_loss: 0.6927622556686401, Disc_loss: 1.2086800336837769, Test_loss: 0.5276506543159485\n",
      "Epoch 217, Gen_loss: 0.8028692603111267, Disc_loss: 1.1264758110046387, Test_loss: 0.49111229181289673\n",
      "Epoch 218, Gen_loss: 0.8269853591918945, Disc_loss: 0.9876287579536438, Test_loss: 0.38787609338760376\n",
      "Epoch 219, Gen_loss: 0.7926574945449829, Disc_loss: 0.990169882774353, Test_loss: 0.42568784952163696\n",
      "Epoch 220, Gen_loss: 0.7880321145057678, Disc_loss: 1.0165704488754272, Test_loss: 0.36785247921943665\n",
      "Epoch 221, Gen_loss: 0.7857218980789185, Disc_loss: 0.9860250949859619, Test_loss: 0.4709647297859192\n",
      "Epoch 222, Gen_loss: 0.790858805179596, Disc_loss: 1.0001063346862793, Test_loss: 0.3884791135787964\n",
      "Epoch 223, Gen_loss: 0.7934582233428955, Disc_loss: 1.2924597263336182, Test_loss: 0.8439503312110901\n",
      "Epoch 224, Gen_loss: 0.9050843119621277, Disc_loss: 1.1195718050003052, Test_loss: 0.5126556158065796\n",
      "Epoch 225, Gen_loss: 0.8285070061683655, Disc_loss: 1.023851752281189, Test_loss: 0.49991631507873535\n",
      "Epoch 226, Gen_loss: 0.7951969504356384, Disc_loss: 1.0989867448806763, Test_loss: 0.5266861319541931\n",
      "Epoch 227, Gen_loss: 0.7997926473617554, Disc_loss: 1.0811041593551636, Test_loss: 0.47612982988357544\n",
      "Epoch 228, Gen_loss: 0.8348797559738159, Disc_loss: 1.0415953397750854, Test_loss: 0.5565112829208374\n",
      "Epoch 229, Gen_loss: 0.8047645688056946, Disc_loss: 1.069412350654602, Test_loss: 0.5227218270301819\n",
      "Epoch 230, Gen_loss: 0.8011088967323303, Disc_loss: 1.0544177293777466, Test_loss: 0.5126953125\n",
      "Epoch 231, Gen_loss: 0.766825258731842, Disc_loss: 1.068864345550537, Test_loss: 0.49135878682136536\n",
      "Epoch 232, Gen_loss: 0.7927615642547607, Disc_loss: 1.080478549003601, Test_loss: 0.46618983149528503\n",
      "Epoch 233, Gen_loss: 0.7789286971092224, Disc_loss: 1.0618551969528198, Test_loss: 0.463445246219635\n",
      "Epoch 234, Gen_loss: 0.699593186378479, Disc_loss: 1.3040497303009033, Test_loss: 0.5312691330909729\n",
      "Epoch 235, Gen_loss: 0.8269805908203125, Disc_loss: 1.0512504577636719, Test_loss: 0.45377397537231445\n",
      "Epoch 236, Gen_loss: 0.8087593913078308, Disc_loss: 1.0184872150421143, Test_loss: 0.4309611916542053\n",
      "Epoch 237, Gen_loss: 0.8116885423660278, Disc_loss: 0.9920085668563843, Test_loss: 0.35055509209632874\n",
      "Epoch 238, Gen_loss: 0.7708633542060852, Disc_loss: 1.0208626985549927, Test_loss: 0.3701471984386444\n",
      "Epoch 239, Gen_loss: 0.797415018081665, Disc_loss: 0.975269079208374, Test_loss: 0.36108991503715515\n",
      "Epoch 240, Gen_loss: 0.8027121424674988, Disc_loss: 0.9685238599777222, Test_loss: 0.39891117811203003\n",
      "Epoch 241, Gen_loss: 0.7770134210586548, Disc_loss: 1.0237925052642822, Test_loss: 0.40602290630340576\n",
      "Epoch 242, Gen_loss: 0.8187668919563293, Disc_loss: 0.9494712948799133, Test_loss: 0.34125086665153503\n",
      "Epoch 243, Gen_loss: 0.7776353359222412, Disc_loss: 1.0045102834701538, Test_loss: 0.39822691679000854\n",
      "Epoch 244, Gen_loss: 0.8281860947608948, Disc_loss: 1.3627644777297974, Test_loss: 0.589123547077179\n",
      "Epoch 245, Gen_loss: 0.84730464220047, Disc_loss: 1.0894159078598022, Test_loss: 0.48932480812072754\n",
      "Epoch 246, Gen_loss: 0.8081445693969727, Disc_loss: 1.072732925415039, Test_loss: 0.5228587985038757\n",
      "Epoch 247, Gen_loss: 0.8134944438934326, Disc_loss: 1.0790839195251465, Test_loss: 0.5186307430267334\n",
      "Epoch 248, Gen_loss: 0.848974347114563, Disc_loss: 1.0295050144195557, Test_loss: 0.49019259214401245\n",
      "Epoch 249, Gen_loss: 0.8199271559715271, Disc_loss: 1.0594446659088135, Test_loss: 0.5160841345787048\n",
      "Epoch 250, Gen_loss: 0.8139612078666687, Disc_loss: 1.0663660764694214, Test_loss: 0.47052323818206787\n",
      "Epoch 251, Gen_loss: 0.797686755657196, Disc_loss: 1.036374568939209, Test_loss: 0.48165491223335266\n",
      "Epoch 252, Gen_loss: 0.7342656254768372, Disc_loss: 1.1675912141799927, Test_loss: 0.5205926299095154\n",
      "Epoch 253, Gen_loss: 0.8238065242767334, Disc_loss: 1.113358974456787, Test_loss: 0.5985055565834045\n",
      "Epoch 254, Gen_loss: 0.8404465317726135, Disc_loss: 1.034543752670288, Test_loss: 0.3746377229690552\n",
      "Epoch 255, Gen_loss: 0.786883533000946, Disc_loss: 0.9921122193336487, Test_loss: 0.343322217464447\n",
      "Epoch 256, Gen_loss: 0.7899109721183777, Disc_loss: 1.0096700191497803, Test_loss: 0.39298444986343384\n",
      "Epoch 257, Gen_loss: 0.801307737827301, Disc_loss: 1.005599856376648, Test_loss: 0.3952946364879608\n",
      "Epoch 258, Gen_loss: 0.8300377726554871, Disc_loss: 0.9667101502418518, Test_loss: 0.3505208492279053\n",
      "Epoch 259, Gen_loss: 0.8075371384620667, Disc_loss: 0.9655129909515381, Test_loss: 0.3533008396625519\n",
      "Epoch 260, Gen_loss: 0.78839510679245, Disc_loss: 0.9823104739189148, Test_loss: 0.3382824659347534\n",
      "Epoch 261, Gen_loss: 0.7856220602989197, Disc_loss: 0.9936603307723999, Test_loss: 0.3443128764629364\n",
      "Epoch 262, Gen_loss: 0.7145255208015442, Disc_loss: 1.294185996055603, Test_loss: 0.7464314699172974\n",
      "Epoch 263, Gen_loss: 0.8903563022613525, Disc_loss: 1.1937695741653442, Test_loss: 0.5658408403396606\n",
      "Epoch 264, Gen_loss: 0.8097658753395081, Disc_loss: 1.0937926769256592, Test_loss: 0.5382207036018372\n",
      "Epoch 265, Gen_loss: 0.8710969686508179, Disc_loss: 1.0048604011535645, Test_loss: 0.5084835290908813\n",
      "Epoch 266, Gen_loss: 0.8402798175811768, Disc_loss: 1.0501863956451416, Test_loss: 0.5371163487434387\n",
      "Epoch 267, Gen_loss: 0.7952128052711487, Disc_loss: 1.0922411680221558, Test_loss: 0.49329090118408203\n",
      "Epoch 268, Gen_loss: 0.7429471611976624, Disc_loss: 1.1463199853897095, Test_loss: 0.5036117434501648\n",
      "Epoch 269, Gen_loss: 0.8646155595779419, Disc_loss: 1.036084532737732, Test_loss: 0.5901632308959961\n",
      "Epoch 270, Gen_loss: 0.8513039350509644, Disc_loss: 1.0146008729934692, Test_loss: 0.43826499581336975\n",
      "Epoch 271, Gen_loss: 0.808788537979126, Disc_loss: 1.0112159252166748, Test_loss: 0.3798368275165558\n",
      "Epoch 272, Gen_loss: 0.7737472653388977, Disc_loss: 1.0070971250534058, Test_loss: 0.39897507429122925\n",
      "Epoch 273, Gen_loss: 0.8407009840011597, Disc_loss: 0.9435057044029236, Test_loss: 0.4417107105255127\n",
      "Epoch 274, Gen_loss: 0.8201944828033447, Disc_loss: 0.9515475034713745, Test_loss: 0.41372576355934143\n",
      "Epoch 275, Gen_loss: 0.7338424921035767, Disc_loss: 1.2894302606582642, Test_loss: 0.7765546441078186\n",
      "Epoch 276, Gen_loss: 0.9469643235206604, Disc_loss: 1.1083500385284424, Test_loss: 0.5430809259414673\n",
      "Epoch 277, Gen_loss: 0.8443385362625122, Disc_loss: 1.0385414361953735, Test_loss: 0.5343956351280212\n",
      "Epoch 278, Gen_loss: 0.8153877258300781, Disc_loss: 1.0820003747940063, Test_loss: 0.49779555201530457\n",
      "Epoch 279, Gen_loss: 0.8525607585906982, Disc_loss: 1.0421680212020874, Test_loss: 0.5148387551307678\n",
      "Epoch 280, Gen_loss: 0.8428514003753662, Disc_loss: 1.0394657850265503, Test_loss: 0.5444875955581665\n",
      "Epoch 281, Gen_loss: 0.823515772819519, Disc_loss: 1.0533818006515503, Test_loss: 0.5459982752799988\n",
      "Epoch 282, Gen_loss: 0.8296438455581665, Disc_loss: 1.0491387844085693, Test_loss: 0.4921225905418396\n",
      "Epoch 283, Gen_loss: 0.7235067486763, Disc_loss: 1.2888069152832031, Test_loss: 0.5053437948226929\n",
      "Epoch 284, Gen_loss: 0.8934869766235352, Disc_loss: 1.038655161857605, Test_loss: 0.4828225076198578\n",
      "Epoch 285, Gen_loss: 0.836224377155304, Disc_loss: 1.0313247442245483, Test_loss: 0.4256422519683838\n",
      "Epoch 286, Gen_loss: 0.8004332780838013, Disc_loss: 1.0026748180389404, Test_loss: 0.41304853558540344\n",
      "Epoch 287, Gen_loss: 0.817758321762085, Disc_loss: 0.9896247982978821, Test_loss: 0.4597508907318115\n",
      "Epoch 288, Gen_loss: 0.8417702317237854, Disc_loss: 0.9708769917488098, Test_loss: 0.37080109119415283\n",
      "Epoch 289, Gen_loss: 0.7757495641708374, Disc_loss: 1.0695947408676147, Test_loss: 0.7019078135490417\n",
      "Epoch 290, Gen_loss: 0.9311899542808533, Disc_loss: 1.2778031826019287, Test_loss: 0.579444944858551\n",
      "Epoch 291, Gen_loss: 0.8552281856536865, Disc_loss: 1.048635482788086, Test_loss: 0.5425763130187988\n",
      "Epoch 292, Gen_loss: 0.843174397945404, Disc_loss: 1.024398922920227, Test_loss: 0.49066755175590515\n",
      "Epoch 293, Gen_loss: 0.8228262066841125, Disc_loss: 1.0477008819580078, Test_loss: 0.5404420495033264\n",
      "Epoch 294, Gen_loss: 0.8356108069419861, Disc_loss: 1.0483496189117432, Test_loss: 0.4845675826072693\n",
      "Epoch 295, Gen_loss: 0.84938645362854, Disc_loss: 1.010228157043457, Test_loss: 0.48134347796440125\n",
      "Epoch 296, Gen_loss: 0.8517663478851318, Disc_loss: 1.0538016557693481, Test_loss: 0.5137583613395691\n",
      "Epoch 297, Gen_loss: 0.8470143675804138, Disc_loss: 1.038879156112671, Test_loss: 0.5210878849029541\n",
      "Epoch 298, Gen_loss: 0.834835410118103, Disc_loss: 1.0358058214187622, Test_loss: 0.4837018549442291\n",
      "Epoch 299, Gen_loss: 0.8016829490661621, Disc_loss: 1.0622050762176514, Test_loss: 0.4333599805831909\n",
      "Epoch 300, Gen_loss: 0.770957887172699, Disc_loss: 1.1418260335922241, Test_loss: 0.4857079088687897\n",
      "Epoch 301, Gen_loss: 0.8848268389701843, Disc_loss: 0.9899919033050537, Test_loss: 0.4077971577644348\n",
      "Epoch 302, Gen_loss: 0.8455387353897095, Disc_loss: 1.019396424293518, Test_loss: 0.4600873291492462\n",
      "Epoch 303, Gen_loss: 0.8287634253501892, Disc_loss: 0.9982501268386841, Test_loss: 0.3592202663421631\n",
      "Epoch 304, Gen_loss: 0.8589842915534973, Disc_loss: 0.9621383547782898, Test_loss: 0.46028733253479004\n",
      "Epoch 305, Gen_loss: 0.8578095436096191, Disc_loss: 0.9806131720542908, Test_loss: 0.39739790558815\n",
      "Epoch 306, Gen_loss: 0.8602879047393799, Disc_loss: 0.9281027317047119, Test_loss: 0.34308379888534546\n",
      "Epoch 307, Gen_loss: 0.800073504447937, Disc_loss: 1.3418803215026855, Test_loss: 0.6421964764595032\n",
      "Epoch 308, Gen_loss: 0.9011110663414001, Disc_loss: 1.0592232942581177, Test_loss: 0.5280125141143799\n",
      "Epoch 309, Gen_loss: 0.8298348188400269, Disc_loss: 1.0630221366882324, Test_loss: 0.5333839654922485\n",
      "Epoch 310, Gen_loss: 0.8309165239334106, Disc_loss: 1.0673495531082153, Test_loss: 0.498104065656662\n",
      "Epoch 311, Gen_loss: 0.876872181892395, Disc_loss: 1.0214399099349976, Test_loss: 0.528276264667511\n",
      "Epoch 312, Gen_loss: 0.8641446232795715, Disc_loss: 1.0251179933547974, Test_loss: 0.5106377005577087\n",
      "Epoch 313, Gen_loss: 0.836353063583374, Disc_loss: 1.026851773262024, Test_loss: 0.533201277256012\n",
      "Epoch 314, Gen_loss: 0.8716322183609009, Disc_loss: 1.030737280845642, Test_loss: 0.524117112159729\n",
      "Epoch 315, Gen_loss: 0.7505877017974854, Disc_loss: 1.2085373401641846, Test_loss: 0.501133918762207\n",
      "Epoch 316, Gen_loss: 0.8795960545539856, Disc_loss: 1.057394027709961, Test_loss: 0.42891573905944824\n",
      "Epoch 317, Gen_loss: 0.8661496639251709, Disc_loss: 1.009852647781372, Test_loss: 0.5285630226135254\n",
      "Epoch 318, Gen_loss: 0.8393886089324951, Disc_loss: 0.989611804485321, Test_loss: 0.45431607961654663\n",
      "Epoch 319, Gen_loss: 0.8525835871696472, Disc_loss: 0.9499860405921936, Test_loss: 0.416500449180603\n",
      "Epoch 320, Gen_loss: 0.8455553650856018, Disc_loss: 0.972739577293396, Test_loss: 0.3776405453681946\n",
      "Epoch 321, Gen_loss: 0.8149414658546448, Disc_loss: 1.0130881071090698, Test_loss: 0.5033846497535706\n",
      "Epoch 322, Gen_loss: 0.9511698484420776, Disc_loss: 1.2335865497589111, Test_loss: 0.6163809895515442\n",
      "Epoch 323, Gen_loss: 0.8877847194671631, Disc_loss: 1.017841100692749, Test_loss: 0.5033038258552551\n",
      "Epoch 324, Gen_loss: 0.8364355564117432, Disc_loss: 1.0377521514892578, Test_loss: 0.4817613959312439\n",
      "Epoch 325, Gen_loss: 0.8555383682250977, Disc_loss: 1.0374776124954224, Test_loss: 0.5403258800506592\n",
      "Epoch 326, Gen_loss: 0.8775203227996826, Disc_loss: 1.029335379600525, Test_loss: 0.5365597009658813\n",
      "Epoch 327, Gen_loss: 0.8740928769111633, Disc_loss: 0.9901456832885742, Test_loss: 0.5548064112663269\n",
      "Epoch 328, Gen_loss: 0.8921133875846863, Disc_loss: 1.046576976776123, Test_loss: 0.5300742387771606\n",
      "Epoch 329, Gen_loss: 0.8409200310707092, Disc_loss: 1.045007348060608, Test_loss: 0.4972381889820099\n",
      "Epoch 330, Gen_loss: 0.7598910927772522, Disc_loss: 1.1951320171356201, Test_loss: 0.5033114552497864\n",
      "Epoch 331, Gen_loss: 0.8877251744270325, Disc_loss: 0.9992043375968933, Test_loss: 0.4368326663970947\n",
      "Epoch 332, Gen_loss: 0.8544084429740906, Disc_loss: 1.0217288732528687, Test_loss: 0.4458553194999695\n",
      "Epoch 333, Gen_loss: 0.8809234499931335, Disc_loss: 0.9626854062080383, Test_loss: 0.38251665234565735\n",
      "Epoch 334, Gen_loss: 0.8613123297691345, Disc_loss: 0.9634191989898682, Test_loss: 0.38248011469841003\n",
      "Epoch 335, Gen_loss: 0.8694669008255005, Disc_loss: 0.9225118160247803, Test_loss: 0.3873847424983978\n",
      "Epoch 336, Gen_loss: 0.8589176535606384, Disc_loss: 0.9475030899047852, Test_loss: 0.3738980293273926\n",
      "Epoch 337, Gen_loss: 0.8118898868560791, Disc_loss: 1.224525809288025, Test_loss: 0.7814512848854065\n",
      "Epoch 338, Gen_loss: 0.9633527994155884, Disc_loss: 1.133826732635498, Test_loss: 0.5106961131095886\n",
      "Epoch 339, Gen_loss: 0.8676758408546448, Disc_loss: 1.0211783647537231, Test_loss: 0.5198643803596497\n",
      "Epoch 340, Gen_loss: 0.8652566075325012, Disc_loss: 1.0265707969665527, Test_loss: 0.5447801947593689\n",
      "Epoch 341, Gen_loss: 0.9046084880828857, Disc_loss: 1.024340033531189, Test_loss: 0.4894573390483856\n",
      "Epoch 342, Gen_loss: 0.8735963702201843, Disc_loss: 0.9752830266952515, Test_loss: 0.5128426551818848\n",
      "Epoch 343, Gen_loss: 0.8923131227493286, Disc_loss: 1.049378514289856, Test_loss: 0.5430626273155212\n",
      "Epoch 344, Gen_loss: 0.8613914251327515, Disc_loss: 1.0346661806106567, Test_loss: 0.5270430445671082\n",
      "Epoch 345, Gen_loss: 0.8653743267059326, Disc_loss: 1.0133612155914307, Test_loss: 0.5675311088562012\n",
      "Epoch 346, Gen_loss: 0.7950549721717834, Disc_loss: 1.223578691482544, Test_loss: 0.587787389755249\n",
      "Epoch 347, Gen_loss: 0.9114996790885925, Disc_loss: 1.0427520275115967, Test_loss: 0.49282434582710266\n",
      "Epoch 348, Gen_loss: 0.8916698694229126, Disc_loss: 1.0040719509124756, Test_loss: 0.42677536606788635\n",
      "Epoch 349, Gen_loss: 0.8779938817024231, Disc_loss: 0.9813381433486938, Test_loss: 0.42497602105140686\n",
      "Epoch 350, Gen_loss: 0.8670206069946289, Disc_loss: 0.9474211931228638, Test_loss: 0.4965589940547943\n",
      "Epoch 351, Gen_loss: 0.8878219723701477, Disc_loss: 0.9578024744987488, Test_loss: 0.4479411542415619\n",
      "Epoch 352, Gen_loss: 0.9747398495674133, Disc_loss: 1.2233762741088867, Test_loss: 0.6603243350982666\n",
      "Epoch 353, Gen_loss: 0.9489642381668091, Disc_loss: 1.021202802658081, Test_loss: 0.5404233932495117\n",
      "Epoch 354, Gen_loss: 0.8364559412002563, Disc_loss: 1.0065990686416626, Test_loss: 0.49600544571876526\n",
      "Epoch 355, Gen_loss: 0.886852502822876, Disc_loss: 1.0203607082366943, Test_loss: 0.5704338550567627\n",
      "Epoch 356, Gen_loss: 0.9224054217338562, Disc_loss: 1.0157101154327393, Test_loss: 0.5195722579956055\n",
      "Epoch 357, Gen_loss: 0.8928275108337402, Disc_loss: 1.006406545639038, Test_loss: 0.5472134351730347\n",
      "Epoch 358, Gen_loss: 0.8795190453529358, Disc_loss: 1.03750741481781, Test_loss: 0.6210206151008606\n",
      "Epoch 359, Gen_loss: 0.9107613563537598, Disc_loss: 1.0201889276504517, Test_loss: 0.4930925667285919\n",
      "Epoch 360, Gen_loss: 0.7927011251449585, Disc_loss: 1.117282748222351, Test_loss: 0.5548461675643921\n",
      "Epoch 361, Gen_loss: 0.8762612342834473, Disc_loss: 1.0948282480239868, Test_loss: 0.5230727195739746\n",
      "Epoch 362, Gen_loss: 0.9054549336433411, Disc_loss: 0.994577169418335, Test_loss: 0.43139249086380005\n",
      "Epoch 363, Gen_loss: 0.909432053565979, Disc_loss: 1.0808345079421997, Test_loss: 0.6951896548271179\n",
      "Epoch 364, Gen_loss: 1.0011659860610962, Disc_loss: 1.0091029405593872, Test_loss: 0.5136589407920837\n",
      "Epoch 365, Gen_loss: 0.886257529258728, Disc_loss: 1.0103094577789307, Test_loss: 0.5162342190742493\n",
      "Epoch 366, Gen_loss: 0.8846622109413147, Disc_loss: 0.9756806492805481, Test_loss: 0.5567981004714966\n",
      "Epoch 367, Gen_loss: 0.9110370874404907, Disc_loss: 1.0885323286056519, Test_loss: 0.6217110753059387\n",
      "Epoch 368, Gen_loss: 0.9013105630874634, Disc_loss: 1.0279990434646606, Test_loss: 0.515171468257904\n",
      "Epoch 369, Gen_loss: 0.8902164101600647, Disc_loss: 0.9851388931274414, Test_loss: 0.48196280002593994\n",
      "Epoch 370, Gen_loss: 0.8071555495262146, Disc_loss: 1.1320370435714722, Test_loss: 0.5922778248786926\n",
      "Epoch 371, Gen_loss: 0.9183560013771057, Disc_loss: 1.0779502391815186, Test_loss: 0.5835802555084229\n",
      "Epoch 372, Gen_loss: 0.9143975973129272, Disc_loss: 0.9913454055786133, Test_loss: 0.3772946298122406\n",
      "Epoch 373, Gen_loss: 0.9264952540397644, Disc_loss: 0.8942405581474304, Test_loss: 0.4728303551673889\n",
      "Epoch 374, Gen_loss: 0.8727797865867615, Disc_loss: 0.964758038520813, Test_loss: 0.4334798753261566\n",
      "Epoch 375, Gen_loss: 0.886124849319458, Disc_loss: 0.9325453042984009, Test_loss: 0.4054211676120758\n",
      "Epoch 376, Gen_loss: 0.8741667866706848, Disc_loss: 0.9437781572341919, Test_loss: 0.4203815758228302\n",
      "Epoch 377, Gen_loss: 0.8960489630699158, Disc_loss: 0.9388965368270874, Test_loss: 0.5193278193473816\n",
      "Epoch 378, Gen_loss: 0.945364236831665, Disc_loss: 0.9260051250457764, Test_loss: 0.45581409335136414\n",
      "Epoch 379, Gen_loss: 0.8162283897399902, Disc_loss: 1.219914436340332, Test_loss: 0.8489096760749817\n",
      "Epoch 380, Gen_loss: 1.093467116355896, Disc_loss: 1.1098659038543701, Test_loss: 0.5878812074661255\n",
      "Epoch 381, Gen_loss: 0.9187103509902954, Disc_loss: 1.014283537864685, Test_loss: 0.5703977346420288\n",
      "Epoch 382, Gen_loss: 0.9036513566970825, Disc_loss: 1.0451711416244507, Test_loss: 0.5890459418296814\n",
      "Epoch 383, Gen_loss: 0.9316858649253845, Disc_loss: 1.0383732318878174, Test_loss: 0.5449276566505432\n",
      "Epoch 384, Gen_loss: 0.9361129999160767, Disc_loss: 0.965577244758606, Test_loss: 0.48159316182136536\n",
      "Epoch 385, Gen_loss: 0.9262322187423706, Disc_loss: 0.9666601419448853, Test_loss: 0.7108067274093628\n",
      "Epoch 386, Gen_loss: 0.8942919373512268, Disc_loss: 1.1109423637390137, Test_loss: 0.5659303069114685\n",
      "Epoch 387, Gen_loss: 0.9335078597068787, Disc_loss: 0.9874409437179565, Test_loss: 0.5572578310966492\n",
      "Epoch 388, Gen_loss: 0.9155438542366028, Disc_loss: 1.0146281719207764, Test_loss: 0.609865128993988\n",
      "Epoch 389, Gen_loss: 0.8881693482398987, Disc_loss: 1.0203553438186646, Test_loss: 0.5758455395698547\n",
      "Epoch 390, Gen_loss: 0.9377524256706238, Disc_loss: 1.0389819145202637, Test_loss: 0.597266674041748\n",
      "Epoch 391, Gen_loss: 0.9268785119056702, Disc_loss: 0.9893896579742432, Test_loss: 0.5539163947105408\n",
      "Epoch 392, Gen_loss: 0.8607872128486633, Disc_loss: 1.1872549057006836, Test_loss: 0.5291334390640259\n",
      "Epoch 393, Gen_loss: 0.9572601318359375, Disc_loss: 1.0238734483718872, Test_loss: 0.6547619104385376\n",
      "Epoch 394, Gen_loss: 0.9047670364379883, Disc_loss: 0.9946388006210327, Test_loss: 0.39722609519958496\n",
      "Epoch 395, Gen_loss: 0.945385217666626, Disc_loss: 0.8854387998580933, Test_loss: 0.46921074390411377\n",
      "Epoch 396, Gen_loss: 0.857479453086853, Disc_loss: 0.9976029396057129, Test_loss: 0.5073927640914917\n",
      "Epoch 397, Gen_loss: 0.8565435409545898, Disc_loss: 0.9677777886390686, Test_loss: 0.37951523065567017\n",
      "Epoch 398, Gen_loss: 0.9638075828552246, Disc_loss: 0.8627108931541443, Test_loss: 0.44229379296302795\n",
      "Epoch 399, Gen_loss: 0.909625768661499, Disc_loss: 0.9947469830513, Test_loss: 0.5310395359992981\n",
      "Epoch 400, Gen_loss: 0.9523656964302063, Disc_loss: 1.144214153289795, Test_loss: 0.676358699798584\n",
      "Epoch 401, Gen_loss: 1.0241544246673584, Disc_loss: 1.0420836210250854, Test_loss: 0.5723586678504944\n",
      "Epoch 402, Gen_loss: 0.9551680088043213, Disc_loss: 0.9880176186561584, Test_loss: 0.594291090965271\n",
      "Epoch 403, Gen_loss: 0.8974237442016602, Disc_loss: 1.0557581186294556, Test_loss: 0.5620479583740234\n",
      "Epoch 404, Gen_loss: 0.945328950881958, Disc_loss: 0.9523016810417175, Test_loss: 0.5837347507476807\n",
      "Epoch 405, Gen_loss: 0.9455037713050842, Disc_loss: 1.048736333847046, Test_loss: 0.6349731683731079\n",
      "Epoch 406, Gen_loss: 0.9529892802238464, Disc_loss: 1.014751672744751, Test_loss: 0.607918918132782\n",
      "Epoch 407, Gen_loss: 0.9079077243804932, Disc_loss: 1.026638150215149, Test_loss: 0.5569294691085815\n",
      "Epoch 408, Gen_loss: 0.891295850276947, Disc_loss: 0.9529913067817688, Test_loss: 0.6691655516624451\n",
      "Epoch 409, Gen_loss: 0.9823911786079407, Disc_loss: 1.0555835962295532, Test_loss: 0.5832827687263489\n",
      "Epoch 410, Gen_loss: 0.9433697462081909, Disc_loss: 1.0326131582260132, Test_loss: 0.6076974868774414\n",
      "Epoch 411, Gen_loss: 0.9283015727996826, Disc_loss: 1.0180466175079346, Test_loss: 0.5741478204727173\n",
      "Epoch 412, Gen_loss: 0.9299338459968567, Disc_loss: 1.0372447967529297, Test_loss: 0.5739312767982483\n",
      "Epoch 413, Gen_loss: 0.9178553819656372, Disc_loss: 1.029811978340149, Test_loss: 0.5315188765525818\n",
      "Epoch 414, Gen_loss: 0.8443863987922668, Disc_loss: 1.2335617542266846, Test_loss: 0.7546455264091492\n",
      "Epoch 415, Gen_loss: 0.9737820029258728, Disc_loss: 1.0207147598266602, Test_loss: 0.5429145693778992\n",
      "Epoch 416, Gen_loss: 0.9131993055343628, Disc_loss: 1.0461719036102295, Test_loss: 0.4311578571796417\n",
      "Epoch 417, Gen_loss: 0.9112412929534912, Disc_loss: 0.9621429443359375, Test_loss: 0.4801156520843506\n",
      "Epoch 418, Gen_loss: 0.9445624351501465, Disc_loss: 0.9217237830162048, Test_loss: 0.4144473969936371\n",
      "Epoch 419, Gen_loss: 0.929652750492096, Disc_loss: 0.8999254703521729, Test_loss: 0.42148953676223755\n",
      "Epoch 420, Gen_loss: 0.9083864092826843, Disc_loss: 1.1019030809402466, Test_loss: 0.8356661796569824\n",
      "Epoch 421, Gen_loss: 1.0510607957839966, Disc_loss: 1.0598253011703491, Test_loss: 0.6371795535087585\n",
      "Epoch 422, Gen_loss: 0.9079626202583313, Disc_loss: 1.0474300384521484, Test_loss: 0.5492824912071228\n",
      "Epoch 423, Gen_loss: 0.9627896547317505, Disc_loss: 0.925409197807312, Test_loss: 0.6390131711959839\n",
      "Epoch 424, Gen_loss: 0.9539358615875244, Disc_loss: 1.0402255058288574, Test_loss: 0.6174852848052979\n",
      "Epoch 425, Gen_loss: 0.956735372543335, Disc_loss: 1.001816987991333, Test_loss: 0.6164860725402832\n",
      "Epoch 426, Gen_loss: 0.9199696779251099, Disc_loss: 0.9616343975067139, Test_loss: 0.4648781716823578\n",
      "Epoch 427, Gen_loss: 0.9554081559181213, Disc_loss: 1.034822702407837, Test_loss: 0.6755713224411011\n",
      "Epoch 428, Gen_loss: 0.9319156408309937, Disc_loss: 1.044272541999817, Test_loss: 0.5637688040733337\n",
      "Epoch 429, Gen_loss: 0.9516844749450684, Disc_loss: 0.9769166707992554, Test_loss: 0.5460834503173828\n",
      "Epoch 430, Gen_loss: 0.8642208576202393, Disc_loss: 1.0825670957565308, Test_loss: 0.5244784355163574\n",
      "Epoch 431, Gen_loss: 0.9236035943031311, Disc_loss: 1.0654090642929077, Test_loss: 0.5735319256782532\n",
      "Epoch 432, Gen_loss: 0.95039302110672, Disc_loss: 0.9585450887680054, Test_loss: 0.399152934551239\n",
      "Epoch 433, Gen_loss: 0.9202815294265747, Disc_loss: 0.9378112554550171, Test_loss: 0.4213009774684906\n",
      "Epoch 434, Gen_loss: 0.9518547058105469, Disc_loss: 0.9220882058143616, Test_loss: 0.5153252482414246\n",
      "Epoch 435, Gen_loss: 0.943024218082428, Disc_loss: 0.9264553785324097, Test_loss: 0.4041283428668976\n",
      "Epoch 436, Gen_loss: 0.9287352561950684, Disc_loss: 0.9038417339324951, Test_loss: 0.3475613594055176\n",
      "Epoch 437, Gen_loss: 0.935497522354126, Disc_loss: 0.8618683815002441, Test_loss: 0.3582249581813812\n",
      "Epoch 438, Gen_loss: 0.884952962398529, Disc_loss: 0.9132949113845825, Test_loss: 0.4507473409175873\n",
      "Epoch 439, Gen_loss: 1.1038978099822998, Disc_loss: 1.1554839611053467, Test_loss: 0.7174770832061768\n",
      "Epoch 440, Gen_loss: 1.0210402011871338, Disc_loss: 1.023171067237854, Test_loss: 0.5926646590232849\n",
      "Epoch 441, Gen_loss: 0.9754313826560974, Disc_loss: 1.0180171728134155, Test_loss: 0.6590412259101868\n",
      "Epoch 442, Gen_loss: 0.9621802568435669, Disc_loss: 1.037592887878418, Test_loss: 0.602198600769043\n",
      "Epoch 443, Gen_loss: 0.9374996423721313, Disc_loss: 1.0095947980880737, Test_loss: 0.5728871822357178\n",
      "Epoch 444, Gen_loss: 0.9254381060600281, Disc_loss: 0.960523247718811, Test_loss: 0.5646027326583862\n",
      "Epoch 445, Gen_loss: 0.983206033706665, Disc_loss: 1.012420654296875, Test_loss: 0.5963481664657593\n",
      "Epoch 446, Gen_loss: 0.9766631126403809, Disc_loss: 1.003899335861206, Test_loss: 0.5892717838287354\n",
      "Epoch 447, Gen_loss: 0.9256786108016968, Disc_loss: 1.0391179323196411, Test_loss: 0.5857992768287659\n",
      "Epoch 448, Gen_loss: 0.9581664800643921, Disc_loss: 0.9263874292373657, Test_loss: 0.7247633337974548\n",
      "Epoch 449, Gen_loss: 0.9854459166526794, Disc_loss: 1.0578728914260864, Test_loss: 0.6244879961013794\n",
      "Epoch 450, Gen_loss: 0.9322258830070496, Disc_loss: 1.0129342079162598, Test_loss: 0.5993327498435974\n",
      "Epoch 451, Gen_loss: 0.9502257704734802, Disc_loss: 1.0067713260650635, Test_loss: 0.5759008526802063\n",
      "Epoch 452, Gen_loss: 0.9549643397331238, Disc_loss: 0.9953504800796509, Test_loss: 0.5832889676094055\n",
      "Epoch 453, Gen_loss: 0.974342405796051, Disc_loss: 1.0110152959823608, Test_loss: 0.5593668818473816\n",
      "Epoch 454, Gen_loss: 0.91234290599823, Disc_loss: 0.9851824641227722, Test_loss: 0.503093957901001\n",
      "Epoch 455, Gen_loss: 0.9557288885116577, Disc_loss: 1.0004469156265259, Test_loss: 0.6042808890342712\n",
      "Epoch 456, Gen_loss: 0.988227903842926, Disc_loss: 0.9989824891090393, Test_loss: 0.6059191823005676\n",
      "Epoch 457, Gen_loss: 0.8728474974632263, Disc_loss: 1.1524757146835327, Test_loss: 0.5713329315185547\n",
      "Epoch 458, Gen_loss: 0.9662708044052124, Disc_loss: 1.0262606143951416, Test_loss: 0.5334264636039734\n",
      "Epoch 459, Gen_loss: 0.9360864162445068, Disc_loss: 0.9790095686912537, Test_loss: 0.49755388498306274\n",
      "Epoch 460, Gen_loss: 0.9601267576217651, Disc_loss: 0.8704283237457275, Test_loss: 0.3601451814174652\n",
      "Epoch 461, Gen_loss: 0.9475051164627075, Disc_loss: 0.9003214240074158, Test_loss: 0.41117268800735474\n",
      "Epoch 462, Gen_loss: 0.9261674880981445, Disc_loss: 0.9181914329528809, Test_loss: 0.4181179702281952\n",
      "Epoch 463, Gen_loss: 0.9562793970108032, Disc_loss: 0.8659051656723022, Test_loss: 0.39417707920074463\n",
      "Epoch 464, Gen_loss: 0.9569056034088135, Disc_loss: 0.8530851006507874, Test_loss: 0.3454191982746124\n",
      "Epoch 465, Gen_loss: 0.971089780330658, Disc_loss: 0.8205673098564148, Test_loss: 0.4104191064834595\n",
      "Epoch 466, Gen_loss: 0.9239146709442139, Disc_loss: 0.9120053648948669, Test_loss: 0.4418630599975586\n",
      "Epoch 467, Gen_loss: 0.9778525233268738, Disc_loss: 0.8275955319404602, Test_loss: 0.2726404070854187\n",
      "Epoch 468, Gen_loss: 0.898466944694519, Disc_loss: 1.1745058298110962, Test_loss: 0.6659460067749023\n",
      "Epoch 469, Gen_loss: 1.1311061382293701, Disc_loss: 1.191820502281189, Test_loss: 0.629092812538147\n",
      "Epoch 470, Gen_loss: 1.0001143217086792, Disc_loss: 0.982822597026825, Test_loss: 0.6057255864143372\n",
      "Epoch 471, Gen_loss: 1.0050748586654663, Disc_loss: 0.9641412496566772, Test_loss: 0.5607605576515198\n",
      "Epoch 472, Gen_loss: 0.9746623635292053, Disc_loss: 1.0208282470703125, Test_loss: 0.6092296838760376\n",
      "Epoch 473, Gen_loss: 0.9770336747169495, Disc_loss: 0.9856711626052856, Test_loss: 0.7115334272384644\n",
      "Epoch 474, Gen_loss: 0.9751130938529968, Disc_loss: 1.0671942234039307, Test_loss: 0.6105591654777527\n",
      "Epoch 475, Gen_loss: 0.996217668056488, Disc_loss: 0.9850175380706787, Test_loss: 0.6086419224739075\n",
      "Epoch 476, Gen_loss: 0.9039664268493652, Disc_loss: 1.0581094026565552, Test_loss: 0.5476895570755005\n",
      "Epoch 477, Gen_loss: 0.9568014144897461, Disc_loss: 0.9634120464324951, Test_loss: 0.6214123368263245\n",
      "Epoch 478, Gen_loss: 1.0149012804031372, Disc_loss: 1.023840069770813, Test_loss: 0.6491392254829407\n",
      "Epoch 479, Gen_loss: 1.0107346773147583, Disc_loss: 1.0080314874649048, Test_loss: 0.6102335453033447\n",
      "Epoch 480, Gen_loss: 0.9830120801925659, Disc_loss: 0.9899525046348572, Test_loss: 0.564539909362793\n",
      "Epoch 481, Gen_loss: 0.9422556757926941, Disc_loss: 1.0279295444488525, Test_loss: 0.5878852605819702\n",
      "Epoch 482, Gen_loss: 0.9540774822235107, Disc_loss: 0.9988196492195129, Test_loss: 0.6228312849998474\n",
      "Epoch 483, Gen_loss: 1.0058419704437256, Disc_loss: 0.9695665836334229, Test_loss: 0.5885871648788452\n",
      "Epoch 484, Gen_loss: 0.9798198938369751, Disc_loss: 1.0438801050186157, Test_loss: 0.615703284740448\n",
      "Epoch 485, Gen_loss: 0.9921318292617798, Disc_loss: 0.9802969098091125, Test_loss: 0.6718353033065796\n",
      "Epoch 486, Gen_loss: 0.9275518655776978, Disc_loss: 1.0035299062728882, Test_loss: 0.5211750864982605\n",
      "Epoch 487, Gen_loss: 1.0328075885772705, Disc_loss: 0.8595895767211914, Test_loss: 0.30163300037384033\n",
      "Epoch 488, Gen_loss: 1.0061649084091187, Disc_loss: 0.8530644178390503, Test_loss: 0.3646126687526703\n",
      "Epoch 489, Gen_loss: 0.908790647983551, Disc_loss: 0.9549084305763245, Test_loss: 0.4123571515083313\n",
      "Epoch 490, Gen_loss: 0.9815589189529419, Disc_loss: 0.8038420081138611, Test_loss: 0.365498423576355\n",
      "Epoch 491, Gen_loss: 1.0037161111831665, Disc_loss: 0.7987514734268188, Test_loss: 0.4145510792732239\n",
      "Epoch 492, Gen_loss: 0.9201961755752563, Disc_loss: 0.9043769836425781, Test_loss: 0.3875894546508789\n",
      "Epoch 493, Gen_loss: 0.9192968010902405, Disc_loss: 0.862314760684967, Test_loss: 0.26517653465270996\n",
      "Epoch 494, Gen_loss: 1.0204908847808838, Disc_loss: 0.7157471776008606, Test_loss: 0.3757937550544739\n",
      "Epoch 495, Gen_loss: 1.0534368753433228, Disc_loss: 1.282578945159912, Test_loss: 0.8234104514122009\n",
      "Epoch 496, Gen_loss: 1.0407912731170654, Disc_loss: 1.0403120517730713, Test_loss: 0.5438803434371948\n",
      "Epoch 497, Gen_loss: 1.0680636167526245, Disc_loss: 0.930988073348999, Test_loss: 0.5815894603729248\n",
      "Epoch 498, Gen_loss: 1.0880160331726074, Disc_loss: 0.9641821384429932, Test_loss: 0.6438705921173096\n",
      "Epoch 499, Gen_loss: 0.9697120785713196, Disc_loss: 1.045183539390564, Test_loss: 0.6230973601341248\n",
      "Epoch 500, Gen_loss: 0.9828599095344543, Disc_loss: 0.9756457805633545, Test_loss: 0.6945883631706238\n",
      "Epoch 501, Gen_loss: 1.041635274887085, Disc_loss: 0.9726032018661499, Test_loss: 0.5504323840141296\n",
      "Epoch 502, Gen_loss: 0.9972835183143616, Disc_loss: 0.9842117428779602, Test_loss: 0.648726761341095\n",
      "Epoch 503, Gen_loss: 0.9352925419807434, Disc_loss: 1.0096479654312134, Test_loss: 0.5580436587333679\n",
      "Epoch 504, Gen_loss: 0.9788593053817749, Disc_loss: 1.070002555847168, Test_loss: 0.6275488138198853\n",
      "Epoch 505, Gen_loss: 1.0455753803253174, Disc_loss: 0.9746166467666626, Test_loss: 0.6172084808349609\n",
      "Epoch 506, Gen_loss: 1.0316734313964844, Disc_loss: 0.985935389995575, Test_loss: 0.6242547631263733\n",
      "Epoch 507, Gen_loss: 0.9851076602935791, Disc_loss: 1.0208895206451416, Test_loss: 0.6885709166526794\n",
      "Epoch 508, Gen_loss: 0.9853710532188416, Disc_loss: 1.0150796175003052, Test_loss: 0.598392128944397\n",
      "Epoch 509, Gen_loss: 0.9767148494720459, Disc_loss: 0.9718923568725586, Test_loss: 0.5885972380638123\n",
      "Epoch 510, Gen_loss: 0.9991065859794617, Disc_loss: 0.9680955410003662, Test_loss: 0.5352036952972412\n",
      "Epoch 511, Gen_loss: 0.9923136234283447, Disc_loss: 0.9790107011795044, Test_loss: 0.7930426597595215\n",
      "Epoch 512, Gen_loss: 1.0009268522262573, Disc_loss: 1.029267430305481, Test_loss: 0.6025018692016602\n",
      "Epoch 513, Gen_loss: 0.986684262752533, Disc_loss: 0.9915454387664795, Test_loss: 0.5977244973182678\n",
      "Epoch 514, Gen_loss: 0.987680971622467, Disc_loss: 0.9728008508682251, Test_loss: 0.5068979263305664\n",
      "Epoch 515, Gen_loss: 1.0309988260269165, Disc_loss: 0.9702391028404236, Test_loss: 0.6930125951766968\n",
      "Epoch 516, Gen_loss: 1.0210431814193726, Disc_loss: 1.0139431953430176, Test_loss: 0.6062399744987488\n",
      "Epoch 517, Gen_loss: 0.9792683720588684, Disc_loss: 1.0231910943984985, Test_loss: 0.6705148816108704\n",
      "Epoch 518, Gen_loss: 0.9216060042381287, Disc_loss: 1.1247525215148926, Test_loss: 0.5343525409698486\n",
      "Epoch 519, Gen_loss: 1.0526320934295654, Disc_loss: 0.890476405620575, Test_loss: 0.2803041934967041\n",
      "Epoch 520, Gen_loss: 0.9638342261314392, Disc_loss: 0.8177156448364258, Test_loss: 0.3946281373500824\n",
      "Epoch 521, Gen_loss: 0.9771730899810791, Disc_loss: 0.8829401731491089, Test_loss: 0.46551668643951416\n",
      "Epoch 522, Gen_loss: 1.0321413278579712, Disc_loss: 0.7906543016433716, Test_loss: 0.39030417799949646\n",
      "Epoch 523, Gen_loss: 0.9371188879013062, Disc_loss: 0.843107283115387, Test_loss: 0.3000728487968445\n",
      "Epoch 524, Gen_loss: 0.9837343096733093, Disc_loss: 0.6642446517944336, Test_loss: 0.12314548343420029\n",
      "Epoch 525, Gen_loss: 1.0455149412155151, Disc_loss: 1.0417168140411377, Test_loss: 0.8932747840881348\n",
      "Epoch 526, Gen_loss: 1.238744854927063, Disc_loss: 1.0883073806762695, Test_loss: 0.618215799331665\n",
      "Epoch 527, Gen_loss: 1.0601664781570435, Disc_loss: 0.9217543005943298, Test_loss: 0.5531813502311707\n",
      "Epoch 528, Gen_loss: 1.07804536819458, Disc_loss: 0.9574201107025146, Test_loss: 0.6974720358848572\n",
      "Epoch 529, Gen_loss: 1.019777536392212, Disc_loss: 1.0333119630813599, Test_loss: 0.6362859606742859\n",
      "Epoch 530, Gen_loss: 1.0020407438278198, Disc_loss: 0.9872044324874878, Test_loss: 0.6235924363136292\n",
      "Epoch 531, Gen_loss: 1.0337090492248535, Disc_loss: 0.9594481587409973, Test_loss: 0.6030922532081604\n",
      "Epoch 532, Gen_loss: 1.0125905275344849, Disc_loss: 0.9239775538444519, Test_loss: 0.40048250555992126\n",
      "Epoch 533, Gen_loss: 0.95744788646698, Disc_loss: 0.9874705076217651, Test_loss: 0.706395149230957\n",
      "Epoch 534, Gen_loss: 1.0146015882492065, Disc_loss: 0.9998524188995361, Test_loss: 0.6485946774482727\n",
      "Epoch 535, Gen_loss: 0.9882320761680603, Disc_loss: 1.038715124130249, Test_loss: 0.5996716022491455\n",
      "Epoch 536, Gen_loss: 1.0171258449554443, Disc_loss: 1.003333568572998, Test_loss: 0.6929988265037537\n",
      "Epoch 537, Gen_loss: 1.0454516410827637, Disc_loss: 1.026741623878479, Test_loss: 0.6155062317848206\n",
      "Epoch 538, Gen_loss: 1.0245468616485596, Disc_loss: 0.9912655353546143, Test_loss: 0.6222283840179443\n",
      "Epoch 539, Gen_loss: 0.947361946105957, Disc_loss: 0.9968274235725403, Test_loss: 0.5805391669273376\n",
      "Epoch 540, Gen_loss: 1.000204086303711, Disc_loss: 0.9779999852180481, Test_loss: 0.5452354550361633\n",
      "Epoch 541, Gen_loss: 1.079912781715393, Disc_loss: 0.8722648620605469, Test_loss: 0.4424532651901245\n",
      "Epoch 542, Gen_loss: 0.938897430896759, Disc_loss: 0.8368985652923584, Test_loss: 0.2604372501373291\n",
      "Epoch 543, Gen_loss: 0.9660971760749817, Disc_loss: 0.7831053137779236, Test_loss: 0.3623740077018738\n",
      "Epoch 544, Gen_loss: 1.0648497343063354, Disc_loss: 0.856030285358429, Test_loss: 0.48155590891838074\n",
      "Epoch 545, Gen_loss: 0.9441753029823303, Disc_loss: 0.9403899908065796, Test_loss: 0.46226802468299866\n",
      "Epoch 546, Gen_loss: 1.0425784587860107, Disc_loss: 0.7651688456535339, Test_loss: 0.3195941150188446\n",
      "Epoch 547, Gen_loss: 1.0234841108322144, Disc_loss: 0.8189468383789062, Test_loss: 0.42646390199661255\n",
      "Epoch 548, Gen_loss: 1.0297439098358154, Disc_loss: 0.8267242312431335, Test_loss: 0.40168872475624084\n",
      "Epoch 549, Gen_loss: 0.9720497727394104, Disc_loss: 0.7618486881256104, Test_loss: 0.24742834270000458\n",
      "Epoch 550, Gen_loss: 0.9402137994766235, Disc_loss: 0.6617119908332825, Test_loss: 0.25681647658348083\n",
      "Epoch 551, Gen_loss: 1.1649643182754517, Disc_loss: 1.2062945365905762, Test_loss: 0.8494014143943787\n",
      "Epoch 552, Gen_loss: 1.199803113937378, Disc_loss: 1.0279269218444824, Test_loss: 0.6449035406112671\n",
      "Epoch 553, Gen_loss: 1.137962818145752, Disc_loss: 0.9221885800361633, Test_loss: 0.5765624046325684\n",
      "Epoch 554, Gen_loss: 1.0429561138153076, Disc_loss: 0.988403856754303, Test_loss: 0.6375718116760254\n",
      "Epoch 555, Gen_loss: 1.0197230577468872, Disc_loss: 0.9814094305038452, Test_loss: 0.5444807410240173\n",
      "Epoch 556, Gen_loss: 1.0311368703842163, Disc_loss: 0.955149233341217, Test_loss: 0.6881694197654724\n",
      "Epoch 557, Gen_loss: 1.0561935901641846, Disc_loss: 0.9822529554367065, Test_loss: 0.6524785757064819\n",
      "Epoch 558, Gen_loss: 1.0121872425079346, Disc_loss: 1.05235755443573, Test_loss: 0.6606603860855103\n",
      "Epoch 559, Gen_loss: 1.0141412019729614, Disc_loss: 0.8950281739234924, Test_loss: 0.6304818987846375\n",
      "Epoch 560, Gen_loss: 1.030039668083191, Disc_loss: 1.070149540901184, Test_loss: 0.7075737714767456\n",
      "Epoch 561, Gen_loss: 1.0702558755874634, Disc_loss: 0.9906466007232666, Test_loss: 0.6853564381599426\n",
      "Epoch 562, Gen_loss: 1.00295090675354, Disc_loss: 1.042175531387329, Test_loss: 0.6052871346473694\n",
      "Epoch 563, Gen_loss: 1.0242564678192139, Disc_loss: 0.9930440187454224, Test_loss: 0.6104874610900879\n",
      "Epoch 564, Gen_loss: 1.0173059701919556, Disc_loss: 1.0031671524047852, Test_loss: 0.6269267797470093\n",
      "Epoch 565, Gen_loss: 1.0418503284454346, Disc_loss: 0.9433488845825195, Test_loss: 0.507154107093811\n",
      "Epoch 566, Gen_loss: 0.996491551399231, Disc_loss: 1.0032542943954468, Test_loss: 0.694868266582489\n",
      "Epoch 567, Gen_loss: 1.0453627109527588, Disc_loss: 1.0141363143920898, Test_loss: 0.6017480492591858\n",
      "Epoch 568, Gen_loss: 1.0016798973083496, Disc_loss: 0.9971656203269958, Test_loss: 0.6090848445892334\n",
      "Epoch 569, Gen_loss: 0.9378038048744202, Disc_loss: 1.0568205118179321, Test_loss: 0.7998549342155457\n",
      "Epoch 570, Gen_loss: 1.0258996486663818, Disc_loss: 1.0484111309051514, Test_loss: 0.5384641885757446\n",
      "Epoch 571, Gen_loss: 1.100890040397644, Disc_loss: 0.8545237183570862, Test_loss: 0.3872096538543701\n",
      "Epoch 572, Gen_loss: 0.988195538520813, Disc_loss: 0.8862774968147278, Test_loss: 0.40609249472618103\n",
      "Epoch 573, Gen_loss: 0.9839059710502625, Disc_loss: 0.8744755983352661, Test_loss: 0.35355299711227417\n",
      "Epoch 574, Gen_loss: 1.059197187423706, Disc_loss: 0.6689134240150452, Test_loss: 0.14156875014305115\n",
      "Epoch 575, Gen_loss: 0.9611400961875916, Disc_loss: 0.9198570251464844, Test_loss: 0.9661179780960083\n",
      "Epoch 576, Gen_loss: 1.3513165712356567, Disc_loss: 1.0208309888839722, Test_loss: 0.7247238159179688\n",
      "Epoch 577, Gen_loss: 1.0972702503204346, Disc_loss: 1.0000656843185425, Test_loss: 0.65110182762146\n",
      "Epoch 578, Gen_loss: 1.0240529775619507, Disc_loss: 1.0023062229156494, Test_loss: 0.6203813552856445\n",
      "Epoch 579, Gen_loss: 1.0912760496139526, Disc_loss: 0.949367344379425, Test_loss: 0.6357411742210388\n",
      "Epoch 580, Gen_loss: 1.0779163837432861, Disc_loss: 0.9493078589439392, Test_loss: 0.6085265278816223\n",
      "Epoch 581, Gen_loss: 1.0064250230789185, Disc_loss: 0.9691680073738098, Test_loss: 0.7272213697433472\n",
      "Epoch 582, Gen_loss: 0.9997221827507019, Disc_loss: 1.004824161529541, Test_loss: 0.6304450035095215\n",
      "Epoch 583, Gen_loss: 1.0379140377044678, Disc_loss: 0.9687782526016235, Test_loss: 0.6287330985069275\n",
      "Epoch 584, Gen_loss: 1.0480608940124512, Disc_loss: 0.9535097479820251, Test_loss: 0.5111562609672546\n",
      "Epoch 585, Gen_loss: 0.9919804930686951, Disc_loss: 0.9041173458099365, Test_loss: 0.7670655846595764\n",
      "Epoch 586, Gen_loss: 1.0086078643798828, Disc_loss: 1.0480363368988037, Test_loss: 0.6357443332672119\n",
      "Epoch 587, Gen_loss: 1.0660613775253296, Disc_loss: 1.0087566375732422, Test_loss: 0.6948252320289612\n",
      "Epoch 588, Gen_loss: 1.0356206893920898, Disc_loss: 0.9953988790512085, Test_loss: 0.6763282418251038\n",
      "Epoch 589, Gen_loss: 1.0734838247299194, Disc_loss: 0.9857890605926514, Test_loss: 0.6864452362060547\n",
      "Epoch 590, Gen_loss: 1.0415842533111572, Disc_loss: 0.9784087538719177, Test_loss: 0.6676771640777588\n",
      "Epoch 591, Gen_loss: 0.9765000939369202, Disc_loss: 1.0274014472961426, Test_loss: 0.810182511806488\n",
      "Epoch 592, Gen_loss: 0.9882637858390808, Disc_loss: 0.996583878993988, Test_loss: 0.6328054070472717\n",
      "Epoch 593, Gen_loss: 1.0621064901351929, Disc_loss: 0.8572484254837036, Test_loss: 0.3818812072277069\n",
      "Epoch 594, Gen_loss: 1.0074646472930908, Disc_loss: 0.8485414981842041, Test_loss: 0.36487361788749695\n",
      "Epoch 595, Gen_loss: 1.0319966077804565, Disc_loss: 0.8096932172775269, Test_loss: 0.37046709656715393\n",
      "Epoch 596, Gen_loss: 1.0537210702896118, Disc_loss: 0.7933158278465271, Test_loss: 0.4365617334842682\n",
      "Epoch 597, Gen_loss: 0.9790087342262268, Disc_loss: 0.7664919495582581, Test_loss: 0.19957782328128815\n",
      "Epoch 598, Gen_loss: 0.9901275634765625, Disc_loss: 0.5752461552619934, Test_loss: 0.04663742706179619\n",
      "Epoch 599, Gen_loss: 0.7834519147872925, Disc_loss: 1.0045100450515747, Test_loss: 1.364717960357666\n",
      "Epoch 600, Gen_loss: 1.2462329864501953, Disc_loss: 1.300197720527649, Test_loss: 0.6717324256896973\n",
      "Epoch 601, Gen_loss: 1.171545386314392, Disc_loss: 0.9116297960281372, Test_loss: 0.5939714908599854\n",
      "Epoch 602, Gen_loss: 1.0461710691452026, Disc_loss: 0.9661644101142883, Test_loss: 0.5972243547439575\n",
      "Epoch 603, Gen_loss: 1.0207436084747314, Disc_loss: 1.002645492553711, Test_loss: 0.5369477868080139\n",
      "Epoch 604, Gen_loss: 1.0413120985031128, Disc_loss: 0.8792914748191833, Test_loss: 0.7845465540885925\n",
      "Epoch 605, Gen_loss: 1.0152891874313354, Disc_loss: 1.04243803024292, Test_loss: 0.6482033729553223\n",
      "Epoch 606, Gen_loss: 1.0857926607131958, Disc_loss: 1.0251376628875732, Test_loss: 0.7094701528549194\n",
      "Epoch 607, Gen_loss: 1.0593483448028564, Disc_loss: 0.9762910604476929, Test_loss: 0.5890206098556519\n",
      "Epoch 608, Gen_loss: 1.0812177658081055, Disc_loss: 0.9516509771347046, Test_loss: 0.6208480000495911\n",
      "Epoch 609, Gen_loss: 1.0494662523269653, Disc_loss: 0.9888522624969482, Test_loss: 0.7313719391822815\n",
      "Epoch 610, Gen_loss: 0.9675409197807312, Disc_loss: 0.9787530899047852, Test_loss: 0.5330758094787598\n",
      "Epoch 611, Gen_loss: 1.1127777099609375, Disc_loss: 0.8950533270835876, Test_loss: 0.5341390371322632\n",
      "Epoch 612, Gen_loss: 1.1231776475906372, Disc_loss: 0.8435662388801575, Test_loss: 0.4558660387992859\n",
      "Epoch 613, Gen_loss: 0.9742394089698792, Disc_loss: 0.8116246461868286, Test_loss: 0.1989945024251938\n",
      "Epoch 614, Gen_loss: 1.0129423141479492, Disc_loss: 0.6367197632789612, Test_loss: 0.241909459233284\n",
      "Epoch 615, Gen_loss: 0.9776033759117126, Disc_loss: 0.9477778077125549, Test_loss: 0.502744734287262\n",
      "Epoch 616, Gen_loss: 1.1445907354354858, Disc_loss: 0.8714158535003662, Test_loss: 0.42329463362693787\n",
      "Epoch 617, Gen_loss: 0.9879313111305237, Disc_loss: 1.0387064218521118, Test_loss: 0.8546299338340759\n",
      "Epoch 618, Gen_loss: 1.1265252828598022, Disc_loss: 1.183089256286621, Test_loss: 0.7639519572257996\n",
      "Epoch 619, Gen_loss: 1.1414090394973755, Disc_loss: 0.9783501625061035, Test_loss: 0.6109998822212219\n",
      "Epoch 620, Gen_loss: 1.1115248203277588, Disc_loss: 0.9279078841209412, Test_loss: 0.6753695607185364\n",
      "Epoch 621, Gen_loss: 1.0636322498321533, Disc_loss: 1.0072013139724731, Test_loss: 0.7138364911079407\n",
      "Epoch 622, Gen_loss: 1.066694974899292, Disc_loss: 1.0243573188781738, Test_loss: 0.6680192351341248\n",
      "Epoch 623, Gen_loss: 1.0984423160552979, Disc_loss: 0.9108625650405884, Test_loss: 0.4827890396118164\n",
      "Epoch 624, Gen_loss: 1.0224721431732178, Disc_loss: 0.9657867550849915, Test_loss: 0.69502192735672\n",
      "Epoch 625, Gen_loss: 1.0556696653366089, Disc_loss: 1.0331907272338867, Test_loss: 0.7732625007629395\n",
      "Epoch 626, Gen_loss: 1.0913487672805786, Disc_loss: 1.015113353729248, Test_loss: 0.6949437856674194\n",
      "Epoch 627, Gen_loss: 1.0712748765945435, Disc_loss: 0.9886093735694885, Test_loss: 0.6299636960029602\n",
      "Epoch 628, Gen_loss: 1.045891523361206, Disc_loss: 0.9284458756446838, Test_loss: 0.7193939089775085\n",
      "Epoch 629, Gen_loss: 1.0396512746810913, Disc_loss: 1.055066466331482, Test_loss: 0.6981766223907471\n",
      "Epoch 630, Gen_loss: 1.0660516023635864, Disc_loss: 1.0046913623809814, Test_loss: 0.6763560771942139\n",
      "Epoch 631, Gen_loss: 1.0644806623458862, Disc_loss: 0.9658789038658142, Test_loss: 0.6263067126274109\n",
      "Epoch 632, Gen_loss: 1.050346851348877, Disc_loss: 0.9754690527915955, Test_loss: 0.6828636527061462\n",
      "Epoch 633, Gen_loss: 1.0620849132537842, Disc_loss: 0.9012485146522522, Test_loss: 0.502945601940155\n",
      "Epoch 634, Gen_loss: 1.033401608467102, Disc_loss: 1.0323816537857056, Test_loss: 0.720604658126831\n",
      "Epoch 635, Gen_loss: 1.0628654956817627, Disc_loss: 1.0038518905639648, Test_loss: 0.65701824426651\n",
      "Epoch 636, Gen_loss: 1.0559357404708862, Disc_loss: 0.9755821824073792, Test_loss: 0.6028621792793274\n",
      "Epoch 637, Gen_loss: 1.0643337965011597, Disc_loss: 0.9413163065910339, Test_loss: 0.6949335932731628\n",
      "Epoch 638, Gen_loss: 1.0350613594055176, Disc_loss: 1.0061588287353516, Test_loss: 0.7718425989151001\n",
      "Epoch 639, Gen_loss: 1.0380817651748657, Disc_loss: 1.030569314956665, Test_loss: 0.5661386847496033\n",
      "Epoch 640, Gen_loss: 1.1020535230636597, Disc_loss: 0.8378111720085144, Test_loss: 0.2657723128795624\n",
      "Epoch 641, Gen_loss: 1.0178004503250122, Disc_loss: 0.6753453612327576, Test_loss: 0.214925155043602\n",
      "Epoch 642, Gen_loss: 0.9424068927764893, Disc_loss: 0.8195557594299316, Test_loss: 0.41075459122657776\n",
      "Epoch 643, Gen_loss: 1.0737042427062988, Disc_loss: 0.8579393029212952, Test_loss: 0.40462127327919006\n",
      "Epoch 644, Gen_loss: 1.1241285800933838, Disc_loss: 0.7486040592193604, Test_loss: 0.3923748731613159\n",
      "Epoch 645, Gen_loss: 1.0103334188461304, Disc_loss: 0.8566904664039612, Test_loss: 0.37733644247055054\n",
      "Epoch 646, Gen_loss: 1.0723387002944946, Disc_loss: 0.795033872127533, Test_loss: 0.401046484708786\n",
      "Epoch 647, Gen_loss: 1.039363145828247, Disc_loss: 0.8053305149078369, Test_loss: 0.3816663920879364\n",
      "Epoch 648, Gen_loss: 1.039780855178833, Disc_loss: 0.794142484664917, Test_loss: 0.342925488948822\n",
      "Epoch 649, Gen_loss: 1.0154398679733276, Disc_loss: 0.7024089097976685, Test_loss: 0.19543665647506714\n",
      "Epoch 650, Gen_loss: 1.0122904777526855, Disc_loss: 0.5986459255218506, Test_loss: 0.11068066209554672\n",
      "Epoch 651, Gen_loss: 0.9881423711776733, Disc_loss: 0.826863169670105, Test_loss: 0.49669161438941956\n",
      "Epoch 652, Gen_loss: 1.239945411682129, Disc_loss: 1.0614759922027588, Test_loss: 0.8074491024017334\n",
      "Epoch 653, Gen_loss: 1.1128053665161133, Disc_loss: 1.0070635080337524, Test_loss: 0.6335952877998352\n",
      "Epoch 654, Gen_loss: 1.089310646057129, Disc_loss: 0.9403622150421143, Test_loss: 0.6876178979873657\n",
      "Epoch 655, Gen_loss: 1.123766541481018, Disc_loss: 0.9723460674285889, Test_loss: 0.6502344608306885\n",
      "Epoch 656, Gen_loss: 1.1112614870071411, Disc_loss: 0.981716513633728, Test_loss: 0.6502948999404907\n",
      "Epoch 657, Gen_loss: 1.0913468599319458, Disc_loss: 0.9115181565284729, Test_loss: 0.3594187796115875\n",
      "Epoch 658, Gen_loss: 0.993427038192749, Disc_loss: 0.8567987084388733, Test_loss: 0.7739196419715881\n",
      "Epoch 659, Gen_loss: 1.0248347520828247, Disc_loss: 1.0388973951339722, Test_loss: 0.7063621878623962\n",
      "Epoch 660, Gen_loss: 1.1053192615509033, Disc_loss: 1.0308657884597778, Test_loss: 0.7121945023536682\n",
      "Epoch 661, Gen_loss: 1.073583960533142, Disc_loss: 1.0006738901138306, Test_loss: 0.6844761967658997\n",
      "Epoch 662, Gen_loss: 1.0874468088150024, Disc_loss: 0.9695528745651245, Test_loss: 0.6211304068565369\n",
      "Epoch 663, Gen_loss: 1.0558900833129883, Disc_loss: 0.8889015913009644, Test_loss: 0.6775268912315369\n",
      "Epoch 664, Gen_loss: 1.0676809549331665, Disc_loss: 1.057279348373413, Test_loss: 0.7125419974327087\n",
      "Epoch 665, Gen_loss: 1.0738928318023682, Disc_loss: 1.0353233814239502, Test_loss: 0.6887016892433167\n",
      "Epoch 666, Gen_loss: 1.0606805086135864, Disc_loss: 0.9976282119750977, Test_loss: 0.6588138937950134\n",
      "Epoch 667, Gen_loss: 1.0866618156433105, Disc_loss: 0.9554513096809387, Test_loss: 0.6224856376647949\n",
      "Epoch 668, Gen_loss: 1.0497087240219116, Disc_loss: 0.976139485836029, Test_loss: 0.7150086760520935\n",
      "Epoch 669, Gen_loss: 1.0243862867355347, Disc_loss: 1.028207540512085, Test_loss: 0.5472275018692017\n",
      "Epoch 670, Gen_loss: 1.1579500436782837, Disc_loss: 0.8317349553108215, Test_loss: 0.3473338484764099\n",
      "Epoch 671, Gen_loss: 1.0451898574829102, Disc_loss: 0.8592414259910583, Test_loss: 0.3862256407737732\n",
      "Epoch 672, Gen_loss: 1.0451451539993286, Disc_loss: 0.8081128597259521, Test_loss: 0.37250784039497375\n",
      "Epoch 673, Gen_loss: 1.0105305910110474, Disc_loss: 0.6950669288635254, Test_loss: 0.1652933806180954\n",
      "Epoch 674, Gen_loss: 1.0159631967544556, Disc_loss: 0.6516754031181335, Test_loss: 0.15925686061382294\n",
      "Epoch 675, Gen_loss: 1.0183292627334595, Disc_loss: 0.8266875147819519, Test_loss: 0.399494469165802\n",
      "Epoch 676, Gen_loss: 1.151798129081726, Disc_loss: 0.752763032913208, Test_loss: 0.47507190704345703\n",
      "Epoch 677, Gen_loss: 1.0872784852981567, Disc_loss: 0.7597905397415161, Test_loss: 0.27467793226242065\n",
      "Epoch 678, Gen_loss: 1.0448404550552368, Disc_loss: 0.6323621273040771, Test_loss: 0.18978852033615112\n",
      "Epoch 679, Gen_loss: 1.0583820343017578, Disc_loss: 0.5850629210472107, Test_loss: 0.1393233984708786\n",
      "Epoch 680, Gen_loss: 1.0322256088256836, Disc_loss: 0.84821617603302, Test_loss: 0.3750341832637787\n",
      "Epoch 681, Gen_loss: 1.1021617650985718, Disc_loss: 0.887627899646759, Test_loss: 0.40332651138305664\n",
      "Epoch 682, Gen_loss: 1.1553839445114136, Disc_loss: 0.738656759262085, Test_loss: 0.42139407992362976\n",
      "Epoch 683, Gen_loss: 1.101784110069275, Disc_loss: 0.7729026675224304, Test_loss: 0.3532121479511261\n",
      "Epoch 684, Gen_loss: 1.0502598285675049, Disc_loss: 0.821732759475708, Test_loss: 0.3224433958530426\n",
      "Epoch 685, Gen_loss: 1.0589075088500977, Disc_loss: 0.6648932099342346, Test_loss: 0.22067637741565704\n",
      "Epoch 686, Gen_loss: 1.0399556159973145, Disc_loss: 0.6538479924201965, Test_loss: 0.2545064389705658\n",
      "Epoch 687, Gen_loss: 0.9976773858070374, Disc_loss: 0.7351298928260803, Test_loss: 0.3798655569553375\n",
      "Epoch 688, Gen_loss: 1.1270004510879517, Disc_loss: 0.8457189202308655, Test_loss: 0.47739899158477783\n",
      "Epoch 689, Gen_loss: 1.1584612131118774, Disc_loss: 0.784907877445221, Test_loss: 0.4392711818218231\n",
      "Epoch 690, Gen_loss: 1.079338788986206, Disc_loss: 0.8080695867538452, Test_loss: 0.46599477529525757\n",
      "Epoch 691, Gen_loss: 1.1566526889801025, Disc_loss: 0.74875807762146, Test_loss: 0.4032861292362213\n",
      "Epoch 692, Gen_loss: 1.0929358005523682, Disc_loss: 0.7798996567726135, Test_loss: 0.38163843750953674\n",
      "Epoch 693, Gen_loss: 1.0882675647735596, Disc_loss: 0.7501586675643921, Test_loss: 0.3380906581878662\n",
      "Epoch 694, Gen_loss: 1.062002182006836, Disc_loss: 0.7824947834014893, Test_loss: 0.39945271611213684\n",
      "Epoch 695, Gen_loss: 1.0597436428070068, Disc_loss: 0.8288140296936035, Test_loss: 0.3904849886894226\n",
      "Epoch 696, Gen_loss: 1.1160401105880737, Disc_loss: 0.7340285181999207, Test_loss: 0.44313159584999084\n",
      "Epoch 697, Gen_loss: 1.0109165906906128, Disc_loss: 1.4201982021331787, Test_loss: 0.9241994619369507\n",
      "Epoch 698, Gen_loss: 1.2073547840118408, Disc_loss: 1.0524976253509521, Test_loss: 0.711079478263855\n",
      "Epoch 699, Gen_loss: 1.1741145849227905, Disc_loss: 0.8454217910766602, Test_loss: 0.7170649170875549\n",
      "Epoch 700, Gen_loss: 1.0918649435043335, Disc_loss: 0.9900426268577576, Test_loss: 0.6552373766899109\n",
      "Epoch 701, Gen_loss: 1.1261241436004639, Disc_loss: 1.012619137763977, Test_loss: 0.7430775165557861\n",
      "Epoch 702, Gen_loss: 1.1003154516220093, Disc_loss: 0.9896801710128784, Test_loss: 0.6733649969100952\n",
      "Epoch 703, Gen_loss: 1.101930022239685, Disc_loss: 0.9727715253829956, Test_loss: 0.6892173290252686\n",
      "Epoch 704, Gen_loss: 1.093616008758545, Disc_loss: 0.9793334007263184, Test_loss: 0.779107928276062\n",
      "Epoch 705, Gen_loss: 1.1332311630249023, Disc_loss: 0.9903704524040222, Test_loss: 0.628670871257782\n",
      "Epoch 706, Gen_loss: 1.1207531690597534, Disc_loss: 0.9783861041069031, Test_loss: 0.7611821889877319\n",
      "Epoch 707, Gen_loss: 1.0579367876052856, Disc_loss: 0.8893327713012695, Test_loss: 0.6415033340454102\n",
      "Epoch 708, Gen_loss: 1.0292770862579346, Disc_loss: 1.0503039360046387, Test_loss: 0.7242356538772583\n",
      "Epoch 709, Gen_loss: 1.1678005456924438, Disc_loss: 1.0107029676437378, Test_loss: 0.7822407484054565\n",
      "Epoch 710, Gen_loss: 1.0987944602966309, Disc_loss: 1.0069422721862793, Test_loss: 0.6626469492912292\n",
      "Epoch 711, Gen_loss: 1.119555115699768, Disc_loss: 0.9239234328269958, Test_loss: 0.6230853199958801\n",
      "Epoch 712, Gen_loss: 1.0723443031311035, Disc_loss: 1.0452954769134521, Test_loss: 0.5278040170669556\n",
      "Epoch 713, Gen_loss: 1.2034388780593872, Disc_loss: 0.8936285972595215, Test_loss: 0.609501838684082\n",
      "Epoch 714, Gen_loss: 1.1702882051467896, Disc_loss: 0.7854411602020264, Test_loss: 0.3838827908039093\n",
      "Epoch 715, Gen_loss: 1.0952364206314087, Disc_loss: 0.7809975743293762, Test_loss: 0.3688955008983612\n",
      "Epoch 716, Gen_loss: 1.0429209470748901, Disc_loss: 0.810729444026947, Test_loss: 0.39075347781181335\n",
      "Epoch 717, Gen_loss: 1.0381503105163574, Disc_loss: 0.6865218877792358, Test_loss: 0.261182963848114\n",
      "Epoch 718, Gen_loss: 1.004488468170166, Disc_loss: 0.7390270829200745, Test_loss: 0.4914913773536682\n",
      "Epoch 719, Gen_loss: 1.140669345855713, Disc_loss: 0.8375802636146545, Test_loss: 0.4679948687553406\n",
      "Epoch 720, Gen_loss: 1.1562994718551636, Disc_loss: 0.8006945252418518, Test_loss: 0.3631560206413269\n",
      "Epoch 721, Gen_loss: 1.1024795770645142, Disc_loss: 0.753009021282196, Test_loss: 0.3211749196052551\n",
      "Epoch 722, Gen_loss: 1.10335111618042, Disc_loss: 0.7724040746688843, Test_loss: 0.41121459007263184\n",
      "Epoch 723, Gen_loss: 1.1164698600769043, Disc_loss: 0.7930984497070312, Test_loss: 0.3554227352142334\n",
      "Epoch 724, Gen_loss: 1.0682373046875, Disc_loss: 0.7887226343154907, Test_loss: 0.44439205527305603\n",
      "Epoch 725, Gen_loss: 1.1513261795043945, Disc_loss: 0.7575307488441467, Test_loss: 0.3872879147529602\n",
      "Epoch 726, Gen_loss: 1.1419864892959595, Disc_loss: 0.8128238320350647, Test_loss: 0.4337633550167084\n",
      "Epoch 727, Gen_loss: 1.1270347833633423, Disc_loss: 0.757361114025116, Test_loss: 0.3629678785800934\n",
      "Epoch 728, Gen_loss: 1.0333296060562134, Disc_loss: 0.8085499405860901, Test_loss: 0.27743226289749146\n",
      "Epoch 729, Gen_loss: 1.060292363166809, Disc_loss: 1.132285475730896, Test_loss: 1.016882061958313\n",
      "Epoch 730, Gen_loss: 1.2790329456329346, Disc_loss: 1.037537693977356, Test_loss: 0.7641991972923279\n",
      "Epoch 731, Gen_loss: 1.2319484949111938, Disc_loss: 0.9406222701072693, Test_loss: 0.6237649917602539\n",
      "Epoch 732, Gen_loss: 1.1182314157485962, Disc_loss: 0.9431983232498169, Test_loss: 0.6301547884941101\n",
      "Epoch 733, Gen_loss: 1.1296031475067139, Disc_loss: 0.9923297166824341, Test_loss: 0.7614261507987976\n",
      "Epoch 734, Gen_loss: 1.1323496103286743, Disc_loss: 0.78809654712677, Test_loss: 0.4659765660762787\n",
      "Epoch 735, Gen_loss: 0.9779328107833862, Disc_loss: 1.1200484037399292, Test_loss: 0.7756929397583008\n",
      "Epoch 736, Gen_loss: 1.1674327850341797, Disc_loss: 1.0178625583648682, Test_loss: 0.7103831768035889\n",
      "Epoch 737, Gen_loss: 1.1942278146743774, Disc_loss: 0.9762257933616638, Test_loss: 0.7204682230949402\n",
      "Epoch 738, Gen_loss: 1.083156943321228, Disc_loss: 0.9984636306762695, Test_loss: 0.72492516040802\n",
      "Epoch 739, Gen_loss: 1.0801527500152588, Disc_loss: 0.9571306705474854, Test_loss: 0.6694294810295105\n",
      "Epoch 740, Gen_loss: 1.1262727975845337, Disc_loss: 0.9764692783355713, Test_loss: 0.757774293422699\n",
      "Epoch 741, Gen_loss: 1.0949803590774536, Disc_loss: 1.038155198097229, Test_loss: 0.7212443947792053\n",
      "Epoch 742, Gen_loss: 1.1149381399154663, Disc_loss: 0.9270820021629333, Test_loss: 0.7140242457389832\n",
      "Epoch 743, Gen_loss: 1.155285120010376, Disc_loss: 0.9688196182250977, Test_loss: 0.7028478980064392\n",
      "Epoch 744, Gen_loss: 1.124122142791748, Disc_loss: 0.9719099402427673, Test_loss: 0.5342223644256592\n",
      "Epoch 745, Gen_loss: 1.0410733222961426, Disc_loss: 0.9743207097053528, Test_loss: 0.7769715189933777\n",
      "Epoch 746, Gen_loss: 1.123688817024231, Disc_loss: 1.0083012580871582, Test_loss: 0.708452582359314\n",
      "Epoch 747, Gen_loss: 1.1363476514816284, Disc_loss: 1.0135260820388794, Test_loss: 0.7143071293830872\n",
      "Epoch 748, Gen_loss: 1.1423360109329224, Disc_loss: 0.9626577496528625, Test_loss: 0.7095823287963867\n",
      "Epoch 749, Gen_loss: 1.141653060913086, Disc_loss: 0.9422822594642639, Test_loss: 0.633696973323822\n",
      "Epoch 750, Gen_loss: 1.0684492588043213, Disc_loss: 0.9567062258720398, Test_loss: 0.7871140837669373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9f55ee96bdc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgenerated_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdf_gen_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame_from_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gencell_ep'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdf_gen_prof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/gen_prof_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             )\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Running...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Save checkpoints and gen example data\n",
    "    if epoch % 10 == 0:   \n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        # Generate a profile set\n",
    "        noise = gen_noise(EX_GEN_BATCH_SIZE)\n",
    "        generated_profile = generator(noise, training=False)\n",
    "        df_gen_prof = data_frame_from_gen(generated_profile, 'gencell_ep' + str(epoch) + '_')\n",
    "        df_gen_prof.to_csv(data_path + '/gen_prof_' + str(epoch) + '.csv')\n",
    "    \n",
    "    # Logging\n",
    "    start = time.time()\n",
    "    \n",
    "    #Train the epoch\n",
    "    for data_batch in train_dataset:\n",
    "        train_step(data_batch)\n",
    "        \n",
    "    #Run test data through discriminator\n",
    "    for data_batch in test_dataset:\n",
    "        test_decision = discriminator(data_batch, training=False)\n",
    "\n",
    "    test_loss = cross_entropy(tf.ones_like(test_decision), test_decision)\n",
    "    met_test_loss(test_loss)\n",
    "    \n",
    "    #Log metrics\n",
    "    with all_summary_writer.as_default():\n",
    "        tf.summary.scalar('2_gen_loss', met_gen_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_disc_loss', met_disc_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_test_loss', met_test_loss.result(), step=epoch)\n",
    "    \n",
    "    with gen_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_gen_loss.result(), step=epoch)\n",
    "           \n",
    "    with disc_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_disc_loss.result(), step=epoch)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_test_loss.result(), step=epoch)\n",
    "\n",
    "    # Logging\n",
    "    #print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "    time.time()\n",
    "      \n",
    "    #Log stats\n",
    "    template = 'Epoch {}, Gen_loss: {}, Disc_loss: {}, Test_loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                           met_gen_loss.result(), \n",
    "                           met_disc_loss.result(),\n",
    "                           met_test_loss.result()))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    met_gen_loss.reset_states()\n",
    "    met_disc_loss.reset_states()\n",
    "    met_test_loss.reset_states()\n",
    "    \n",
    "# Generate a profile set\n",
    "noise = gen_noise(EX_GEN_BATCH_SIZE)\n",
    "generated_profile = generator(noise, training=False)\n",
    "df_gen_prof = data_frame_from_gen(generated_profile, 'gencell_ep' + str(EPOCHS) + '_')\n",
    "df_gen_prof.to_csv(data_path + '/gen_prof_' + str(EPOCHS) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {train_log_dir} --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
