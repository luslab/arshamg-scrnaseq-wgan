{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN - CHECK SPELLING\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "\n",
    "train_feature_path = data_path + '/tpm_combined.csv'\n",
    "train_gene_name_path = data_path + '/tpm_combined_rows.csv'\n",
    "train_cell_name_path = data_path + '/tpm_combined_cols.csv'\n",
    "\n",
    "test_feature_path = data_path + '/tpm_combined_test.csv'\n",
    "test_gene_name_path = data_path + '/tpm_combined_rows_test.csv'\n",
    "test_cell_name_path = data_path + '/tpm_combined_cols_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load datasets into frames and check all the shapes match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n",
      "(1798, 1)\n",
      "(6807, 1798)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names = pd.read_csv(train_gene_name_path, header=None)\n",
    "df_cell_names = pd.read_csv(train_cell_name_path, header=None)\n",
    "df_training_data = pd.read_csv(train_feature_path, header=None)\n",
    "\n",
    "print(df_gene_names.shape)\n",
    "print(df_cell_names.shape)\n",
    "print(df_training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n",
      "(500, 1)\n",
      "(6807, 500)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names_test = pd.read_csv(test_gene_name_path, header=None)\n",
    "df_cell_names_test = pd.read_csv(test_cell_name_path, header=None)\n",
    "df_test_data = pd.read_csv(test_feature_path, header=None)\n",
    "\n",
    "print(df_gene_names_test.shape)\n",
    "print(df_cell_names_test.shape)\n",
    "print(df_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of genes in the input dataset determines the generator output as well as the dicriminator inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_genes = df_gene_names.shape[0]\n",
    "df_gene_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1788</th>\n",
       "      <th>1789</th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.437635</td>\n",
       "      <td>0.422952</td>\n",
       "      <td>0.500639</td>\n",
       "      <td>0.460063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625194</td>\n",
       "      <td>0.636415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.621385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806046</td>\n",
       "      <td>0.461961</td>\n",
       "      <td>0.533844</td>\n",
       "      <td>0.691345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479120</td>\n",
       "      <td>0.373748</td>\n",
       "      <td>0.779602</td>\n",
       "      <td>0.652176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330974</td>\n",
       "      <td>0.599975</td>\n",
       "      <td>0.538749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501173</td>\n",
       "      <td>0.315232</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.528174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.423527</td>\n",
       "      <td>0.412909</td>\n",
       "      <td>0.488703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>0.522253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440998</td>\n",
       "      <td>0.837320</td>\n",
       "      <td>0.353001</td>\n",
       "      <td>0.536904</td>\n",
       "      <td>0.983997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614253</td>\n",
       "      <td>0.823296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379869</td>\n",
       "      <td>0.381094</td>\n",
       "      <td>0.137787</td>\n",
       "      <td>0.309987</td>\n",
       "      <td>0.571913</td>\n",
       "      <td>0.548388</td>\n",
       "      <td>0.384074</td>\n",
       "      <td>0.455198</td>\n",
       "      <td>0.469107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428848</td>\n",
       "      <td>0.816352</td>\n",
       "      <td>0.531461</td>\n",
       "      <td>0.244717</td>\n",
       "      <td>0.813174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327914</td>\n",
       "      <td>0.048955</td>\n",
       "      <td>0.729328</td>\n",
       "      <td>0.346599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666779</td>\n",
       "      <td>0.565507</td>\n",
       "      <td>0.350199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561451</td>\n",
       "      <td>0.339109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652654</td>\n",
       "      <td>0.036969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158096</td>\n",
       "      <td>0.404081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609758</td>\n",
       "      <td>0.521244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525897</td>\n",
       "      <td>0.672412</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411294</td>\n",
       "      <td>0.481663</td>\n",
       "      <td>0.346585</td>\n",
       "      <td>0.315634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307813</td>\n",
       "      <td>0.399935</td>\n",
       "      <td>0.519273</td>\n",
       "      <td>0.468670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541795</td>\n",
       "      <td>0.634956</td>\n",
       "      <td>0.370401</td>\n",
       "      <td>0.348183</td>\n",
       "      <td>0.443963</td>\n",
       "      <td>0.396523</td>\n",
       "      <td>0.646611</td>\n",
       "      <td>0.477193</td>\n",
       "      <td>0.585492</td>\n",
       "      <td>0.497864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>0.307292</td>\n",
       "      <td>0.582062</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592147</td>\n",
       "      <td>0.534678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584783</td>\n",
       "      <td>0.670761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659579</td>\n",
       "      <td>0.540393</td>\n",
       "      <td>0.564257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642897</td>\n",
       "      <td>0.555258</td>\n",
       "      <td>0.136405</td>\n",
       "      <td>0.577916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.665896</td>\n",
       "      <td>0.659554</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597672</td>\n",
       "      <td>0.154375</td>\n",
       "      <td>0.653057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392436</td>\n",
       "      <td>0.773484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362291</td>\n",
       "      <td>0.740020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389472</td>\n",
       "      <td>0.754507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6807 rows Ã— 1798 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.437635  0.422952  0.500639  0.460063  0.000000  0.625194  0.636415   \n",
       "1     0.000000  0.330974  0.599975  0.538749  0.000000  0.000000  0.000000   \n",
       "2     0.423527  0.412909  0.488703  0.000000  0.602726  0.522253  0.000000   \n",
       "3     0.000000  0.379869  0.381094  0.137787  0.309987  0.571913  0.548388   \n",
       "4     0.000000  0.666779  0.565507  0.350199  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6802  0.000000  0.000000  0.158096  0.404081  0.000000  0.000000  0.000000   \n",
       "6803  0.000000  0.411294  0.481663  0.346585  0.315634  0.000000  0.307813   \n",
       "6804  0.307292  0.582062  0.466408  0.000000  0.328192  0.000000  0.000000   \n",
       "6805  0.000000  0.659579  0.540393  0.564257  0.000000  0.000000  0.000000   \n",
       "6806  0.000000  0.597672  0.154375  0.653057  0.000000  0.000000  0.000000   \n",
       "\n",
       "          7         8         9     ...      1788      1789      1790  \\\n",
       "0     0.000000  0.000000  0.621385  ...  0.000000  0.806046  0.461961   \n",
       "1     0.000000  0.000000  0.000000  ...  0.000000  0.806895  0.000000   \n",
       "2     0.541224  0.000000  0.000000  ...  0.440998  0.837320  0.353001   \n",
       "3     0.384074  0.455198  0.469107  ...  0.428848  0.816352  0.531461   \n",
       "4     0.000000  0.000000  0.000000  ...  0.000000  0.171824  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6802  0.720074  0.000000  0.490125  ...  0.000000  0.609758  0.521244   \n",
       "6803  0.399935  0.519273  0.468670  ...  0.541795  0.634956  0.370401   \n",
       "6804  0.000000  0.000000  0.000000  ...  0.000000  0.574837  0.000000   \n",
       "6805  0.666009  0.000000  0.000000  ...  0.000000  0.642897  0.555258   \n",
       "6806  0.000000  0.000000  0.000000  ...  0.392436  0.773484  0.000000   \n",
       "\n",
       "          1791      1792      1793      1794      1795      1796      1797  \n",
       "0     0.533844  0.691345  0.000000  0.479120  0.373748  0.779602  0.652176  \n",
       "1     0.000000  0.765089  0.000000  0.501173  0.315232  0.255972  0.528174  \n",
       "2     0.536904  0.983997  0.000000  0.000000  0.614253  0.823296  0.000000  \n",
       "3     0.244717  0.813174  0.000000  0.327914  0.048955  0.729328  0.346599  \n",
       "4     0.561451  0.339109  0.000000  0.000000  0.652654  0.036969  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6802  0.000000  0.297598  0.000000  0.000000  0.525897  0.672412  0.000000  \n",
       "6803  0.348183  0.443963  0.396523  0.646611  0.477193  0.585492  0.497864  \n",
       "6804  0.592147  0.534678  0.000000  0.000000  0.584783  0.670761  0.000000  \n",
       "6805  0.136405  0.577916  0.000000  0.459978  0.665896  0.659554  0.000000  \n",
       "6806  0.362291  0.740020  0.000000  0.000000  0.389472  0.754507  0.000000  \n",
       "\n",
       "[6807 rows x 1798 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model variables - COMMENT ON EACH ONE TO DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LATENT_VARIABLE_SIZE = 100\n",
    "GEN_L1_DENSE_SIZE = 600\n",
    "GEN_L2_DENSE_SIZE = 600\n",
    "GEN_L3_DENSE_SIZE = num_genes\n",
    "\n",
    "DIS_INPUT_SIZE = num_genes\n",
    "DIS_L1_DENSE_SIZE = 200\n",
    "DIS_L2_DENSE_SIZE = 200\n",
    "\n",
    "NOISE_STDEV = 0.1\n",
    "POISSON_LAM = 1\n",
    "\n",
    "# Training params\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_BUFFER_SIZE = 10000\n",
    "TEST_BATCH_SIZE = 500\n",
    "TEST_BUFFER_SIZE = 500\n",
    "GEN_BATCH_SIZE = 10\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test datasets\n",
    "\n",
    "Create tensors from training data - Convert to Int32 for better work on GPU with batch and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 6807), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(df_training_data.T.values.astype('float32')).shuffle(TRAIN_BUFFER_SIZE).batch(TRAIN_BATCH_SIZE)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 6807), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(df_test_data.T.values.astype('float32')).shuffle(TEST_BUFFER_SIZE).batch(TEST_BATCH_SIZE)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GANN model\n",
    "\n",
    "Define function for contructing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(GEN_L1_DENSE_SIZE, use_bias=False, input_shape=(LATENT_VARIABLE_SIZE,)))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L1_DENSE_SIZE, 1)  # Note: None is the batch size\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(GEN_L2_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L2_DENSE_SIZE, 1)\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Dense(GEN_L3_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L3_DENSE_SIZE, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for constructing discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(DIS_L1_DENSE_SIZE, use_bias=False, input_shape=(DIS_INPUT_SIZE,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(DIS_L2_DENSE_SIZE, use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise():\n",
    "    # Create some random noise for the generator\n",
    "    n_noise = tf.random.normal([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], mean=0.0, stddev=NOISE_STDEV)\n",
    "    p_noise = tf.random.poisson([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], lam=POISSON_LAM)\n",
    "    noise = tf.abs(n_noise + p_noise)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "    #total_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #total_loss = -tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a batch of real cell profiles from the training set\n",
    "# @tf.function\n",
    "def train_step(cell_profiles):\n",
    "    noise = gen_noise()\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_profiles = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(cell_profiles, training=True)\n",
    "        fake_output = discriminator(generated_profiles, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        met_gen_loss(gen_loss)\n",
    "        met_disc_loss(disc_loss)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GANN model\n",
    "\n",
    "Create generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from test data to check network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6807)\n",
      "-0.22056821\n",
      "0.6908798\n",
      "[[0.01891878]\n",
      " [0.06468181]\n",
      " [0.3347569 ]\n",
      " [0.08462244]\n",
      " [0.16778599]\n",
      " [0.04435993]\n",
      " [0.0873422 ]\n",
      " [0.12802176]\n",
      " [0.04316174]\n",
      " [0.16580933]]\n"
     ]
    }
   ],
   "source": [
    "noise = gen_noise()\n",
    "generated_profile = generator(noise, training=False)\n",
    "print(generated_profile.shape)\n",
    "print(generated_profile.numpy().min())\n",
    "print(generated_profile.numpy().max())\n",
    "\n",
    "decision = discriminator(generated_profile)\n",
    "#print(decision.shape)\n",
    "print(decision.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GANN\n",
    "\n",
    "Define tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_gen_loss = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
    "met_disc_loss = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)\n",
    "met_test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create log directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "gen_log_dir = 'logs/gradient_tape/' + current_time + '/gen_train'\n",
    "disc_log_dir = 'logs/gradient_tape/' + current_time + '/disc_train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/disc_test'\n",
    "all_log_dir = 'logs/gradient_tape/' + current_time + '/all'\n",
    "\n",
    "all_summary_writer = tf.summary.create_file_writer(all_log_dir)\n",
    "gen_summary_writer = tf.summary.create_file_writer(gen_log_dir)\n",
    "disc_summary_writer = tf.summary.create_file_writer(disc_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Epoch 1, Gen_loss: 1192.5345458984375, Disc_loss: 111.31814575195312, Test_loss: 26.741315841674805\n",
      "Epoch 2, Gen_loss: 8.587278366088867, Disc_loss: 13.908161163330078, Test_loss: 0.8916416764259338\n",
      "Epoch 3, Gen_loss: 11.624980926513672, Disc_loss: 4.125429630279541, Test_loss: 1.1865520477294922\n",
      "Epoch 4, Gen_loss: 30.56155014038086, Disc_loss: 2.6224136352539062, Test_loss: 1.1014779806137085\n",
      "Epoch 5, Gen_loss: 32.0594482421875, Disc_loss: 2.7788422107696533, Test_loss: 0.3355412185192108\n",
      "Epoch 6, Gen_loss: 19.79607582092285, Disc_loss: 0.7746967673301697, Test_loss: 0.27555131912231445\n",
      "Epoch 7, Gen_loss: 16.76173973083496, Disc_loss: 0.762294590473175, Test_loss: 0.41003280878067017\n",
      "Epoch 8, Gen_loss: 7.910008430480957, Disc_loss: 0.6947462558746338, Test_loss: 0.8840394020080566\n",
      "Epoch 9, Gen_loss: 19.745668411254883, Disc_loss: 0.8389370441436768, Test_loss: 0.7124609351158142\n",
      "Time for epoch 10 is 11.10207462310791 sec.\n",
      "Epoch 10, Gen_loss: 11.881868362426758, Disc_loss: 0.7658928036689758, Test_loss: 0.5140447616577148\n",
      "Epoch 11, Gen_loss: 13.13477611541748, Disc_loss: 1.0020712614059448, Test_loss: 0.4266798496246338\n",
      "Epoch 12, Gen_loss: 19.23836898803711, Disc_loss: 0.9481934309005737, Test_loss: 0.2021850198507309\n",
      "Epoch 13, Gen_loss: 17.74778175354004, Disc_loss: 0.665607213973999, Test_loss: 0.4527863562107086\n",
      "Epoch 14, Gen_loss: 9.901998519897461, Disc_loss: 0.5247420072555542, Test_loss: 0.24562224745750427\n",
      "Epoch 15, Gen_loss: 10.695779800415039, Disc_loss: 0.4739033281803131, Test_loss: 0.36054766178131104\n",
      "Epoch 16, Gen_loss: 6.399227619171143, Disc_loss: 0.49483540654182434, Test_loss: 0.2770991921424866\n",
      "Epoch 17, Gen_loss: 7.938018321990967, Disc_loss: 0.45632052421569824, Test_loss: 0.04527272656559944\n",
      "Epoch 18, Gen_loss: 8.343562126159668, Disc_loss: 0.47397372126579285, Test_loss: 0.34198132157325745\n",
      "Epoch 19, Gen_loss: 5.469278812408447, Disc_loss: 0.3247705101966858, Test_loss: 0.38999128341674805\n",
      "Time for epoch 20 is 8.584950685501099 sec.\n",
      "Epoch 20, Gen_loss: 3.862210512161255, Disc_loss: 0.27416637539863586, Test_loss: 0.21502339839935303\n",
      "Epoch 21, Gen_loss: 4.8401031494140625, Disc_loss: 0.27364087104797363, Test_loss: 0.30443429946899414\n",
      "Epoch 22, Gen_loss: 3.759885549545288, Disc_loss: 0.31746533513069153, Test_loss: 0.16317856311798096\n",
      "Epoch 23, Gen_loss: 3.6066651344299316, Disc_loss: 0.262783408164978, Test_loss: 0.18263056874275208\n",
      "Epoch 24, Gen_loss: 3.2073185443878174, Disc_loss: 0.32181259989738464, Test_loss: 0.3186132609844208\n",
      "Epoch 25, Gen_loss: 3.163041591644287, Disc_loss: 0.27133065462112427, Test_loss: 0.13350191712379456\n",
      "Epoch 26, Gen_loss: 3.030086040496826, Disc_loss: 0.31278204917907715, Test_loss: 0.32340508699417114\n",
      "Epoch 27, Gen_loss: 2.88279390335083, Disc_loss: 0.2747800648212433, Test_loss: 0.29732197523117065\n",
      "Epoch 28, Gen_loss: 2.6247596740722656, Disc_loss: 0.32021889090538025, Test_loss: 0.12717270851135254\n",
      "Epoch 29, Gen_loss: 2.834615468978882, Disc_loss: 0.28423187136650085, Test_loss: 0.1994689404964447\n",
      "Time for epoch 30 is 9.30461072921753 sec.\n",
      "Epoch 30, Gen_loss: 2.663532257080078, Disc_loss: 0.2733842730522156, Test_loss: 0.1567896455526352\n",
      "Epoch 31, Gen_loss: 2.4105849266052246, Disc_loss: 0.2991791367530823, Test_loss: 0.21511057019233704\n",
      "Epoch 32, Gen_loss: 2.70939302444458, Disc_loss: 0.2967202961444855, Test_loss: 0.043945491313934326\n",
      "Epoch 33, Gen_loss: 2.5335373878479004, Disc_loss: 0.3086507022380829, Test_loss: 0.3456994891166687\n",
      "Epoch 34, Gen_loss: 5.382375240325928, Disc_loss: 0.37184733152389526, Test_loss: 1.5460667610168457\n",
      "Epoch 35, Gen_loss: 4.42287015914917, Disc_loss: 0.27732980251312256, Test_loss: 0.22462967038154602\n",
      "Epoch 36, Gen_loss: 2.338911294937134, Disc_loss: 0.27864140272140503, Test_loss: 0.1497630476951599\n",
      "Epoch 37, Gen_loss: 2.226266384124756, Disc_loss: 0.28133758902549744, Test_loss: 0.2517072856426239\n",
      "Epoch 38, Gen_loss: 7.840942859649658, Disc_loss: 0.4548206031322479, Test_loss: 0.036924149841070175\n",
      "Epoch 39, Gen_loss: 16.69700050354004, Disc_loss: 0.7994620203971863, Test_loss: 0.3357877731323242\n",
      "Time for epoch 40 is 9.204581022262573 sec.\n",
      "Epoch 40, Gen_loss: 3.8562800884246826, Disc_loss: 0.23296266794204712, Test_loss: 0.17208196222782135\n",
      "Epoch 41, Gen_loss: 3.006195068359375, Disc_loss: 0.2408977746963501, Test_loss: 0.16142505407333374\n",
      "Epoch 42, Gen_loss: 2.666306734085083, Disc_loss: 0.23404522240161896, Test_loss: 0.10404312610626221\n",
      "Epoch 43, Gen_loss: 14.656481742858887, Disc_loss: 0.727394163608551, Test_loss: 0.17352168262004852\n",
      "Epoch 44, Gen_loss: 46952.765625, Disc_loss: 2913.518798828125, Test_loss: 679.3778686523438\n",
      "Epoch 45, Gen_loss: 5821.83740234375, Disc_loss: 623.763427734375, Test_loss: 380.1045837402344\n",
      "Epoch 46, Gen_loss: 1111.7581787109375, Disc_loss: 269.8316345214844, Test_loss: 153.87921142578125\n",
      "Epoch 47, Gen_loss: 784.4549560546875, Disc_loss: 89.47172546386719, Test_loss: 37.87534713745117\n",
      "Epoch 48, Gen_loss: 420.86444091796875, Disc_loss: 36.40925979614258, Test_loss: 25.206924438476562\n",
      "Epoch 49, Gen_loss: 293.9687805175781, Disc_loss: 25.853445053100586, Test_loss: 22.923635482788086\n",
      "Time for epoch 50 is 9.659350872039795 sec.\n",
      "Epoch 50, Gen_loss: 316.31524658203125, Disc_loss: 15.099373817443848, Test_loss: 14.585467338562012\n"
     ]
    }
   ],
   "source": [
    "print('Running...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    #Train the epoch\n",
    "    for data_batch in train_dataset:\n",
    "        train_step(data_batch)\n",
    "        \n",
    "    #Run test data through discriminator\n",
    "    for data_batch in test_dataset:\n",
    "        test_decision = discriminator(data_batch, training=False)\n",
    "\n",
    "    test_loss = cross_entropy(tf.ones_like(test_decision), test_decision)\n",
    "    met_test_loss(test_loss)\n",
    "    \n",
    "    #Log metrics\n",
    "    with all_summary_writer.as_default():\n",
    "        tf.summary.scalar('2_gen_loss', met_gen_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_disc_loss', met_disc_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_test_loss', met_test_loss.result(), step=epoch)\n",
    "    \n",
    "    with gen_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_gen_loss.result(), step=epoch)\n",
    "           \n",
    "    with disc_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_disc_loss.result(), step=epoch)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_test_loss.result(), step=epoch)\n",
    "    \n",
    "    #Do some basic time logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    else:\n",
    "        time.time()\n",
    "    \n",
    "    #Log stats\n",
    "    template = 'Epoch {}, Gen_loss: {}, Disc_loss: {}, Test_loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                           met_gen_loss.result(), \n",
    "                           met_disc_loss.result(),\n",
    "                           met_test_loss.result()))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    met_gen_loss.reset_states()\n",
    "    met_disc_loss.reset_states()\n",
    "    met_test_loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {train_log_dir} --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
