{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN - CHECK SPELLING\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "feature_path = data_path + '/tpm_combined.csv'\n",
    "gene_name_path = data_path + '/tpm_combined_rows.csv'\n",
    "cell_name_path = data_path + '/tpm_combined_cols.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load datasets into frames and check all the shapes match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19248, 1)\n",
      "(771, 1)\n",
      "(19248, 771)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names = pd.read_csv(gene_name_path, header=None)\n",
    "df_cell_names = pd.read_csv(cell_name_path, header=None)\n",
    "df_training_data = pd.read_csv(feature_path, header=None)\n",
    "\n",
    "print(df_gene_names.shape)\n",
    "print(df_cell_names.shape)\n",
    "print(df_training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of genes in the input dataset determines the generator output as well as the dicriminator inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19248, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_genes = df_gene_names.shape[0]\n",
    "df_gene_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19248 entries, 0 to 19247\n",
      "Columns: 771 entries, 0 to 770\n",
      "dtypes: float64(771)\n",
      "memory usage: 113.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "      <th>769</th>\n",
       "      <th>770</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.217231</td>\n",
       "      <td>3.003602</td>\n",
       "      <td>4.209453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.296824</td>\n",
       "      <td>5.300856</td>\n",
       "      <td>5.587965</td>\n",
       "      <td>3.826803</td>\n",
       "      <td>3.414136</td>\n",
       "      <td>4.888013</td>\n",
       "      <td>...</td>\n",
       "      <td>4.441616</td>\n",
       "      <td>3.732269</td>\n",
       "      <td>3.347666</td>\n",
       "      <td>3.945795</td>\n",
       "      <td>3.503349</td>\n",
       "      <td>1.992768</td>\n",
       "      <td>0.739848</td>\n",
       "      <td>4.901108</td>\n",
       "      <td>4.623516</td>\n",
       "      <td>4.512859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.547203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.368768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.269033</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.821838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.835924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.109361</td>\n",
       "      <td>5.938286</td>\n",
       "      <td>5.093391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.693766</td>\n",
       "      <td>4.627607</td>\n",
       "      <td>6.537141</td>\n",
       "      <td>3.842979</td>\n",
       "      <td>2.786596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.545968</td>\n",
       "      <td>0.641546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.670161</td>\n",
       "      <td>3.389567</td>\n",
       "      <td>5.522307</td>\n",
       "      <td>3.014355</td>\n",
       "      <td>6.703627</td>\n",
       "      <td>2.608809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19243</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19244</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.440952</td>\n",
       "      <td>2.097611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.385431</td>\n",
       "      <td>3.339137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.599318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.193378</td>\n",
       "      <td>6.386294</td>\n",
       "      <td>2.114367</td>\n",
       "      <td>6.742680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.936402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.265287</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19245</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19246</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>7.179013</td>\n",
       "      <td>10.450252</td>\n",
       "      <td>8.366235</td>\n",
       "      <td>8.709325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.797662</td>\n",
       "      <td>9.444497</td>\n",
       "      <td>9.306312</td>\n",
       "      <td>6.193575</td>\n",
       "      <td>7.741602</td>\n",
       "      <td>...</td>\n",
       "      <td>11.506675</td>\n",
       "      <td>11.987982</td>\n",
       "      <td>10.014341</td>\n",
       "      <td>10.988521</td>\n",
       "      <td>9.570615</td>\n",
       "      <td>11.136901</td>\n",
       "      <td>11.464178</td>\n",
       "      <td>9.998844</td>\n",
       "      <td>10.938102</td>\n",
       "      <td>10.111149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19248 rows × 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2         3         4         5         6    \\\n",
       "0      4.217231   3.003602  4.209453  0.000000  5.296824  5.300856  5.587965   \n",
       "1      0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2      0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3      0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4      3.109361   5.938286  5.093391  0.000000  0.000000  2.693766  4.627607   \n",
       "...         ...        ...       ...       ...       ...       ...       ...   \n",
       "19243  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19244  0.000000   5.440952  2.097611  0.000000  0.000000  3.385431  3.339137   \n",
       "19245  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19246  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19247  7.179013  10.450252  8.366235  8.709325  0.000000  7.797662  9.444497   \n",
       "\n",
       "            7         8         9    ...        761        762        763  \\\n",
       "0      3.826803  3.414136  4.888013  ...   4.441616   3.732269   3.347666   \n",
       "1      0.000000  0.000000  0.000000  ...   3.547203   0.000000   0.000000   \n",
       "2      6.821838  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "3      0.000000  0.000000  3.835924  ...   0.000000   0.000000   0.000000   \n",
       "4      6.537141  3.842979  2.786596  ...   0.000000   2.545968   0.641546   \n",
       "...         ...       ...       ...  ...        ...        ...        ...   \n",
       "19243  0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "19244  0.000000  5.599318  0.000000  ...   6.193378   6.386294   2.114367   \n",
       "19245  0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "19246  0.000000  0.000000  0.000000  ...   0.000000   0.000000   0.000000   \n",
       "19247  9.306312  6.193575  7.741602  ...  11.506675  11.987982  10.014341   \n",
       "\n",
       "             764       765        766        767       768        769  \\\n",
       "0       3.945795  3.503349   1.992768   0.739848  4.901108   4.623516   \n",
       "1       0.000000  0.000000   3.368768   0.000000  0.000000   1.269033   \n",
       "2       0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "3       0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "4       0.000000  3.670161   3.389567   5.522307  3.014355   6.703627   \n",
       "...          ...       ...        ...        ...       ...        ...   \n",
       "19243   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "19244   6.742680  0.000000   5.936402   0.000000  0.000000   6.265287   \n",
       "19245   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "19246   0.000000  0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "19247  10.988521  9.570615  11.136901  11.464178  9.998844  10.938102   \n",
       "\n",
       "             770  \n",
       "0       4.512859  \n",
       "1       0.000000  \n",
       "2       0.000000  \n",
       "3       0.000000  \n",
       "4       2.608809  \n",
       "...          ...  \n",
       "19243   0.000000  \n",
       "19244   0.000000  \n",
       "19245   0.000000  \n",
       "19246   0.000000  \n",
       "19247  10.111149  \n",
       "\n",
       "[19248 rows x 771 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.161062384674267\n"
     ]
    }
   ],
   "source": [
    "training_data_max = df_training_data.max()\n",
    "training_data_max = training_data_max.max()\n",
    "print(training_data_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "(19248,)\n"
     ]
    }
   ],
   "source": [
    "np_training_data = df_training_data.T.values\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(np_training_data))\n",
    "\n",
    "# Check which dimension we are fitting to - if we are fitting to gene expression then should be equal to number of genes\n",
    "print(scaler.data_max_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19248, 771)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_training_data_norm = np.transpose(scaler.transform(np_training_data))\n",
    "np_training_data_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get max values for noise generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "training_data_max = np_training_data_norm.max()\n",
    "training_data_max = training_data_max.max()\n",
    "print(training_data_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model variables - COMMENT ON EACH ONE TO DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LATENT_VARIABLE_SIZE = 100\n",
    "GEN_L1_DENSE_SIZE = 600\n",
    "GEN_L2_DENSE_SIZE = 600\n",
    "GEN_L3_DENSE_SIZE = num_genes\n",
    "\n",
    "DIS_INPUT_SIZE = num_genes\n",
    "DIS_L1_DENSE_SIZE = 200\n",
    "DIS_L2_DENSE_SIZE = 200\n",
    "\n",
    "NOISE_STDEV = training_data_max / 10\n",
    "POISSON_LAM = 1\n",
    "\n",
    "# Training params\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "GEN_BATCH_SIZE = 10\n",
    "BUFFER_SIZE = 10000\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(NOISE_STDEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "Create tensors from training data - Convert to Int32 for better work on GPU with batch and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 19248), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(df_training_data.T.values.astype('float32')).shuffle(BUFFER_SIZE).batch(TRAIN_BATCH_SIZE)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GANN model\n",
    "\n",
    "Define function for contructing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(GEN_L1_DENSE_SIZE, use_bias=False, input_shape=(LATENT_VARIABLE_SIZE,)))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L1_DENSE_SIZE, 1)  # Note: None is the batch size\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(GEN_L2_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L2_DENSE_SIZE, 1)\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Dense(GEN_L3_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L3_DENSE_SIZE, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for constructing discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(DIS_L1_DENSE_SIZE, use_bias=False, input_shape=(DIS_INPUT_SIZE,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(DIS_L2_DENSE_SIZE, use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise():\n",
    "    # Create some random noise for the generator\n",
    "    n_noise = tf.random.normal([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], mean=0.0, stddev=NOISE_STDEV)\n",
    "    p_noise = tf.random.poisson([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], lam=POISSON_LAM)\n",
    "    noise = tf.abs(n_noise + p_noise)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "    #total_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #total_loss = -tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a batch of real cell profiles from the training set\n",
    "# @tf.function\n",
    "def train_step(cell_profiles):\n",
    "    noise = gen_noise()\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_profiles = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(cell_profiles, training=True)\n",
    "        fake_output = discriminator(generated_profiles, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        met_gen_loss(gen_loss)\n",
    "        met_disc_loss(disc_loss)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GANN model\n",
    "\n",
    "Create generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from test data to check network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 19248)\n",
      "-0.1516566\n",
      "0.520472\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "noise = gen_noise()\n",
    "generated_profile = generator(noise, training=False)\n",
    "print(generated_profile.shape)\n",
    "print(generated_profile.numpy().min())\n",
    "print(generated_profile.numpy().max())\n",
    "\n",
    "decision = discriminator(generated_profile)\n",
    "print(decision.shape)\n",
    "#print(decision.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GANN\n",
    "\n",
    "Define tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_gen_loss = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
    "met_disc_loss = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create log directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n"
     ]
    }
   ],
   "source": [
    "print('Running...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    #Train the epoch\n",
    "    for data_batch in train_dataset:\n",
    "        train_step(data_batch)\n",
    "    \n",
    "    #Log metrics\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_loss', met_gen_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('disc_loss', met_disc_loss.result(), step=epoch)\n",
    "    \n",
    "    #Do some basic time logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "    else:\n",
    "        time.time()\n",
    "    \n",
    "    #Log stats\n",
    "    template = 'Epoch {}, Gen_loss: {}, Disc_loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                           met_gen_loss.result(), \n",
    "                           met_disc_loss.result()))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    met_gen_loss.reset_states()\n",
    "    met_disc_loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {train_log_dir} --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
