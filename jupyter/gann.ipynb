{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANN - CHECK SPELLING\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "\n",
    "train_feature_path = data_path + '/tpm_combined.csv'\n",
    "train_gene_name_path = data_path + '/tpm_combined_rows.csv'\n",
    "train_cell_name_path = data_path + '/tpm_combined_cols.csv'\n",
    "\n",
    "test_feature_path = data_path + '/tpm_combined_test.csv'\n",
    "test_gene_name_path = data_path + '/tpm_combined_rows_test.csv'\n",
    "test_cell_name_path = data_path + '/tpm_combined_cols_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load datasets into frames and check all the shapes match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n",
      "(1798, 1)\n",
      "(6807, 1798)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names = pd.read_csv(train_gene_name_path, header=None)\n",
    "df_cell_names = pd.read_csv(train_cell_name_path, header=None)\n",
    "df_training_data = pd.read_csv(train_feature_path, header=None)\n",
    "\n",
    "print(df_gene_names.shape)\n",
    "print(df_cell_names.shape)\n",
    "print(df_training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 1)\n",
      "(500, 1)\n",
      "(6807, 500)\n"
     ]
    }
   ],
   "source": [
    "df_gene_names_test = pd.read_csv(test_gene_name_path, header=None)\n",
    "df_cell_names_test = pd.read_csv(test_cell_name_path, header=None)\n",
    "df_test_data = pd.read_csv(test_feature_path, header=None)\n",
    "\n",
    "print(df_gene_names_test.shape)\n",
    "print(df_cell_names_test.shape)\n",
    "print(df_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of genes in the input dataset determines the generator output as well as the dicriminator inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_genes = df_gene_names.shape[0]\n",
    "df_gene_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1788</th>\n",
       "      <th>1789</th>\n",
       "      <th>1790</th>\n",
       "      <th>1791</th>\n",
       "      <th>1792</th>\n",
       "      <th>1793</th>\n",
       "      <th>1794</th>\n",
       "      <th>1795</th>\n",
       "      <th>1796</th>\n",
       "      <th>1797</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.437635</td>\n",
       "      <td>0.422952</td>\n",
       "      <td>0.500639</td>\n",
       "      <td>0.460063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625194</td>\n",
       "      <td>0.636415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.621385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806046</td>\n",
       "      <td>0.461961</td>\n",
       "      <td>0.533844</td>\n",
       "      <td>0.691345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479120</td>\n",
       "      <td>0.373748</td>\n",
       "      <td>0.779602</td>\n",
       "      <td>0.652176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330974</td>\n",
       "      <td>0.599975</td>\n",
       "      <td>0.538749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501173</td>\n",
       "      <td>0.315232</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.528174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.423527</td>\n",
       "      <td>0.412909</td>\n",
       "      <td>0.488703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602726</td>\n",
       "      <td>0.522253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440998</td>\n",
       "      <td>0.837320</td>\n",
       "      <td>0.353001</td>\n",
       "      <td>0.536904</td>\n",
       "      <td>0.983997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614253</td>\n",
       "      <td>0.823296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379869</td>\n",
       "      <td>0.381094</td>\n",
       "      <td>0.137787</td>\n",
       "      <td>0.309987</td>\n",
       "      <td>0.571913</td>\n",
       "      <td>0.548388</td>\n",
       "      <td>0.384074</td>\n",
       "      <td>0.455198</td>\n",
       "      <td>0.469107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428848</td>\n",
       "      <td>0.816352</td>\n",
       "      <td>0.531461</td>\n",
       "      <td>0.244717</td>\n",
       "      <td>0.813174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327914</td>\n",
       "      <td>0.048955</td>\n",
       "      <td>0.729328</td>\n",
       "      <td>0.346599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666779</td>\n",
       "      <td>0.565507</td>\n",
       "      <td>0.350199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561451</td>\n",
       "      <td>0.339109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652654</td>\n",
       "      <td>0.036969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158096</td>\n",
       "      <td>0.404081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609758</td>\n",
       "      <td>0.521244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525897</td>\n",
       "      <td>0.672412</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411294</td>\n",
       "      <td>0.481663</td>\n",
       "      <td>0.346585</td>\n",
       "      <td>0.315634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307813</td>\n",
       "      <td>0.399935</td>\n",
       "      <td>0.519273</td>\n",
       "      <td>0.468670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541795</td>\n",
       "      <td>0.634956</td>\n",
       "      <td>0.370401</td>\n",
       "      <td>0.348183</td>\n",
       "      <td>0.443963</td>\n",
       "      <td>0.396523</td>\n",
       "      <td>0.646611</td>\n",
       "      <td>0.477193</td>\n",
       "      <td>0.585492</td>\n",
       "      <td>0.497864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>0.307292</td>\n",
       "      <td>0.582062</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592147</td>\n",
       "      <td>0.534678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584783</td>\n",
       "      <td>0.670761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659579</td>\n",
       "      <td>0.540393</td>\n",
       "      <td>0.564257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642897</td>\n",
       "      <td>0.555258</td>\n",
       "      <td>0.136405</td>\n",
       "      <td>0.577916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.665896</td>\n",
       "      <td>0.659554</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597672</td>\n",
       "      <td>0.154375</td>\n",
       "      <td>0.653057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392436</td>\n",
       "      <td>0.773484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362291</td>\n",
       "      <td>0.740020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389472</td>\n",
       "      <td>0.754507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6807 rows Ã— 1798 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.437635  0.422952  0.500639  0.460063  0.000000  0.625194  0.636415   \n",
       "1     0.000000  0.330974  0.599975  0.538749  0.000000  0.000000  0.000000   \n",
       "2     0.423527  0.412909  0.488703  0.000000  0.602726  0.522253  0.000000   \n",
       "3     0.000000  0.379869  0.381094  0.137787  0.309987  0.571913  0.548388   \n",
       "4     0.000000  0.666779  0.565507  0.350199  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6802  0.000000  0.000000  0.158096  0.404081  0.000000  0.000000  0.000000   \n",
       "6803  0.000000  0.411294  0.481663  0.346585  0.315634  0.000000  0.307813   \n",
       "6804  0.307292  0.582062  0.466408  0.000000  0.328192  0.000000  0.000000   \n",
       "6805  0.000000  0.659579  0.540393  0.564257  0.000000  0.000000  0.000000   \n",
       "6806  0.000000  0.597672  0.154375  0.653057  0.000000  0.000000  0.000000   \n",
       "\n",
       "          7         8         9     ...      1788      1789      1790  \\\n",
       "0     0.000000  0.000000  0.621385  ...  0.000000  0.806046  0.461961   \n",
       "1     0.000000  0.000000  0.000000  ...  0.000000  0.806895  0.000000   \n",
       "2     0.541224  0.000000  0.000000  ...  0.440998  0.837320  0.353001   \n",
       "3     0.384074  0.455198  0.469107  ...  0.428848  0.816352  0.531461   \n",
       "4     0.000000  0.000000  0.000000  ...  0.000000  0.171824  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6802  0.720074  0.000000  0.490125  ...  0.000000  0.609758  0.521244   \n",
       "6803  0.399935  0.519273  0.468670  ...  0.541795  0.634956  0.370401   \n",
       "6804  0.000000  0.000000  0.000000  ...  0.000000  0.574837  0.000000   \n",
       "6805  0.666009  0.000000  0.000000  ...  0.000000  0.642897  0.555258   \n",
       "6806  0.000000  0.000000  0.000000  ...  0.392436  0.773484  0.000000   \n",
       "\n",
       "          1791      1792      1793      1794      1795      1796      1797  \n",
       "0     0.533844  0.691345  0.000000  0.479120  0.373748  0.779602  0.652176  \n",
       "1     0.000000  0.765089  0.000000  0.501173  0.315232  0.255972  0.528174  \n",
       "2     0.536904  0.983997  0.000000  0.000000  0.614253  0.823296  0.000000  \n",
       "3     0.244717  0.813174  0.000000  0.327914  0.048955  0.729328  0.346599  \n",
       "4     0.561451  0.339109  0.000000  0.000000  0.652654  0.036969  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6802  0.000000  0.297598  0.000000  0.000000  0.525897  0.672412  0.000000  \n",
       "6803  0.348183  0.443963  0.396523  0.646611  0.477193  0.585492  0.497864  \n",
       "6804  0.592147  0.534678  0.000000  0.000000  0.584783  0.670761  0.000000  \n",
       "6805  0.136405  0.577916  0.000000  0.459978  0.665896  0.659554  0.000000  \n",
       "6806  0.362291  0.740020  0.000000  0.000000  0.389472  0.754507  0.000000  \n",
       "\n",
       "[6807 rows x 1798 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model variables - COMMENT ON EACH ONE TO DESCRIBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "LATENT_VARIABLE_SIZE = 100\n",
    "GEN_L1_DENSE_SIZE = 600\n",
    "GEN_L2_DENSE_SIZE = 600\n",
    "GEN_L3_DENSE_SIZE = num_genes\n",
    "\n",
    "DIS_INPUT_SIZE = num_genes\n",
    "DIS_L1_DENSE_SIZE = 200\n",
    "DIS_L2_DENSE_SIZE = 200\n",
    "\n",
    "NOISE_STDEV = 0.1\n",
    "POISSON_LAM = 1\n",
    "\n",
    "# Training params\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "TRAIN_BUFFER_SIZE = 10000\n",
    "TEST_BATCH_SIZE = 500\n",
    "TEST_BUFFER_SIZE = 500\n",
    "GEN_BATCH_SIZE = 10\n",
    "EPOCHS = 30\n",
    "\n",
    "#LEARNING_RATE = 0.001\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test datasets\n",
    "\n",
    "Create tensors from training data - Convert to Int32 for better work on GPU with batch and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 6807), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(df_training_data.T.values.astype('float32')).shuffle(TRAIN_BUFFER_SIZE).batch(TRAIN_BATCH_SIZE)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None, 6807), types: tf.float32>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(df_test_data.T.values.astype('float32')).shuffle(TEST_BUFFER_SIZE).batch(TEST_BATCH_SIZE)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GANN model\n",
    "\n",
    "Define function for contructing the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(GEN_L1_DENSE_SIZE, use_bias=False, input_shape=(LATENT_VARIABLE_SIZE,)))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L1_DENSE_SIZE, 1)  # Note: None is the batch size\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(GEN_L2_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L2_DENSE_SIZE, 1)\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Dense(GEN_L3_DENSE_SIZE, use_bias=False))\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #assert model.output_shape == (None, GEN_L3_DENSE_SIZE, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for constructing discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    #L1\n",
    "    model.add(layers.Dense(DIS_L1_DENSE_SIZE, use_bias=False, input_shape=(DIS_INPUT_SIZE,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L2\n",
    "    model.add(layers.Dense(DIS_L2_DENSE_SIZE, use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    #L3\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noise():\n",
    "    # Create some random noise for the generator\n",
    "    n_noise = tf.random.normal([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], mean=0.0, stddev=NOISE_STDEV)\n",
    "    p_noise = tf.random.poisson([GEN_BATCH_SIZE, LATENT_VARIABLE_SIZE], lam=POISSON_LAM)\n",
    "    noise = tf.abs(n_noise + p_noise)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "    #total_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    #total_loss = -tf.reduce_mean(fake_output)\n",
    "    #return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a batch of real cell profiles from the training set\n",
    "# @tf.function\n",
    "def train_step(cell_profiles):\n",
    "    noise = gen_noise()\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_profiles = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(cell_profiles, training=True)\n",
    "        fake_output = discriminator(generated_profiles, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        met_gen_loss(gen_loss)\n",
    "        met_disc_loss(disc_loss)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GANN model\n",
    "\n",
    "Create generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = create_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate from test data to check network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6807)\n",
      "-0.21753356\n",
      "0.73110306\n",
      "[[0.09995452]\n",
      " [0.20936958]\n",
      " [0.12328199]\n",
      " [0.04046891]\n",
      " [0.09720568]\n",
      " [0.05201562]\n",
      " [0.15192515]\n",
      " [0.2383784 ]\n",
      " [0.03936909]\n",
      " [0.05383674]]\n"
     ]
    }
   ],
   "source": [
    "noise = gen_noise()\n",
    "generated_profile = generator(noise, training=False)\n",
    "print(generated_profile.shape)\n",
    "print(generated_profile.numpy().min())\n",
    "print(generated_profile.numpy().max())\n",
    "\n",
    "decision = discriminator(generated_profile)\n",
    "#print(decision.shape)\n",
    "print(decision.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GANN\n",
    "\n",
    "Define tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_gen_loss = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
    "met_disc_loss = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)\n",
    "met_test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create log directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "gen_log_dir = 'logs/gradient_tape/' + current_time + '/gen_train'\n",
    "disc_log_dir = 'logs/gradient_tape/' + current_time + '/disc_train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/disc_test'\n",
    "all_log_dir = 'logs/gradient_tape/' + current_time + '/all'\n",
    "\n",
    "all_summary_writer = tf.summary.create_file_writer(all_log_dir)\n",
    "gen_summary_writer = tf.summary.create_file_writer(gen_log_dir)\n",
    "disc_summary_writer = tf.summary.create_file_writer(disc_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "Epoch 1, Gen_loss: 3452.90283203125, Disc_loss: 248.2937469482422, Test_loss: 239.1258544921875\n",
      "Epoch 2, Gen_loss: 222.57615661621094, Disc_loss: 114.05821228027344, Test_loss: 41.0180549621582\n",
      "Epoch 3, Gen_loss: 77.71817016601562, Disc_loss: 21.69118881225586, Test_loss: 12.847174644470215\n",
      "Epoch 4, Gen_loss: 73.7674560546875, Disc_loss: 9.622663497924805, Test_loss: 3.48417329788208\n",
      "Epoch 5, Gen_loss: 30.723207473754883, Disc_loss: 1.1051472425460815, Test_loss: 1.064454197883606\n",
      "Epoch 6, Gen_loss: 12.148962020874023, Disc_loss: 1.2752946615219116, Test_loss: 0.7034788131713867\n",
      "Epoch 7, Gen_loss: 21.68143081665039, Disc_loss: 1.243031620979309, Test_loss: 0.48461708426475525\n",
      "Epoch 8, Gen_loss: 35.41208267211914, Disc_loss: 1.6895216703414917, Test_loss: 1.430935025215149\n",
      "Epoch 9, Gen_loss: 10.027359008789062, Disc_loss: 0.8146588802337646, Test_loss: 0.543753445148468\n",
      "Time for epoch 10 is 9.871910572052002 sec.\n",
      "Epoch 10, Gen_loss: 24.187450408935547, Disc_loss: 0.6829692125320435, Test_loss: 0.14706850051879883\n",
      "Epoch 11, Gen_loss: 27.364137649536133, Disc_loss: 0.9862479567527771, Test_loss: 0.9453535079956055\n",
      "Epoch 12, Gen_loss: 15.36042308807373, Disc_loss: 0.9195045232772827, Test_loss: 0.4011319577693939\n",
      "Epoch 13, Gen_loss: 31.5499324798584, Disc_loss: 0.8538434505462646, Test_loss: 0.4521077871322632\n",
      "Epoch 14, Gen_loss: 13.78425407409668, Disc_loss: 1.3571404218673706, Test_loss: 0.6926357746124268\n",
      "Epoch 15, Gen_loss: 31.93528175354004, Disc_loss: 1.064903736114502, Test_loss: 0.26404768228530884\n",
      "Epoch 16, Gen_loss: 23.853424072265625, Disc_loss: 0.7086110711097717, Test_loss: 1.085284948348999\n",
      "Epoch 17, Gen_loss: 9.87856674194336, Disc_loss: 0.5030363202095032, Test_loss: 0.4154509902000427\n",
      "Epoch 18, Gen_loss: 15.773338317871094, Disc_loss: 0.5747794508934021, Test_loss: 0.37407535314559937\n",
      "Epoch 19, Gen_loss: 12.533706665039062, Disc_loss: 0.49108749628067017, Test_loss: 0.17900757491588593\n",
      "Time for epoch 20 is 9.745449542999268 sec.\n",
      "Epoch 20, Gen_loss: 8.731657981872559, Disc_loss: 0.3995508849620819, Test_loss: 0.1268107295036316\n",
      "Epoch 21, Gen_loss: 11.002132415771484, Disc_loss: 0.33072343468666077, Test_loss: 0.15741094946861267\n",
      "Epoch 22, Gen_loss: 7.136517524719238, Disc_loss: 0.2814999520778656, Test_loss: 0.14882764220237732\n",
      "Epoch 23, Gen_loss: 10.76513671875, Disc_loss: 0.3478087782859802, Test_loss: 0.08575331419706345\n",
      "Epoch 24, Gen_loss: 5.003943920135498, Disc_loss: 0.2920219600200653, Test_loss: 0.14185170829296112\n",
      "Epoch 25, Gen_loss: 5.777581691741943, Disc_loss: 0.22885528206825256, Test_loss: 0.1792771965265274\n",
      "Epoch 26, Gen_loss: 7.693286895751953, Disc_loss: 0.35031992197036743, Test_loss: 0.08461485803127289\n",
      "Epoch 27, Gen_loss: 4.949298858642578, Disc_loss: 0.2655167877674103, Test_loss: 0.6009771227836609\n",
      "Epoch 28, Gen_loss: 7.913883686065674, Disc_loss: 0.2915792763233185, Test_loss: 0.1544165462255478\n",
      "Epoch 29, Gen_loss: 5.799867153167725, Disc_loss: 0.26581379771232605, Test_loss: 0.08502760529518127\n",
      "Time for epoch 30 is 10.412178039550781 sec.\n",
      "Epoch 30, Gen_loss: 5.539407730102539, Disc_loss: 0.18930575251579285, Test_loss: 0.14628300070762634\n"
     ]
    }
   ],
   "source": [
    "print('Running...')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    #Train the epoch\n",
    "    for data_batch in train_dataset:\n",
    "        train_step(data_batch)\n",
    "        \n",
    "    #Run test data through discriminator\n",
    "    for data_batch in test_dataset:\n",
    "        test_decision = discriminator(data_batch, training=False)\n",
    "\n",
    "    test_loss = cross_entropy(tf.ones_like(test_decision), test_decision)\n",
    "    met_test_loss(test_loss)\n",
    "    \n",
    "    #Log metrics\n",
    "    with all_summary_writer.as_default():\n",
    "        tf.summary.scalar('2_gen_loss', met_gen_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_disc_loss', met_disc_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('3_test_loss', met_test_loss.result(), step=epoch)\n",
    "    \n",
    "    with gen_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_gen_loss.result(), step=epoch)\n",
    "           \n",
    "    with disc_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_disc_loss.result(), step=epoch)\n",
    "    \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('1_loss', met_test_loss.result(), step=epoch)\n",
    "    \n",
    "    #Do some basic time logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print ('Time for epoch {} is {} sec.'.format(epoch + 1, time.time()-start))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    else:\n",
    "        time.time()\n",
    "    \n",
    "    #Log stats\n",
    "    template = 'Epoch {}, Gen_loss: {}, Disc_loss: {}, Test_loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                           met_gen_loss.result(), \n",
    "                           met_disc_loss.result(),\n",
    "                           met_test_loss.result()))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    met_gen_loss.reset_states()\n",
    "    met_disc_loss.reset_states()\n",
    "    met_test_loss.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir {train_log_dir} --host localhost --port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
